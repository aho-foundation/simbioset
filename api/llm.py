"""
LLM API Proxy Client - Provides LLM access with retry logic and model management
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç context_size_hint –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≤—ã–±–æ—Ä–∞ –º–æ–¥–µ–ª–∏.
"""

import asyncio
import json
from typing import List, Optional, Dict, Any
from openai import AsyncOpenAI
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type
from api.logger import root_logger
from api.settings import LLM_PROXY_URL, LLM_PROXY_TOKEN
import aiohttp

log = root_logger.info

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∏–µ–Ω—Ç–æ–≤ –¥–ª—è –ø—Ä–æ–∫—Å–∏
_openai_client: Optional[AsyncOpenAI] = None
_http_session: Optional[aiohttp.ClientSession] = None


def get_llm_client() -> AsyncOpenAI:
    """
    –ü–æ–ª—É—á–∞–µ—Ç OpenAI –∫–ª–∏–µ–Ω—Ç –¥–ª—è –ø—Ä–æ–∫—Å–∏ (–ø—É–±–ª–∏—á–Ω—ã–π API).

    Returns:
        AsyncOpenAI –∫–ª–∏–µ–Ω—Ç, –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –ø—Ä–æ–∫—Å–∏
    """
    return _get_openai_client()


def _get_openai_client() -> AsyncOpenAI:
    """–ü–æ–ª—É—á–∞–µ—Ç –∏–ª–∏ —Å–æ–∑–¥–∞–µ—Ç OpenAI –∫–ª–∏–µ–Ω—Ç –¥–ª—è –ø—Ä–æ–∫—Å–∏."""
    global _openai_client
    if _openai_client is None:
        _openai_client = AsyncOpenAI(
            base_url=f"{LLM_PROXY_URL}/v1",
            api_key=LLM_PROXY_TOKEN or "not-needed",  # OpenAI SDK —Ç—Ä–µ–±—É–µ—Ç api_key, –¥–∞–∂–µ –µ—Å–ª–∏ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è
            timeout=120.0,
        )
    return _openai_client


def _get_http_session() -> aiohttp.ClientSession:
    """–ü–æ–ª—É—á–∞–µ—Ç –∏–ª–∏ —Å–æ–∑–¥–∞–µ—Ç aiohttp —Å–µ—Å—Å–∏—é –¥–ª—è –ø—Ä—è–º—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ –ø—Ä–æ–∫—Å–∏."""
    global _http_session
    if _http_session is None:
        timeout = aiohttp.ClientTimeout(total=120.0)
        _http_session = aiohttp.ClientSession(
            timeout=timeout,
        )
    return _http_session


async def close_llm_clients() -> None:
    """
    –ó–∞–∫—Ä—ã–≤–∞–µ—Ç –≤—Å–µ –∫–ª–∏–µ–Ω—Ç—ã LLM (aiohttp —Å–µ—Å—Å–∏—é).
    –í—ã–∑—ã–≤–∞–µ—Ç—Å—è –ø—Ä–∏ shutdown –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è.
    """
    global _http_session
    if _http_session is not None:
        await _http_session.close()
        _http_session = None
        log("‚úÖ LLM HTTP session closed")


# --- LLM Exceptions ---
class LLMError(Exception):
    """Base exception for LLM errors"""

    pass


class LLMTemporaryError(LLMError):
    """Temporary error, can be retried"""

    pass


class LLMPermanentError(LLMError):
    """Permanent error, retry is useless"""

    pass


async def _call_proxy_api(
    messages: List[dict],
    model: Optional[str] = None,
    context_size_hint: Optional[str] = None,
) -> str:
    """
    –í—ã–∑—ã–≤–∞–µ—Ç –ø—Ä–æ–∫—Å–∏-—Å–µ—Ä–≤–∏—Å –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –æ—Ç–≤–µ—Ç–∞ –æ—Ç LLM.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä—è–º–æ–π HTTP –∑–∞–ø—Ä–æ—Å –¥–ª—è –ø–æ–¥–¥–µ—Ä–∂–∫–∏ context_size_hint.

    Args:
        messages: –°–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π –≤ —Ñ–æ—Ä–º–∞—Ç–µ OpenAI API
        model: –ò–º—è –º–æ–¥–µ–ª–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω–æ - –≤—ã–±–∏—Ä–∞–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)
        context_size_hint: –ü–æ–¥—Å–∫–∞–∑–∫–∞ –æ —Ä–∞–∑–º–µ—Ä–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ ("normal" –∏–ª–∏ "large")

    Returns:
        –¢–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–∞ –æ—Ç LLM

    Raises:
        LLMTemporaryError: –ü—Ä–∏ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ—à–∏–±–∫–∞—Ö
        LLMPermanentError: –ü—Ä–∏ –ø–æ—Å—Ç–æ—è–Ω–Ω—ã—Ö –æ—à–∏–±–∫–∞—Ö (–∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –∏ —Ç.–¥.)
    """
    session = _get_http_session()
    url = f"{LLM_PROXY_URL}/v1/chat/completions"

    try:
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Ç–µ–ª–æ –∑–∞–ø—Ä–æ—Å–∞
        body: Dict[str, Any] = {
            "messages": messages,
            "stream": False,
        }

        # –î–æ–±–∞–≤–ª—è–µ–º –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        if model:
            body["model"] = model
        if context_size_hint:
            body["context_size_hint"] = context_size_hint

        # –§–æ—Ä–º–∏—Ä—É–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ —Å —Ç–æ–∫–µ–Ω–æ–º –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏
        headers: Dict[str, str] = {}
        if LLM_PROXY_TOKEN:
            headers["Authorization"] = f"Bearer {LLM_PROXY_TOKEN}"
        else:
            log("‚ö†Ô∏è LLM_PROXY_TOKEN –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –∑–∞–ø—Ä–æ—Å –º–æ–∂–µ—Ç –±—ã—Ç—å –æ—Ç–∫–ª–æ–Ω–µ–Ω")

        # –í—ã–ø–æ–ª–Ω—è–µ–º –∑–∞–ø—Ä–æ—Å
        async with session.post(url, json=body, headers=headers) as response:
            status_code = response.status

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ç—É—Å –æ—Ç–≤–µ—Ç–∞
            if status_code == 401:
                raise LLMPermanentError(f"Authorization error: HTTP {status_code}")
            elif status_code == 503:
                raise LLMTemporaryError(f"Service unavailable: HTTP {status_code}")
            elif status_code >= 500:
                raise LLMTemporaryError(f"Server error: HTTP {status_code}")
            elif status_code >= 400:
                raise LLMPermanentError(f"HTTP error {status_code}")

            data = await response.json()

        if not data.get("choices") or len(data["choices"]) == 0:
            raise LLMTemporaryError("Empty response from proxy: no choices returned")

        reply = data["choices"][0].get("message", {}).get("content", "")

        if not reply or len(reply.strip()) < 10:
            raise LLMTemporaryError("Got empty or too short response from proxy")

        return reply.strip()

    except aiohttp.ClientResponseError as e:
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ HTTP –æ—à–∏–±–æ–∫ –æ—Ç aiohttp
        status_code = e.status
        if status_code == 401:
            raise LLMPermanentError(f"Authorization error: {e}")
        elif status_code == 503:
            raise LLMTemporaryError(f"Service unavailable: {e}")
        elif status_code >= 500:
            raise LLMTemporaryError(f"Server error: {e}")
        else:
            raise LLMPermanentError(f"HTTP error {status_code}: {e}")

    except (aiohttp.ServerTimeoutError, asyncio.TimeoutError) as e:
        raise LLMTemporaryError(f"Request timeout: {e}")

    except aiohttp.ClientError as e:
        raise LLMTemporaryError(f"Network error: {e}")

    except Exception as e:
        # –ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–µ –æ—à–∏–±–∫–∏
        error_msg = str(e).lower()
        if "api_key" in error_msg or "authorization" in error_msg or "401" in error_msg:
            raise LLMPermanentError(f"Authorization error: {e}")
        raise LLMTemporaryError(f"Unexpected error: {e}")


def _determine_context_size_hint(context: str) -> str:
    """
    –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –ø–æ–¥—Å–∫–∞–∑–∫—É –æ —Ä–∞–∑–º–µ—Ä–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –µ–≥–æ –¥–ª–∏–Ω—ã.

    Args:
        context: –¢–µ–∫—Å—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞

    Returns:
        "normal" –∏–ª–∏ "large"
    """
    # –ü—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω—ã–π —Ä–∞—Å—á–µ—Ç: 1 —Ç–æ–∫–µ–Ω ‚âà 4 —Å–∏–º–≤–æ–ª–∞
    # –ë–æ–ª—å—à–æ–π –∫–æ–Ω—Ç–µ–∫—Å—Ç: > 10000 —Å–∏–º–≤–æ–ª–æ–≤ (‚âà 2500 —Ç–æ–∫–µ–Ω–æ–≤)
    return "large" if len(context) > 10000 else "normal"


@retry(
    retry=retry_if_exception_type(LLMTemporaryError),
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=4, max=10),
    reraise=True,
)
async def call_llm_with_retry(
    llm_context: str,
    *,
    origin: str | None = None,
    context_size_hint: Optional[str] = None,
    model: Optional[str] = None,
) -> str:
    """
    –í—ã–∑—ã–≤–∞–µ—Ç LLM —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º–∏ –ø–æ–≤—Ç–æ—Ä–∞–º–∏ –ø—Ä–∏ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ—à–∏–±–∫–∞—Ö.
    –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç context_size_hint –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞–∑–º–µ—Ä–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.

    Args:
        llm_context: –ö–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è LLM
        origin: –¢–µ–≥ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
        context_size_hint: –ü–æ–¥—Å–∫–∞–∑–∫–∞ –æ —Ä–∞–∑–º–µ—Ä–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ ("normal" –∏–ª–∏ "large").
                          –ï—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω–æ, –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏.
        model: –ò–º—è –º–æ–¥–µ–ª–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω–æ - –≤—ã–±–∏—Ä–∞–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)

    Returns:
        –û—Ç–≤–µ—Ç –æ—Ç LLM

    Raises:
        LLMPermanentError: –ü—Ä–∏ –Ω–µ–≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏–º—ã—Ö –æ—à–∏–±–∫–∞—Ö
        LLMTemporaryError: –ü—Ä–∏ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ—à–∏–±–∫–∞—Ö (–±—É–¥–µ—Ç –ø–æ–≤—Ç–æ—Ä–µ–Ω–æ)
    """
    try:
        # –õ–æ–≥–∏—Ä—É–µ–º –≤—Ö–æ–¥—è—â–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ (–∫–æ—Ä–æ—Ç–∫–∏–π –ø—Ä–µ–≤—å—é, –±–µ–∑ –¥–∞–º–ø–∞ –≤—Å–µ–≥–æ —Ç–µ–∫—Å—Ç–∞)
        preview = llm_context.strip().replace("\n", " ")
        if len(preview) > 200:
            preview = preview[:200] + "‚Ä¶"
        origin_tag = origin or "generic"
        log(f"üìù LLM request origin={origin_tag} context_len={len(llm_context)} preview='{preview}'")

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º context_size_hint –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω
        if context_size_hint is None:
            context_size_hint = _determine_context_size_hint(llm_context)

        # –í—ã–∑—ã–≤–∞–µ–º –ø—Ä–æ–∫—Å–∏-—Å–µ—Ä–≤–∏—Å –±–µ–∑ —É–∫–∞–∑–∞–Ω–∏—è –º–æ–¥–µ–ª–∏ - –ø—Ä–æ–∫—Å–∏ –≤—ã–±–µ—Ä–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
        log(f"ü§ñ Generating reply via proxy {LLM_PROXY_URL} (origin={origin_tag}, context_hint={context_size_hint})")
        messages = [{"role": "user", "content": llm_context}]
        reply = await _call_proxy_api(messages, model=model, context_size_hint=context_size_hint)

        log(f"‚úÖ reply successfully generated ({len(reply)} characters)")
        return reply

    except asyncio.TimeoutError as e:
        log(f"‚è±Ô∏è Timeout calling proxy: {e}")
        raise LLMTemporaryError(f"Timeout: {e}")
    except LLMTemporaryError:
        # –ü—Ä–æ–±—Ä–∞—Å—ã–≤–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –æ—à–∏–±–∫–∏ –∫–∞–∫ –µ—Å—Ç—å
        raise
    except LLMPermanentError as e:
        # –ü–æ—Å—Ç–æ—è–Ω–Ω—ã–µ –æ—à–∏–±–∫–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è) - –º–æ–¥–µ–ª—å —É–∂–µ —É–¥–∞–ª–µ–Ω–∞ –∏–∑ —Å–ø–∏—Å–∫–∞ –≤ –ø—Ä–æ–∫—Å–∏
        # –ü—Ä–æ–±—Ä–∞—Å—ã–≤–∞–µ–º –∫–∞–∫ –µ—Å—Ç—å
        raise
    except Exception as e:
        error_msg = str(e).lower()
        log(f"‚ùå Unexpected error calling proxy: {e}")
        raise LLMTemporaryError(f"Unexpected error: {e}")


async def list_models() -> List[Dict[str, Any]]:
    """
    –ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∏–∑ –ø—Ä–æ–∫—Å–∏.

    Returns:
        –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –º–æ–¥–µ–ª—è—Ö

    Raises:
        LLMPermanentError: –ü—Ä–∏ –æ—à–∏–±–∫–∞—Ö –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏
        LLMTemporaryError: –ü—Ä–∏ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ—à–∏–±–∫–∞—Ö
    """
    session = _get_http_session()
    url = f"{LLM_PROXY_URL}/v1/models"

    try:
        async with session.get(url) as response:
            status_code = response.status

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ç—É—Å –æ—Ç–≤–µ—Ç–∞
            if status_code == 401:
                raise LLMPermanentError(f"Authorization error: HTTP {status_code}")
            elif status_code >= 500:
                raise LLMTemporaryError(f"Server error: HTTP {status_code}")
            elif status_code >= 400:
                raise LLMPermanentError(f"HTTP error {status_code}")

            data = await response.json()

        return data.get("data", [])

    except aiohttp.ClientResponseError as e:
        status_code = e.status
        if status_code == 401:
            raise LLMPermanentError(f"Authorization error: {e}")
        elif status_code >= 500:
            raise LLMTemporaryError(f"Server error: {e}")
        else:
            raise LLMPermanentError(f"HTTP error {status_code}: {e}")

    except (aiohttp.ServerTimeoutError, asyncio.TimeoutError) as e:
        raise LLMTemporaryError(f"Request timeout: {e}")

    except aiohttp.ClientError as e:
        raise LLMTemporaryError(f"Network error: {e}")

    except Exception as e:
        raise LLMTemporaryError(f"Unexpected error: {e}")


class LLMClientWrapper:
    """
    –û–±–µ—Ä—Ç–∫–∞ –Ω–∞–¥ OpenAI –∫–ª–∏–µ–Ω—Ç–æ–º –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å –∫–æ–¥–æ–º,
    –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–µ—Ç–æ–¥ generate().
    """

    async def generate(self, prompt: str, *, context_size_hint: Optional[str] = None) -> str:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –Ω–∞ –ø—Ä–æ–º–ø—Ç —á–µ—Ä–µ–∑ LLM.

        Args:
            prompt: –¢–µ–∫—Å—Ç –ø—Ä–æ–º–ø—Ç–∞
            context_size_hint: –ü–æ–¥—Å–∫–∞–∑–∫–∞ –æ —Ä–∞–∑–º–µ—Ä–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ ("normal" –∏–ª–∏ "large")

        Returns:
            –û—Ç–≤–µ—Ç –æ—Ç LLM
        """
        return await call_llm_with_retry(prompt, origin="llm_client_wrapper", context_size_hint=context_size_hint)


def get_llm_client_wrapper() -> LLMClientWrapper:
    """
    –ü–æ–ª—É—á–∞–µ—Ç –æ–±–µ—Ä—Ç–∫—É –Ω–∞–¥ LLM –∫–ª–∏–µ–Ω—Ç–æ–º —Å –º–µ—Ç–æ–¥–æ–º generate().
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å –∫–æ–¥–æ–º, –∫–æ—Ç–æ—Ä—ã–π –æ–∂–∏–¥–∞–µ—Ç –º–µ—Ç–æ–¥ generate().

    Returns:
        LLMClientWrapper —Å –º–µ—Ç–æ–¥–æ–º generate()
    """
    return LLMClientWrapper()


async def rephrase_search_query(query: str) -> List[str]:
    """
    –ü–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä—É–µ—Ç –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –¥–ª—è –ø–æ–∏—Å–∫–∞ –∫–æ—Å–≤–µ–Ω–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π —á–µ—Ä–µ–∑ LLM.
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç 2-3 –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏.

    Args:
        query: –ò—Å—Ö–æ–¥–Ω—ã–π –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å

    Returns:
        –°–ø–∏—Å–æ–∫ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã—Ö —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–æ–∫ –∑–∞–ø—Ä–æ—Å–∞
    """
    prompt = f"""
    You are a search expert. The user's query returned no results.
    Generate 3 alternative search queries that might find relevant information even if the exact keywords are missing.
    Focus on:
    1. Synonyms and related concepts
    2. Broader or narrower terms
    3. Different linguistic formulations
     
    User Query: "{query}"
     
    Output ONLY a JSON array of strings, e.g. ["query 1", "query 2", "query 3"].
    """
    try:
        response = await call_llm_with_retry(prompt, origin="rephrase_search_query", context_size_hint="normal")
        # Extract JSON from response (it might have markdown code blocks)
        import re

        json_match = re.search(r"\[.*\]", response, re.DOTALL)
        if json_match:
            result = json.loads(json_match.group(0))
            # Ensure result is a list of strings
            if isinstance(result, list) and all(isinstance(item, str) for item in result):
                return result
            else:
                return []
        return []
    except Exception as e:
        log(f"‚ö†Ô∏è Query rephrasing failed: {e}")
        return []
