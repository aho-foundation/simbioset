"""
LLM API Proxy Client - Provides LLM access through proxy.

–†–µ—Ç—Ä–∞–∏ –∏ –æ—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –≤—ã–ø–æ–ª–Ω—è—é—Ç—Å—è –≤ –ø—Ä–æ–∫—Å–∏.
"""

import asyncio
import json
import re
from typing import List, Optional, Dict, Any
from openai import AsyncOpenAI
from api.logger import root_logger
from api.settings import LLM_PROXY_URL, LLM_PROXY_TOKEN
import aiohttp

log = root_logger.info

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∏–µ–Ω—Ç–æ–≤ –¥–ª—è –ø—Ä–æ–∫—Å–∏
_openai_client: Optional[AsyncOpenAI] = None
_http_session: Optional[aiohttp.ClientSession] = None


def get_llm_client() -> AsyncOpenAI:
    """
    –ü–æ–ª—É—á–∞–µ—Ç OpenAI –∫–ª–∏–µ–Ω—Ç –¥–ª—è –ø—Ä–æ–∫—Å–∏ (–ø—É–±–ª–∏—á–Ω—ã–π API).

    Returns:
        AsyncOpenAI –∫–ª–∏–µ–Ω—Ç, –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –ø—Ä–æ–∫—Å–∏
    """
    return _get_openai_client()


def _get_openai_client() -> AsyncOpenAI:
    """–ü–æ–ª—É—á–∞–µ—Ç –∏–ª–∏ —Å–æ–∑–¥–∞–µ—Ç OpenAI –∫–ª–∏–µ–Ω—Ç –¥–ª—è –ø—Ä–æ–∫—Å–∏."""
    global _openai_client
    if _openai_client is None:
        _openai_client = AsyncOpenAI(
            base_url=f"{LLM_PROXY_URL}/v1",
            api_key=LLM_PROXY_TOKEN or "not-needed",  # OpenAI SDK —Ç—Ä–µ–±—É–µ—Ç api_key, –¥–∞–∂–µ –µ—Å–ª–∏ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è
            timeout=120.0,
        )
    return _openai_client


def _get_http_session() -> aiohttp.ClientSession:
    """–ü–æ–ª—É—á–∞–µ—Ç –∏–ª–∏ —Å–æ–∑–¥–∞–µ—Ç aiohttp —Å–µ—Å—Å–∏—é –¥–ª—è –ø—Ä—è–º—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ –ø—Ä–æ–∫—Å–∏."""
    global _http_session
    if _http_session is None:
        # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Ç–∞–π–º–∞—É—Ç –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ LLM
        timeout = aiohttp.ClientTimeout(total=180.0, connect=30.0)
        _http_session = aiohttp.ClientSession(
            timeout=timeout,
        )
    return _http_session


async def close_llm_clients() -> None:
    """
    –ó–∞–∫—Ä—ã–≤–∞–µ—Ç –≤—Å–µ –∫–ª–∏–µ–Ω—Ç—ã LLM (aiohttp —Å–µ—Å—Å–∏—é).
    –í—ã–∑—ã–≤–∞–µ—Ç—Å—è –ø—Ä–∏ shutdown –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è.
    """
    global _http_session
    if _http_session is not None:
        await _http_session.close()
        _http_session = None
        log("‚úÖ LLM HTTP session closed")


# --- LLM Exceptions ---
class LLMError(Exception):
    """Base exception for LLM errors"""

    pass


class LLMTemporaryError(LLMError):
    """Temporary error, can be retried"""

    pass


class LLMPermanentError(LLMError):
    """Permanent error, retry is useless"""

    pass


async def _call_proxy_api(
    messages: List[dict],
    model: Optional[str] = None,
) -> str:
    """
    –í—ã–∑—ã–≤–∞–µ—Ç –ø—Ä–æ–∫—Å–∏-—Å–µ—Ä–≤–∏—Å –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –æ—Ç–≤–µ—Ç–∞ –æ—Ç LLM.

    Args:
        messages: –°–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π –≤ —Ñ–æ—Ä–º–∞—Ç–µ OpenAI API
        model: –ò–º—è –º–æ–¥–µ–ª–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω–æ - –≤—ã–±–∏—Ä–∞–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)

    Returns:
        –¢–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–∞ –æ—Ç LLM

    Raises:
        LLMTemporaryError: –ü—Ä–∏ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ—à–∏–±–∫–∞—Ö
        LLMPermanentError: –ü—Ä–∏ –ø–æ—Å—Ç–æ—è–Ω–Ω—ã—Ö –æ—à–∏–±–∫–∞—Ö (–∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –∏ —Ç.–¥.)
    """
    session = _get_http_session()
    url = f"{LLM_PROXY_URL}/v1/chat/completions"

    try:
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Ç–µ–ª–æ –∑–∞–ø—Ä–æ—Å–∞
        body: Dict[str, Any] = {
            "messages": messages,
            "stream": False,
        }

        # –î–æ–±–∞–≤–ª—è–µ–º –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        if model:
            body["model"] = model

        # –§–æ—Ä–º–∏—Ä—É–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ —Å —Ç–æ–∫–µ–Ω–æ–º –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏
        headers: Dict[str, str] = {}
        if LLM_PROXY_TOKEN:
            headers["Authorization"] = f"Bearer {LLM_PROXY_TOKEN}"
        else:
            log("‚ö†Ô∏è LLM_PROXY_TOKEN –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –∑–∞–ø—Ä–æ—Å –º–æ–∂–µ—Ç –±—ã—Ç—å –æ—Ç–∫–ª–æ–Ω–µ–Ω")

        # –í—ã–ø–æ–ª–Ω—è–µ–º –∑–∞–ø—Ä–æ—Å
        async with session.post(url, json=body, headers=headers) as response:
            status_code = response.status

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ç—É—Å –æ—Ç–≤–µ—Ç–∞
            if status_code == 401:
                raise LLMPermanentError(f"Authorization error: HTTP {status_code}")
            elif status_code in [503, 504]:  # Service unavailable, Gateway timeout
                raise LLMTemporaryError(f"Service unavailable: HTTP {status_code}")
            elif status_code >= 500:
                raise LLMTemporaryError(f"Server error: HTTP {status_code}")
            elif status_code >= 400:
                raise LLMPermanentError(f"HTTP error {status_code}")

            data = await response.json()

        if not data.get("choices") or len(data["choices"]) == 0:
            raise LLMTemporaryError("Empty response from proxy: no choices returned")

        reply = data["choices"][0].get("message", {}).get("content", "")

        if not reply or len(reply.strip()) < 10:
            raise LLMTemporaryError("Got empty or too short response from proxy")

        return reply.strip()

    except aiohttp.ClientResponseError as e:
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ HTTP –æ—à–∏–±–æ–∫ –æ—Ç aiohttp
        status_code = e.status
        if status_code == 401:
            raise LLMPermanentError(f"Authorization error: {e}")
        elif status_code == 503:
            raise LLMTemporaryError(f"Service unavailable: {e}")
        elif status_code >= 500:
            raise LLMTemporaryError(f"Server error: {e}")
        else:
            raise LLMPermanentError(f"HTTP error {status_code}: {e}")

    except (aiohttp.ServerTimeoutError, asyncio.TimeoutError) as e:
        raise LLMTemporaryError(f"Request timeout: {e}")

    except aiohttp.ClientError as e:
        raise LLMTemporaryError(f"Network error: {e}")

    except Exception as e:
        # –ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–µ –æ—à–∏–±–∫–∏
        error_msg = str(e).lower()
        if "api_key" in error_msg or "authorization" in error_msg or "401" in error_msg:
            raise LLMPermanentError(f"Authorization error: {e}")
        raise LLMTemporaryError(f"Unexpected error: {e}")


async def call_llm(
    llm_context: str,
    *,
    origin: str | None = None,
    model: Optional[str] = None,
) -> str:
    """
    –í—ã–∑—ã–≤–∞–µ—Ç LLM —á–µ—Ä–µ–∑ –ø—Ä–æ–∫—Å–∏.

    –†–µ—Ç—Ä–∞–∏ –≤—ã–ø–æ–ª–Ω—è—é—Ç—Å—è –≤ –ø—Ä–æ–∫—Å–∏, –∑–¥–µ—Å—å —Ç–æ–ª—å–∫–æ –ø—Ä–æ—Å—Ç–æ–π –≤—ã–∑–æ–≤.

    Args:
        llm_context: –ö–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è LLM
        origin: –¢–µ–≥ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
        model: –ò–º—è –º–æ–¥–µ–ª–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω–æ - –≤—ã–±–∏—Ä–∞–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)

    Returns:
        –û—Ç–≤–µ—Ç –æ—Ç LLM

    Raises:
        LLMPermanentError: –ü—Ä–∏ –Ω–µ–≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏–º—ã—Ö –æ—à–∏–±–∫–∞—Ö
        LLMTemporaryError: –ü—Ä–∏ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ—à–∏–±–∫–∞—Ö
    """
    # –õ–æ–≥–∏—Ä—É–µ–º –≤—Ö–æ–¥—è—â–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ (–∫–æ—Ä–æ—Ç–∫–∏–π –ø—Ä–µ–≤—å—é, –±–µ–∑ –¥–∞–º–ø–∞ –≤—Å–µ–≥–æ —Ç–µ–∫—Å—Ç–∞)
    preview = llm_context.strip().replace("\n", " ")
    if len(preview) > 200:
        preview = preview[:200] + "‚Ä¶"
    origin_tag = origin or "generic"
    log(f"üìù LLM request origin={origin_tag} context_len={len(llm_context)} preview='{preview}'")

    # –í—ã–∑—ã–≤–∞–µ–º –ø—Ä–æ–∫—Å–∏-—Å–µ—Ä–≤–∏—Å –±–µ–∑ —É–∫–∞–∑–∞–Ω–∏—è –º–æ–¥–µ–ª–∏ - –ø—Ä–æ–∫—Å–∏ –≤—ã–±–µ—Ä–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
    # –†–µ—Ç—Ä–∞–∏ –≤—ã–ø–æ–ª–Ω—è—é—Ç—Å—è –≤ –ø—Ä–æ–∫—Å–∏
    log(f"ü§ñ Generating reply via proxy {LLM_PROXY_URL} (origin={origin_tag})")
    messages = [{"role": "user", "content": llm_context}]
    reply = await _call_proxy_api(messages, model=model)

    log(f"‚úÖ reply successfully generated ({len(reply)} characters)")
    return reply


async def list_models() -> List[Dict[str, Any]]:
    """
    –ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∏–∑ –ø—Ä–æ–∫—Å–∏.

    Returns:
        –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –º–æ–¥–µ–ª—è—Ö

    Raises:
        LLMPermanentError: –ü—Ä–∏ –æ—à–∏–±–∫–∞—Ö –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏
        LLMTemporaryError: –ü—Ä–∏ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ—à–∏–±–∫–∞—Ö
    """
    session = _get_http_session()
    url = f"{LLM_PROXY_URL}/v1/models"

    try:
        async with session.get(url) as response:
            status_code = response.status

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ç—É—Å –æ—Ç–≤–µ—Ç–∞
            if status_code == 401:
                raise LLMPermanentError(f"Authorization error: HTTP {status_code}")
            elif status_code >= 500:
                raise LLMTemporaryError(f"Server error: HTTP {status_code}")
            elif status_code >= 400:
                raise LLMPermanentError(f"HTTP error {status_code}")

            data = await response.json()

        return data.get("data", [])

    except aiohttp.ClientResponseError as e:
        status_code = e.status
        if status_code == 401:
            raise LLMPermanentError(f"Authorization error: {e}")
        elif status_code >= 500:
            raise LLMTemporaryError(f"Server error: {e}")
        else:
            raise LLMPermanentError(f"HTTP error {status_code}: {e}")

    except (aiohttp.ServerTimeoutError, asyncio.TimeoutError) as e:
        raise LLMTemporaryError(f"Request timeout: {e}")

    except aiohttp.ClientError as e:
        raise LLMTemporaryError(f"Network error: {e}")

    except Exception as e:
        raise LLMTemporaryError(f"Unexpected error: {e}")


class LLMClientWrapper:
    """
    –û–±–µ—Ä—Ç–∫–∞ –Ω–∞–¥ OpenAI –∫–ª–∏–µ–Ω—Ç–æ–º –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å –∫–æ–¥–æ–º,
    –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–µ—Ç–æ–¥ generate().
    """

    async def generate(self, prompt: str) -> str:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –Ω–∞ –ø—Ä–æ–º–ø—Ç —á–µ—Ä–µ–∑ LLM.

        Args:
            prompt: –¢–µ–∫—Å—Ç –ø—Ä–æ–º–ø—Ç–∞

        Returns:
            –û—Ç–≤–µ—Ç –æ—Ç LLM
        """
        return await call_llm(prompt, origin="llm_client_wrapper")


def get_llm_client_wrapper() -> LLMClientWrapper:
    """
    –ü–æ–ª—É—á–∞–µ—Ç –æ–±–µ—Ä—Ç–∫—É –Ω–∞–¥ LLM –∫–ª–∏–µ–Ω—Ç–æ–º —Å –º–µ—Ç–æ–¥–æ–º generate().
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å –∫–æ–¥–æ–º, –∫–æ—Ç–æ—Ä—ã–π –æ–∂–∏–¥–∞–µ—Ç –º–µ—Ç–æ–¥ generate().

    Returns:
        LLMClientWrapper —Å –º–µ—Ç–æ–¥–æ–º generate()
    """
    return LLMClientWrapper()
