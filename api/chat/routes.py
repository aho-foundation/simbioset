"""FastAPI routes for Chat API."""

import time
from datetime import datetime
from typing import List, Dict, Optional

from fastapi import APIRouter, HTTPException, Request, Response, status, Body
from pydantic import BaseModel

from api.logger import root_logger as logger
from api.detect.web_search import web_search_service
from api.chat.models import ChatSession, ChatSessionCreate, ChatMessageCreate, ChatDragToChat
from api.chat.service import chat_session_service, generate_starters


class LocalizeRequest(BaseModel):
    sessionId: str
    latitude: float
    longitude: float
    conversationText: str
    action: Optional[str] = "localize"  # "localize", "expand_context", "new_branch"


from api.kb.models import ConceptNode
from api.kb.service import KBService
from api.storage.paragraph_service import ParagraphService
from api.llm import LLMPermanentError, LLMTemporaryError, call_llm
from api.sessions import session_manager
from api.logger import root_logger
import re

log = root_logger.debug
from api.chat.context_builder import (
    build_context_for_llm,
    should_include_context,
    get_weather_context,
    extract_ecosystem_and_location,
    format_ecosystem_context,
)
from api.detect.localize import reverse_geocode_location

router = APIRouter(prefix="/api/chat", tags=["Chat"])

# Services will be injected from app.state in route handlers

# –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è LLM –æ—Ç–≤–µ—Ç–æ–≤ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –≤ –ø—Ä–æ–∫—Å–∏ (llm_proxy)
# –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∏—Å–∫–∞–∂–µ–Ω–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –≤ call_llm


def remove_sources_section_from_content(content: str) -> str:
    """
    –£–¥–∞–ª—è–µ—Ç —Ä–∞–∑–¥–µ–ª "## –ò—Å—Ç–æ—á–Ω–∏–∫–∏" –∏–∑ —Ç–µ–∫—Å—Ç–∞ –æ—Ç–≤–µ—Ç–∞ LLM.

    –≠—Ç–æ –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ - –æ–Ω–∏ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç—Å—è –æ—Ç–¥–µ–ª—å–Ω–æ,
    –∞ –Ω–µ –≤ —Ç–µ–∫—Å—Ç–µ —Å–æ–æ–±—â–µ–Ω–∏—è.

    Args:
        content: –ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–∞ LLM

    Returns:
        –¢–µ–∫—Å—Ç –±–µ–∑ —Ä–∞–∑–¥–µ–ª–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
    """
    # –£–¥–∞–ª—è–µ–º —Ä–∞–∑–¥–µ–ª "## –ò—Å—Ç–æ—á–Ω–∏–∫–∏" –≤ —Ä–∞–∑–Ω—ã—Ö –≤–∞—Ä–∏–∞–Ω—Ç–∞—Ö:
    # - ## –ò—Å—Ç–æ—á–Ω–∏–∫–∏
    # - ## –ò—Å—Ç–æ—á–Ω–∏–∫–∏:
    # - ### –ò—Å—Ç–æ—á–Ω–∏–∫–∏
    # - –° –ø–µ—Ä–µ–Ω–æ—Å–æ–º —Å—Ç—Ä–æ–∫–∏ –∏–ª–∏ –±–µ–∑
    patterns = [
        # –ó–∞–≥–æ–ª–æ–≤–æ–∫ —Å ## –∏–ª–∏ ###, —Å –¥–≤–æ–µ—Ç–æ—á–∏–µ–º –∏–ª–∏ –±–µ–∑, –∏ –≤—Å–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –¥–æ —Å–ª–µ–¥—É—é—â–µ–≥–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞ –∏–ª–∏ –∫–æ–Ω—Ü–∞
        r"\n?##+\s*–ò—Å—Ç–æ—á–Ω–∏–∫–∏:?\s*\n.*?(?=\n?##+|\Z)",
        # –ó–∞–≥–æ–ª–æ–≤–æ–∫ –≤ –Ω–∞—á–∞–ª–µ —Å—Ç—Ä–æ–∫–∏
        r"^##+\s*–ò—Å—Ç–æ—á–Ω–∏–∫–∏:?\s*\n.*?(?=\n?##+|\Z)",
        # –ó–∞–≥–æ–ª–æ–≤–æ–∫ –≤ —Å–µ—Ä–µ–¥–∏–Ω–µ —Ç–µ–∫—Å—Ç–∞
        r"\n##+\s*–ò—Å—Ç–æ—á–Ω–∏–∫–∏:?\s*\n.*?(?=\n##+|\n###+|\Z)",
    ]

    cleaned_content = content
    for pattern in patterns:
        cleaned_content = re.sub(pattern, "", cleaned_content, flags=re.DOTALL | re.IGNORECASE | re.MULTILINE)

    # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏ –≤ –∫–æ–Ω—Ü–µ
    return cleaned_content.strip()


def parse_sources_from_response(response_content: str) -> List[Dict[str, str]]:
    """
    –ü–∞—Ä—Å–∏—Ç –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –∏–∑ –æ—Ç–≤–µ—Ç–∞ LLM —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ —Ñ–∞–ª–ª–±–µ–∫–∞–º–∏.

    –°–Ω–∞—á–∞–ª–∞ –∏—â–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–∞–∑–¥–µ–ª "## –ò—Å—Ç–æ—á–Ω–∏–∫–∏", –∑–∞—Ç–µ–º –ø—ã—Ç–∞–µ—Ç—Å—è
    –∏–∑–≤–ª–µ—á—å –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞ –≤ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–∞—Ö.

    Args:
        response_content: –ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–∞ LLM

    Returns:
        –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏ [{'title': '–ù–∞–∑–≤–∞–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∞', 'type': '–¢–∏–ø –∏—Å—Ç–æ—á–Ω–∏–∫–∞', 'url': '—Å—Å—ã–ª–∫–∞'}]
    """
    sources: List[Dict[str, str]] = []

    # –§–ê–õ–õ–ë–ï–ö 1: –ò—â–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–∞–∑–¥–µ–ª "## –ò—Å—Ç–æ—á–Ω–∏–∫–∏"
    sources_match = re.search(
        r"##\s*–ò—Å—Ç–æ—á–Ω–∏–∫–∏?\s*\n(.*?)(?=\n##|\n###|\Z)", response_content, re.DOTALL | re.IGNORECASE
    )

    if sources_match:
        sources_text = sources_match.group(1).strip()
        # –ü–∞—Ä—Å–∏–º –ø—Ä–æ–Ω—É–º–µ—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        lines = sources_text.split("\n")
        for line in lines:
            line = line.strip()
            if not line:
                continue

            # –£–±–∏—Ä–∞–µ–º –Ω—É–º–µ—Ä–∞—Ü–∏—é –≤ –Ω–∞—á–∞–ª–µ —Å—Ç—Ä–æ–∫–∏ (1., 2., etc.)
            line = re.sub(r"^\d+\.\s*", "", line)

            # –ò—â–µ–º URL –≤ —Å—Ç—Ä–æ–∫–µ (https://... –∏–ª–∏ http://...)
            url_match = re.search(r"(https?://[^\s]+)", line)
            url = url_match.group(1) if url_match else None

            # –£–±–∏—Ä–∞–µ–º URL –∏–∑ —Å—Ç—Ä–æ–∫–∏ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –Ω–∞–∑–≤–∞–Ω–∏—è –∏ —Ç–∏–ø–∞
            line_without_url = re.sub(r"\s*https?://[^\s]+\s*", "", line).strip()

            # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –∏ —Ç–∏–ø (–µ—Å–ª–∏ –µ—Å—Ç—å –≤ —Å–∫–æ–±–∫–∞—Ö)
            type_match = re.search(r"\(([^)]+)\)$", line_without_url)
            if type_match:
                title = line_without_url[: type_match.start()].strip()
                source_type = type_match.group(1).strip()
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏ —Å —Ç–∏–ø–æ–º "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø"
                if source_type.lower() in ("–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø", "unknown type", "unknown"):
                    continue
            else:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ —ç–º–æ–¥–∂–∏ –≤ –∫–æ–Ω—Ü–µ —Å—Ç—Ä–æ–∫–∏ (–ø–æ—Å–ª–µ –Ω–∞–∑–≤–∞–Ω–∏—è)
                emoji_match = re.search(r"([^\w\s])\s*$", line_without_url)
                if emoji_match:
                    title = line_without_url[: emoji_match.start()].strip()
                    source_type = emoji_match.group(1).strip()
                else:
                    # –ï—Å–ª–∏ —Ç–∏–ø –Ω–µ —É–∫–∞–∑–∞–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫
                    continue

            # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ —Å –≤–∞–ª–∏–¥–Ω—ã–º –Ω–∞–∑–≤–∞–Ω–∏–µ–º –∏ —Ç–∏–ø–æ–º
            if title and source_type:
                source_dict = {"title": title, "type": source_type}
                if url:
                    source_dict["url"] = url
                sources.append(source_dict)

    # –§–ê–õ–õ–ë–ï–ö 2: –ò—â–µ–º —É–ø–æ–º–∏–Ω–∞–Ω–∏—è —Å—Å—ã–ª–æ–∫ –∏ –¥–æ–º–µ–Ω–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ
    if not sources:
        # –ò—â–µ–º –≤—Å–µ URL –≤ —Ç–µ–∫—Å—Ç–µ
        url_pattern = r'https?://[^\s<>"{}|\\^`\[\]]+'
        found_urls = re.findall(url_pattern, response_content)

        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º URL –ø–æ –¥–æ–º–µ–Ω–∞–º –∏ —Å–æ–∑–¥–∞–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏
        domain_sources = {}
        for url in found_urls[:5]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ 5 –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
            try:
                from urllib.parse import urlparse

                domain = urlparse(url).netloc
                if domain and domain not in domain_sources:
                    domain_sources[domain] = url
            except:
                continue

        # –°–æ–∑–¥–∞–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –∏–∑ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–æ–º–µ–Ω–æ–≤
        for domain, url in domain_sources.items():
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –ø–æ –¥–æ–º–µ–Ω—É
            if "wikipedia" in domain:
                source_type = "üìö"
                title = f"–í–∏–∫–∏–ø–µ–¥–∏—è - {domain}"
            elif "scholar.google" in domain:
                source_type = "üìÑ"
                title = "Google Scholar"
            elif "pubmed" in domain or "nih.gov" in domain:
                source_type = "üî¨"
                title = "PubMed/NCBI"
            elif "researchgate" in domain:
                source_type = "üìÑ"
                title = "ResearchGate"
            elif "arxiv" in domain:
                source_type = "üìÑ"
                title = "arXiv"
            else:
                source_type = "üåê"
                title = f"–í–µ–±-—Ä–µ—Å—É—Ä—Å - {domain}"

            sources.append({"title": title, "type": source_type, "url": url})

    # –§–ê–õ–õ–ë–ï–ö 3: –ò—â–µ–º —É–ø–æ–º–∏–Ω–∞–Ω–∏—è —Ç–∏–ø–æ–≤ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ (–µ—Å–ª–∏ –Ω–µ—Ç URL)
    if not sources:
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∏–∑–≤–µ—Å—Ç–Ω—ã–µ —Ç–∏–ø—ã –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        source_type_patterns = {
            "–Ω–∞—É—á–Ω–∞—è –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞": "–ù–∞—É—á–Ω–∞—è –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞",
            "scientific literature": "–ù–∞—É—á–Ω–∞—è –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞",
            "–≤–µ–±-–ø–æ–∏—Å–∫": "–í–µ–±-–ø–æ–∏—Å–∫",
            "web search": "–í–µ–±-–ø–æ–∏—Å–∫",
            "–±–∞–∑–∞ –∑–Ω–∞–Ω–∏–π": "–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π",
            "knowledge base": "–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π",
            "–Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å": "–ù–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å",
            "neural network": "–ù–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å",
            "—ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–µ –∑–Ω–∞–Ω–∏—è": "–≠–∫—Å–ø–µ—Ä—Ç–Ω—ã–µ –∑–Ω–∞–Ω–∏—è",
            "expert knowledge": "–≠–∫—Å–ø–µ—Ä—Ç–Ω—ã–µ –∑–Ω–∞–Ω–∏—è",
            "–ø—É–±–ª–∏–∫–∞—Ü–∏—è": "–ü—É–±–ª–∏–∫–∞—Ü–∏—è",
            "publication": "–ü—É–±–ª–∏–∫–∞—Ü–∏—è",
            "–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ",
            "research": "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ",
        }

        found_types = []
        content_lower = response_content.lower()

        for pattern, source_type in source_type_patterns.items():
            if pattern in content_lower and source_type not in found_types:
                found_types.append(source_type)

        # –°–æ–∑–¥–∞–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Ç–∏–ø–æ–≤ —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å —è–≤–Ω—ã–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è
        for source_type in found_types:
            title = f"{source_type}"
            sources.append({"title": title, "type": source_type})

    # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ (–Ω–µ –¥–æ–±–∞–≤–ª—è–µ–º —Ñ–µ–π–∫–æ–≤—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏)

    return sources


@router.post("/session", response_model=ChatSession)
async def create_chat_session(session_data: ChatSessionCreate):
    """
    Create a new chat session.

    Creates a new chat session.
    """
    try:
        # Create a new chat session
        session = chat_session_service.create_session(session_data)

        # If ecosystem data is provided, set up the session location
        if session_data.ecosystem:
            ecosystem_data = {
                "location": session_data.ecosystem.get("coordinates", {}),
                "ecosystems": [
                    {"name": session_data.ecosystem.get("type", "–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è —ç–∫–æ—Å–∏—Å—Ç–µ–º–∞"), "scale": "regional"}
                ],
                "coordinates": session_data.ecosystem.get("coordinates"),
                "time_reference": None,
                "source": "branch_creation",
                "parent_session_id": session_data.ecosystem.get("parentSessionId"),
                "artifacts_summary": session_data.ecosystem.get("artifactsSummary"),
            }
            chat_session_service.update_session_location(session.id, ecosystem_data)

        return session
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail={"error": "InternalError", "message": str(e)},
        )


@router.post("/message", response_model=dict)
async def send_chat_message(message_data: ChatMessageCreate, request: Request, response: Response):
    """
    Send a message in a chat session.

    Adds the message to the concept tree and potentially generates a response.
    Automatically creates user session if not exists.
    """
    # Check if the message is about symbiosis or related topics to trigger web search
    message_lower = message_data.message.lower()
    trigger_keywords = ["—Å–∏–º–±–∏–æ–∑", "symbiosis", "symbiotic", "—ç–∫–æ—Å–∏—Å—Ç–µ–º–∞", "ecosystem", "–≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ", "interaction"]
    needs_web_search = any(keyword in message_lower for keyword in trigger_keywords)
    try:
        # Initialize session_id to None to avoid UnboundLocalError
        session_id = None

        # Check if this is a Telegram request with user ID
        telegram_user_id = message_data.context.get("telegram_user_id") if message_data.context else None

        if telegram_user_id:
            # Use persistent session for Telegram user (async)
            session_id = await session_manager.get_or_create_telegram_session(telegram_user_id, message_data.author)
            user_session = await session_manager.get_session(session_id)
        else:
            # Use sessionId from request body if provided, otherwise from cookies
            if message_data.sessionId is not None:
                session_id = message_data.sessionId
                user_session = await session_manager.get_session(session_id)
            else:
                # Try to get session_id from cookies first
                session_id = request.cookies.get("session_id")

                if session_id:
                    # If we have session_id from cookies, get the session (async)
                    user_session = await session_manager.get_session(session_id)
                else:
                    # No session in cookies, create a new one
                    user_session = None

                if not user_session:
                    # Create new session if none exists (async)
                    session_id = await session_manager.create_session(
                        {
                            "user_id": message_data.author,
                            "created_at": "auto",
                            "message_count": 0,
                            "last_activity": datetime.now(),
                        }
                    )
                    response.set_cookie("session_id", session_id, httponly=True, max_age=2592000)  # 30 days
                    user_session = await session_manager.get_session(session_id)

        # Update session activity (async)
        if user_session and session_id:
            # Use the correct session_id for updates
            await session_manager.increment_message_count(session_id)

        # Get services from app.state (injected at startup)
        service: KBService = request.app.state.kb_service
        paragraph_service: ParagraphService = request.app.state.paragraph_service

        # Verify the session exists, create if not found or default
        # Note: ChatSession —É–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è —á–µ—Ä–µ–∑ chat_session_service, –Ω–µ —á–µ—Ä–µ–∑ ConceptNode
        # ConceptNode –º–æ–∂–µ—Ç –∏–º–µ—Ç—å sessionId, –Ω–æ —Å–∞–º –Ω–µ —è–≤–ª—è–µ—Ç—Å—è —Å–µ—Å—Å–∏–µ–π
        session_node = service.get_node(session_id)  # type: ignore
        if not session_node or session_id == "default-session":
            # Create root node for this chat session (system message, not a session itself)
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–∫—Å—Ç –ø–µ—Ä–≤–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è –∫–∞–∫ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ —É–∑–ª–∞
            session_node = service.add_concept(
                parent_id=None,
                content=message_data.message,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ä—Ç–µ—Ä–∞/–ø–µ—Ä–≤–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è
                role="system",
                node_type="message",  # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ: –Ω–µ chat_session, –∞ message —Å role=system
                session_id=None,  # Root node doesn't have sessionId
                category="neutral",
            )
            # Use the new node ID as session identifier
            actual_session_id = session_node.id
        else:
            actual_session_id = session_id  # type: ignore

        # Update chat session message count
        chat_session_service.increment_message_count(actual_session_id)

        # Link chat session to user session if not already linked (async)
        if session_id and user_session and not user_session.get("chat_session_id"):
            user_session["chat_session_id"] = actual_session_id
            await session_manager.update_session(session_id, user_session)

        # Add the user message to the concept tree
        user_message_node = service.add_concept(
            parent_id=actual_session_id,
            content=message_data.message,
            role="user",
            node_type="message",
            session_id=actual_session_id,
        )

        # Parse message to paragraphs and save for FAISS search
        try:
            await paragraph_service.save_chat_message_paragraphs(
                message_content=message_data.message,
                session_id=actual_session_id,
                author=message_data.author,
                author_id=None,  # TODO: Add author_id to ChatMessageCreate model
                timestamp=user_message_node.timestamp,
            )
        except Exception as e:
            # Log error but don't fail the request
            print(f"Error saving message paragraphs: {e}")

        # Load the simbio expert prompt template
        with open("api/prompts/simbio_expert.txt", "r", encoding="utf-8") as f:
            prompt_template = f.read()

        # Prepare context data for the prompt
        context = message_data.context or {}
        local_stat = context.get("local_stat", "")

        # Weaviate handles semantic search for all internal knowledge (artifacts, chats, books) automatically
        # Only external sources (web, books) need explicit search
        websearch_context = await web_search_service.get_symbiosis_context(message_data.message)
        books_search_context = await search_books_for_message(
            message_data.message, web_search_service, actual_session_id
        )

        # Combine only external sources (web and books)
        external_sources = "\n".join(filter(None, [websearch_context, books_search_context]))

        # User observation extraction is now handled by LLM instead of keyword matching

        # –ü–æ–ª—É—á–∞–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—É—é –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏—é —Å–µ—Å—Å–∏–∏
        chat_session = chat_session_service.get_session(actual_session_id)
        session_location_data = chat_session.location if chat_session and chat_session.location else None

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ª–æ–∫–∞—Ü–∏—é –∏–∑ —Ç–µ–∫—Å—Ç–∞ —Å–æ–æ–±—â–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (–µ—Å–ª–∏ —è–≤–Ω–æ —É–ø–æ–º—è–Ω—É—Ç–∞)
        user_location_data = None
        if message_data.author != "assistant":  # –¢–æ–ª—å–∫–æ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö —Å–æ–æ–±—â–µ–Ω–∏–π
            try:
                user_location_data = await extract_ecosystem_and_location(message_data.message)
                if user_location_data.get("location"):
                    logger.info(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –ª–æ–∫–∞—Ü–∏—è –≤ —Å–æ–æ–±—â–µ–Ω–∏–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {user_location_data.get('location')}")

                    # –í—ã–ø–æ–ª–Ω—è–µ–º –æ–±—Ä–∞—Ç–Ω–æ–µ –≥–µ–æ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç
                    if user_location_data.get("location"):
                        try:
                            from api.detect.localize import geocode

                            coordinates = geocode(user_location_data["location"])
                            if coordinates:
                                user_location_data["coordinates"] = {
                                    "latitude": coordinates[0],
                                    "longitude": coordinates[1],
                                }
                                logger.info(
                                    f"–ì–µ–æ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω—ã –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –¥–ª—è –ª–æ–∫–∞—Ü–∏–∏ {user_location_data['location']}: {coordinates}"
                                )
                        except Exception as e:
                            logger.warning(
                                f"–ù–µ —É–¥–∞–ª–æ—Å—å –≥–µ–æ–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å –ª–æ–∫–∞—Ü–∏—é {user_location_data.get('location')}: {e}"
                            )

            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –ª–æ–∫–∞—Ü–∏–∏ –≤ —Å–æ–æ–±—â–µ–Ω–∏–∏: {e}")

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—É—é –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏—é —Å–µ—Å—Å–∏–∏ –∏–ª–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—É—é –∏–∑ —Å–æ–æ–±—â–µ–Ω–∏—è
        location = None
        ecosystems = []
        if session_location_data:
            # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–π –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏ —Å–µ—Å—Å–∏–∏
            location = session_location_data.get("location")
            ecosystems = session_location_data.get("ecosystems", [])
            logger.info(
                f"–ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—É—é –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏—é —Å–µ—Å—Å–∏–∏: location={location}, ecosystems={[e.get('name') for e in ecosystems]}"
            )
        elif user_location_data and user_location_data.get("location"):
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ª–æ–∫–∞—Ü–∏—é –∏–∑ —Ç–µ–∫—É—â–µ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è
            location = user_location_data.get("location")
            ecosystems = user_location_data.get("ecosystems", [])

            # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º —ç—Ç—É –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏—é –≤ —Å–µ—Å—Å–∏–∏
            chat_session_service.update_session_location(
                actual_session_id,
                {
                    "location": location,
                    "ecosystems": ecosystems,
                    "coordinates": user_location_data.get("coordinates"),
                    "time_reference": user_location_data.get("time_reference"),
                    "source": "message_analysis",  # –û—Ç–º–µ—á–∞–µ–º, —á—Ç–æ –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞ –∏–∑ –∞–Ω–∞–ª–∏–∑–∞ —Å–æ–æ–±—â–µ–Ω–∏—è
                },
            )
            logger.info(
                f"–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏—è –∏–∑ —Å–æ–æ–±—â–µ–Ω–∏—è: location={location}, ecosystems={[e.get('name') for e in ecosystems]}"
            )

        conversation_summary, recent_messages = await build_context_for_llm(
            actual_session_id, service, location=location, ecosystems=ecosystems
        )

        # Context sections are included if they have content (KISS: no conditional logic needed)

        # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–æ–≥–æ–¥–µ, –µ—Å–ª–∏ –≤ —Å–æ–æ–±—â–µ–Ω–∏–∏ —É–∫–∞–∑–∞–Ω—ã –≥–æ—Ä–æ–¥ –∏ –≤—Ä–µ–º—è
        weather_context = await get_weather_context(message_data.message)

        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –ª–æ–∫–∞–ª—å–Ω–æ–π —ç–∫–æ—Å–∏—Å—Ç–µ–º—ã (–æ–±—ä–µ–¥–∏–Ω—è–µ–º –ø–æ–≥–æ–¥—É –∏ —ç–∫–æ—Å–∏—Å—Ç–µ–º—É)
        ecosystem_context = format_ecosystem_context(ecosystems, location, weather=weather_context)

        # –ü–æ–ª—É—á–∞–µ–º –≥—Ä–∞—Ñ–æ–≤—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç —á–µ—Ä–µ–∑ —Å–∏–º–±–∏–æ—Ç–∏—á–µ—Å–∫–∏–µ —Å–≤—è–∑–∏
        # –ù–ï —Å—Ç—Ä–æ–∏–º –≥—Ä–∞—Ñ–æ–≤—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –ø–µ—Ä–≤—ã—Ö 2 —Å–æ–æ–±—â–µ–Ω–∏–π (—Å—Ç–∞—Ä—Ç–µ—Ä + –æ—Ç–≤–µ—Ç) - —ç—Ç–æ —Å–ª–∏—à–∫–æ–º —Ä–∞–Ω–æ
        graph_context = ""
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–æ–±—â–µ–Ω–∏–π –≤ —Å–µ—Å—Å–∏–∏
            session_messages = service.get_session_messages(actual_session_id)
            message_count = len(session_messages) if session_messages else 0

            # –°—Ç—Ä–æ–∏–º –≥—Ä–∞—Ñ–æ–≤—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –≤ —Å–µ—Å—Å–∏–∏ –±–æ–ª—å—à–µ 2 —Å–æ–æ–±—â–µ–Ω–∏–π
            # (—Å—Ç–∞—Ä—Ç–µ—Ä + –æ—Ç–≤–µ—Ç = 2 —Å–æ–æ–±—â–µ–Ω–∏—è, –ø–æ—Å–ª–µ —ç—Ç–æ–≥–æ —É–∂–µ –º–æ–∂–Ω–æ —Å—Ç—Ä–æ–∏—Ç—å –≥—Ä–∞—Ñ)
            if message_count > 2:
                from api.chat.context_builder import build_graph_context

                # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ (–º–æ–∂–µ—Ç –±—ã—Ç—å FAISS –∏–ª–∏ Weaviate)
                storage = getattr(request.app.state, "storage", None) or getattr(
                    request.app.state, "faiss_storage", None
                )
                graph_context = await build_graph_context(
                    message=message_data.message,
                    session_id=actual_session_id,
                    db_manager=request.app.state.db_manager,
                    storage=storage,
                    max_depth=2,
                    max_relationships=10,
                )
        except Exception as e:
            print(f"Error building graph context: {e}")

        # –ü–æ–ª—É—á–∞–µ–º —Å–∏–º–±–∏–æ–Ω—Ç–æ–≤ –¥–ª—è –≤–∫–ª—é—á–µ–Ω–∏—è –≤ ecosystem_context
        symbionts = []
        try:
            from api.storage.weaviate_storage import WeaviateStorage
            from api.storage.symbiont_service import SymbiontService

            weaviate_storage = WeaviateStorage()
            symbiont_service = SymbiontService(weaviate_storage)
            symbionts = await symbiont_service.search_symbionts(query=message_data.message, limit=5)
        except Exception as e:
            print(f"Error getting symbionts: {e}")

        # –û–±–Ω–æ–≤–ª—è–µ–º ecosystem_context —Å –≤–∫–ª—é—á–µ–Ω–Ω—ã–º–∏ —Å–∏–º–±–∏–æ–Ω—Ç–∞–º–∏
        ecosystem_context = format_ecosystem_context(ecosystems, location, weather=weather_context, symbionts=symbionts)

        # Format the prompt with context and message
        # –ö–æ–Ω—Ç–µ–∫—Å—Ç—ã —É–∂–µ –æ—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω—ã —Ñ—É–Ω–∫—Ü–∏—è–º–∏ format_*, –±–µ–∑ –ª–∏—à–Ω–∏—Ö –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ (KISS: reduce parameters)
        llm_context = prompt_template.format(
            local_stat=local_stat,
            external_sources=external_sources,
            conversation_summary=conversation_summary,
            recent_messages=recent_messages,
            ecosystem_context=ecosystem_context,
            graph_context=graph_context,
            message=message_data.message,
        )

        try:
            # –ü—Ä–æ–∫—Å–∏ —É–∂–µ –≤—ã–ø–æ–ª–Ω—è–µ—Ç —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é –æ—Ç–≤–µ—Ç–æ–≤, –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –Ω–µ –Ω—É–∂–Ω–∞
            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–¥–∏—Ä–æ–≤–∫–∏ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –≤–Ω—É—Ç—Ä–∏ call_llm
            response_content = await call_llm(llm_context, origin="chat_message")

            # –ü–∞—Ä—Å–∏–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –∏–∑ –æ—Ç–≤–µ—Ç–∞ LLM (–¥–æ —É–¥–∞–ª–µ–Ω–∏—è —Ä–∞–∑–¥–µ–ª–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞)
            sources: List[Dict[str, str]] = parse_sources_from_response(response_content)

            # –£–¥–∞–ª—è–µ–º —Ä–∞–∑–¥–µ–ª –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è
            response_content = remove_sources_section_from_content(response_content)
        except (LLMPermanentError, LLMTemporaryError) as e:
            # –î–æ —Å—é–¥–∞ –¥–æ—Ö–æ–¥–∏–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ LLM –Ω–µ —Å–º–æ–≥–ª–∞ –æ—Ç–≤–µ—Ç–∏—Ç—å –¥–∞–∂–µ –ø–æ—Å–ª–µ –≤—Å–µ—Ö —Ä–µ—Ç—Ä–∞–µ–≤.
            # –í –ª–æ–≥–∞—Ö –æ—Å—Ç–∞–≤–ª—è–µ–º –¥–µ—Ç–∞–ª–∏, –Ω–æ –≤ —á–∞—Ç –Ω–µ –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –Ω–∏–∫–∞–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –æ—à–∏–±–∫–∏ –º–æ–¥–µ–ª–∏.
            print(f"Error in send_chat_message (LLM error): {str(e)}")
            return {
                "messageId": user_message_node.id,
                "sessionId": actual_session_id,
                "message": message_data.message,
                "conceptNodeId": message_data.conceptNodeId,
                "author": message_data.author,
                "timestamp": user_message_node.timestamp,
                "response": {
                    "messageId": None,
                    "message": "",
                    "conceptNodeId": None,
                },
            }

        # Add the AI response to the concept tree
        ai_response_node = service.add_concept(
            parent_id=user_message_node.id,
            content=response_content,
            role="assistant",
            node_type="message",
            session_id=actual_session_id,
        )

        # Parse AI response to paragraphs and save for FAISS search
        try:
            await paragraph_service.save_chat_message_paragraphs(
                message_content=response_content,
                session_id=actual_session_id,
                author="assistant",
                author_id=None,
                timestamp=ai_response_node.timestamp,
            )
        except Exception as e:
            # Log error but don't fail the request
            print(f"Error saving AI response paragraphs: {e}")

        # Analysis functions are available through UI buttons (not automatic)

        # Return both the user message and AI response
        return {
            "messageId": user_message_node.id,
            "sessionId": actual_session_id,
            "message": message_data.message,
            "conceptNodeId": message_data.conceptNodeId,
            "author": message_data.author,
            "timestamp": user_message_node.timestamp,
            "response": {
                "messageId": ai_response_node.id,
                "message": ai_response_node.content,
                "conceptNodeId": ai_response_node.id,
                "sources": sources,
            },
        }
    except HTTPException:
        raise
    except Exception as e:
        print(f"Error in send_chat_message: {str(e)}")
        print(f"message_data type: {type(message_data)}, value: {message_data}")
        import traceback

        traceback.print_exc()
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail={"error": "InternalError", "message": str(e)},
        )


@router.get("/session/{sessionId}/history", response_model=list[ConceptNode])
async def get_chat_history(sessionId: str, request: Request):
    """
    Get the history of messages in a chat session.

    Returns all messages in the session as a flat list.
    """
    try:
        service: KBService = request.app.state.kb_service
        # Get the session node
        session_node = service.get_node(sessionId)
        if not session_node:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail={"error": "SessionNotFound", "message": f"Session {sessionId} not found"},
            )

        # Get all messages in the session tree recursively
        messages = service._collect_tree_nodes(sessionId, depth=10, category=None, node_type=None, limit=1000, offset=0)

        # Filter to only message-type nodes - use getattr to safely access type attribute
        messages = [msg for msg in messages if getattr(msg, "type", None) == "message"]

        # Sort messages by timestamp to ensure chronological order
        messages.sort(key=lambda x: x.timestamp)

        return messages
    except HTTPException:
        raise
    except Exception as e:
        print(f"Error in get_chat_history: {str(e)}")
        print(f"sessionId: {sessionId}")
        import traceback

        traceback.print_exc()
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail={"error": "InternalError", "message": str(e)},
        )


@router.post("/concept-drag", response_model=dict)
async def drag_concept_to_chat(drag_data: ChatDragToChat, request: Request):
    """
    Handle dragging content to the chat (concepts, images, documents).

    Supports drag & drop of concept nodes, images, and text documents.
    """
    try:
        service: KBService = request.app.state.kb_service
        # Verify the session exists
        session_node = service.get_node(drag_data.sessionId)
        if not session_node:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail={"error": "SessionNotFound", "message": f"Session {drag_data.sessionId} not found"},
            )

        # Handle concept node drag (existing functionality)
        if drag_data.conceptNodeId:
            concept_node = service.get_node(drag_data.conceptNodeId)
            if not concept_node:
                raise HTTPException(
                    status_code=status.HTTP_404_NOT_FOUND,
                    detail={
                        "error": "ConceptNodeNotFound",
                        "message": f"Concept node {drag_data.conceptNodeId} not found",
                    },
                )

            # Add the concept node to the chat session as a message
            chat_message_node = service.add_concept(
                parent_id=drag_data.sessionId,
                content=f"–°—Å—ã–ª–∫–∞ –Ω–∞ –∫–æ–Ω—Ü–µ–ø—Ü–∏—é: {concept_node.content}",
                role="system",
                node_type="concept_reference",
                session_id=drag_data.sessionId,
            )
            return {
                "status": "success",
                "message": "–ö–æ–Ω—Ü–µ–ø—Ü–∏—è —É—Å–ø–µ—à–Ω–æ –¥–æ–±–∞–≤–ª–µ–Ω–∞ –≤ —á–∞—Ç",
                "conceptNode": chat_message_node,
                "originalConcept": concept_node,
            }

        # Handle file drag & drop
        elif drag_data.file:
            file_data = drag_data.file
            file_name = file_data.get("name", "unknown")
            file_type = file_data.get("type", "")
            file_content = file_data.get("content", "")

            if not file_content:
                raise HTTPException(
                    status_code=status.HTTP_400_BAD_REQUEST,
                    detail={"error": "InvalidFile", "message": "File content is empty"},
                )

            # Process based on file type
            if file_type.startswith("image/"):
                # Handle image files
                message_content = f"–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∑–∞–≥—Ä—É–∂–µ–Ω–æ: {file_name}\n[–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {file_content[:50]}...]"
                node_type = "image_attachment"

            elif file_type.startswith("text/") or file_type in [
                "application/pdf",
                "application/msword",
                "application/vnd.openxmlformats-officedocument.wordprocessingml.document",
            ]:
                # Handle text documents
                # For text files, content should be the actual text
                # For binary documents (PDF, DOC), we might need additional processing
                message_content = f"–î–æ–∫—É–º–µ–Ω—Ç –∑–∞–≥—Ä—É–∂–µ–Ω: {file_name}\n\n–°–æ–¥–µ—Ä–∂–∏–º–æ–µ:\n{file_content}"
                node_type = "document_attachment"

            else:
                raise HTTPException(
                    status_code=status.HTTP_400_BAD_REQUEST,
                    detail={"error": "UnsupportedFileType", "message": f"Unsupported file type: {file_type}"},
                )

            # Add file to chat as a message
            file_message_node = service.add_concept(
                parent_id=drag_data.sessionId,
                content=message_content,
                role="user",  # User uploaded this
                node_type=node_type,
                session_id=drag_data.sessionId,
            )

            return {
                "status": "success",
                "message": f"–§–∞–π–ª '{file_name}' —É—Å–ø–µ—à–Ω–æ –¥–æ–±–∞–≤–ª–µ–Ω –≤ —á–∞—Ç",
                "fileNode": file_message_node,
                "fileType": file_type,
            }

        else:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail={"error": "InvalidRequest", "message": "Either conceptNodeId or file must be provided"},
            )
    except HTTPException:
        raise
    except Exception as e:
        print(f"Error in drag_concept_to_chat: {str(e)}")
        print(f"drag_data type: {type(drag_data)}, value: {drag_data}")
        import traceback

        traceback.print_exc()
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail={"error": "InternalError", "message": str(e)},
        )


@router.get("/session/current", response_model=ChatSession)
async def get_current_chat_session(request: Request, response: Response):
    """
    Get the current chat session for the user.

    Creates a new session if none exists.
    """
    try:
        # Try to get session_id from cookies
        session_id = request.cookies.get("session_id")

        if session_id:
            # Get user session (async)
            user_session = await session_manager.get_session(session_id)
            if user_session and user_session.get("chat_session_id"):
                # Get chat session
                chat_session = chat_session_service.get_session(user_session["chat_session_id"])
                if chat_session:
                    return chat_session

        # No valid session, create new ones
        # Create user session (async)
        user_session_id = await session_manager.create_session(
            {
                "user_id": "anonymous",
                "created_at": "auto",
                "message_count": 0,
                "last_activity": int(time.time()),
            }
        )

        # Create chat session
        chat_session_data = ChatSessionCreate(topic="New Chat Session", conceptTreeId=None, ecosystem=None)
        chat_session = chat_session_service.create_session(chat_session_data)

        # Link them (async)
        user_session = await session_manager.get_session(user_session_id)
        if user_session:
            user_session["chat_session_id"] = chat_session.id
            await session_manager.update_session(user_session_id, user_session)

        # Set cookie
        response.set_cookie("session_id", user_session_id, httponly=True, max_age=2592000)  # 30 days

        return chat_session
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail={"error": "InternalError", "message": str(e)},
        )


@router.get("/session/{sessionId}", response_model=ChatSession)
async def get_chat_session(sessionId: str):
    """
    Get a chat session by ID.

    Returns the chat session.
    """
    try:
        session = chat_session_service.get_session(sessionId)
        if not session:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail={"error": "SessionNotFound", "message": f"Session {sessionId} not found"},
            )

        return session
    except HTTPException:
        raise
    except Exception as e:
        print(f"Error in get_chat_session: {str(e)}")
        print(f"sessionId: {sessionId}")
        import traceback

        traceback.print_exc()
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail={"error": "InternalError", "message": str(e)},
        )


@router.get("/sessions", response_model=list[ChatSession])
async def get_all_chat_sessions():
    """
    Get all chat sessions.

    Returns all chat sessions.
    """
    try:
        # Get all chat sessions
        sessions = chat_session_service.get_all_sessions()
        return sessions
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail={"error": "InternalError", "message": str(e)},
        )


@router.get("/starters", response_model=list[str])
async def get_conversation_starters():
    """
    Get conversation starters for the symbiosis project.

    Generates 3 engaging conversation starters using LLM.
    """
    try:
        starters = await generate_starters()
        return starters
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail={"error": "InternalError", "message": str(e)},
        )


@router.post("/localize")
async def localize_ecosystem(request: LocalizeRequest):
    """
    Localize ecosystem for a chat session based on user location.

    Takes user coordinates and conversation text, determines the local ecosystem
    and updates the session context for better localization.
    """
    try:
        # –û–±—Ä–∞—Ç–Ω–æ–µ –≥–µ–æ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∞–¥—Ä–µ—Å–∞
        location_info = await reverse_geocode_location(request.latitude, request.longitude)

        # –ò–∑–≤–ª–µ–∫–∞–µ–º —ç–∫–æ—Å–∏—Å—Ç–µ–º—É –∏ –ª–æ–∫–∞—Ü–∏—é –∏–∑ —Ç–µ–∫—Å—Ç–∞ –¥–∏–∞–ª–æ–≥–∞
        ecosystem_data = await extract_ecosystem_and_location(request.conversationText)

        # –û–±–Ω–æ–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ —ç–∫–æ—Å–∏—Å—Ç–µ–º—ã —Å —Ä–µ–∞–ª—å–Ω–æ–π –ª–æ–∫–∞—Ü–∏–µ–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        if location_info:
            ecosystem_data["location"] = location_info.get("display_name", f"{request.latitude}, {request.longitude}")
            ecosystem_data["coordinates"] = {"latitude": request.latitude, "longitude": request.longitude}

        # –ü–æ–ª—É—á–∞–µ–º —Å–µ—Å—Å–∏—é —á–∞—Ç–∞
        chat_session = chat_session_service.get_session(request.sessionId)
        if not chat_session:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail={"error": "SessionNotFound", "message": "Chat session not found"},
            )

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫ –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏
        source_type = "user_provided"
        if getattr(request, "action", None) == "expand_context":
            source_type = "context_expansion"
        elif getattr(request, "action", None) == "new_branch":
            source_type = "branch_creation"

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ª–æ–∫–∞—Ü–∏–∏ –≤ —Å–µ—Å—Å–∏–∏ (–ø–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é)
        updated_location_data = {
            **ecosystem_data,
            "source": source_type,
            "timestamp": int(time.time() * 1000),  # –í—Ä–µ–º—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
            "action": getattr(request, "action", "localize"),
        }
        chat_session_service.update_session_location(request.sessionId, updated_location_data)

        # –õ–æ–≥–∏—Ä—É–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏
        logger.info(f"–û–±–Ω–æ–≤–ª–µ–Ω–∞ –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Å—Å–∏–∏ {request.sessionId}: {updated_location_data}")

        # –§–æ—Ä–º–∏—Ä—É–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ –ª–æ–∫–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–π —ç–∫–æ—Å–∏—Å—Ç–µ–º—ã
        location_name = location_info.get("display_name", "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –ª–æ–∫–∞—Ü–∏—è") if location_info else "–ö–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã"
        ecosystem_names = [e.get("name", "") for e in ecosystem_data.get("ecosystems", [])]
        ecosystem_str = ", ".join(ecosystem_names) if ecosystem_names else "–æ–±—â–∞—è —ç–∫–æ—Å–∏—Å—Ç–µ–º–∞"

        description = f"–≠–∫–æ—Å–∏—Å—Ç–µ–º–∞ –¥–∏–∞–ª–æ–≥–∞ –ª–æ–∫–∞–ª–∏–∑–æ–≤–∞–Ω–∞ –≤ {location_name}. "
        if ecosystem_names:
            description += f"–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã —ç–∫–æ—Å–∏—Å—Ç–µ–º—ã: {ecosystem_str}. "
        description += "–ö–æ–Ω—Ç–µ–∫—Å—Ç –ø–æ–∏—Å–∫–∞ —Ç–µ–ø–µ—Ä—å —É—á–∏—Ç—ã–≤–∞–µ—Ç –ª–æ–∫–∞–ª—å–Ω—ã–µ —É—Å–ª–æ–≤–∏—è."

        return {
            "success": True,
            "location": location_info,
            "ecosystems": ecosystem_data.get("ecosystems", []),
            "description": description,
        }

    except Exception as e:
        log(f"Error localizing ecosystem: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail={"error": "InternalError", "message": str(e)},
        )


@router.get("/symbionts/search")
async def search_symbionts(q: str = "", type: Optional[str] = None, category: Optional[str] = None, limit: int = 10):
    """
    Search for symbionts and pathogens in the knowledge base.

    Args:
        q: Search query
        type: Filter by type (symbiont, pathogen, commensal, parasite)
        category: Filter by category (bacteria, virus, fungus, etc.)
        limit: Maximum number of results

    Returns:
        List of matching symbionts/pathogens
    """
    try:
        # –ü–æ–ª—É—á–∞–µ–º –¥–æ—Å—Ç—É–ø –∫ —Ö—Ä–∞–Ω–∏–ª–∏—â—É Weaviate —á–µ—Ä–µ–∑ –≥–ª–æ–±–∞–ª—å–Ω—ã–µ —Å–µ—Ä–≤–∏—Å—ã
        # –í —Ä–µ–∞–ª—å–Ω–æ–º –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–∏ —ç—Ç–æ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –≤–Ω–µ–¥—Ä–µ–Ω–æ —á–µ—Ä–µ–∑ dependency injection
        from api.storage.weaviate_storage import WeaviateStorage
        from api.storage.symbiont_service import SymbiontService

        weaviate_storage = WeaviateStorage()
        symbiont_service = SymbiontService(weaviate_storage)

        # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫
        symbionts = await symbiont_service.search_symbionts(
            query=q, type_filter=type, category_filter=category, limit=limit
        )

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ JSON-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π —Ñ–æ—Ä–º–∞—Ç
        results = []
        for symbiont in symbionts:
            results.append(
                {
                    "id": symbiont.id,
                    "name": symbiont.name,
                    "scientific_name": symbiont.scientific_name,
                    "type": symbiont.type,
                    "category": symbiont.category,
                    "interaction_type": symbiont.interaction_type,
                    "biochemical_role": symbiont.biochemical_role,
                    "risk_level": symbiont.risk_level,
                    "prevalence": symbiont.prevalence,
                    "detection_confidence": symbiont.detection_confidence,
                }
            )

        return {"success": True, "query": q, "total": len(results), "symbionts": results}

    except Exception as e:
        log(f"Error searching symbionts: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail={"error": "InternalError", "message": str(e)},
        )


@router.post("/search/web")
async def search_web(query: str = Body(...)):
    """
    Perform web search for the given query.

    Args:
        query: Search query

    Returns:
        Search results
    """
    try:
        results = await web_search_service.search_and_extract(query, max_results=3)

        if not results:
            return {"results": "–ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞"}

        formatted_results = []
        for i, result in enumerate(results, 1):
            title = result.get("title", "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è")
            content = result.get("content", "")[:500]
            url = result.get("url", "")

            formatted_results.append(f"{i}. {title}")
            if url:
                formatted_results.append(f"   –ò—Å—Ç–æ—á–Ω–∏–∫: {url}")
            formatted_results.append(f"   {content}")
            formatted_results.append("")

        return {"results": "\n".join(formatted_results)}

    except Exception as e:
        log(f"Error performing web search: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail={"error": "InternalError", "message": str(e)},
        )


@router.post("/search/books")
async def search_books(query: str = Body(...), session_id: Optional[str] = None):
    """
    Search for books related to the query and index them for the session.

    Args:
        query: Search query
        session_id: Optional chat session ID for book indexing

    Returns:
        Book search results with indexing information
    """
    try:
        # Perform book search and indexing
        context = await search_books_for_message(query, web_search_service, session_id)

        return {
            "query": query,
            "session_id": session_id,
            "books_context": context,
            "indexed": session_id is not None,
            "timestamp": int(time.time() * 1000),
        }

    except Exception as e:
        log(f"Error performing book search: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail={"error": "InternalError", "message": str(e)},
        )


async def search_books_for_message(message: str, web_search_service, session_id: Optional[str] = None) -> str:
    """
    Search for books related to the message content and index them for the session.

    Args:
        message: User message to search books for
        web_search_service: Web search service instance
        session_id: Chat session ID for book indexing

    Returns:
        Formatted context string with book information
    """
    try:
        # Create book-focused search query
        book_query = f"books about {message}"

        # Use the web search service to find book information
        results = await web_search_service.search_and_extract(book_query, max_results=5)

        if not results:
            return ""

        context_parts = []
        context_parts.append("### –ò–∑ –∫–Ω–∏–≥:")

        indexed_books = []
        for i, result in enumerate(results, 1):
            title = result.get("title", "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è")
            content = result.get("content", "")[:1000]  # Longer excerpts for books
            url = result.get("url", "")

            # Create book index entry
            book_entry = {
                "id": f"book_{session_id}_{i}_{int(time.time())}",
                "title": title,
                "content": content,
                "url": url,
                "indexed_at": int(time.time() * 1000),
                "session_id": session_id,
                "search_query": message,
            }
            indexed_books.append(book_entry)

            # Format as book reference
            context_parts.append(f"üìö {title}")
            if url:
                context_parts.append(f"   –ò—Å—Ç–æ—á–Ω–∏–∫: {url}")
            context_parts.append(f"   {content}")
            context_parts.append("")

        # TODO: Save indexed_books to database for future searches within session
        # This will allow searching within already indexed books
        if session_id and indexed_books:
            try:
                # Save to session context for now (temporary solution)
                # In future: create proper books table and indexing
                chat_session = chat_session_service.get_session(session_id)
                if chat_session:
                    current_books = chat_session.indexed_books or []
                    current_books.extend(indexed_books)
                    # Keep only last 50 books per session to avoid memory issues
                    if len(current_books) > 50:
                        current_books = current_books[-50:]
                    chat_session_service.update_session_books(session_id, current_books)
            except Exception as e:
                log(f"Error saving indexed books: {e}")

        return "\n".join(context_parts)

    except Exception as e:
        log(f"Error searching books: {e}")
        return ""


# Removed automatic analysis - now handled through UI buttons
