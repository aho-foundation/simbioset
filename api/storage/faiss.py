"""
–†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ FAISS –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏ –∏ —á–∞—Ç–∞–º–∏.
–í–∫–ª—é—á–∞–µ—Ç —Ñ—É–Ω–∫—Ü–∏–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏, fact-checking –∏ –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏ —Å–æ–æ–±—â–µ–Ω–∏–π.
"""

from typing import Dict, List, Tuple, Optional, Any, cast
import json
import faiss  # type: ignore
import numpy as np
from sentence_transformers import SentenceTransformer  # type: ignore
from dataclasses import dataclass, field
from enum import Enum
import hashlib
from datetime import datetime

from api.settings import EMBEDDING_MODEL_NAME, MODELS_CACHE_DIR, ENABLE_AUTOMATIC_DETECTORS
from api.logger import root_logger

log = root_logger.debug


class DocumentType(Enum):
    """–¢–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞ - —á–∞—Ç, –∑–Ω–∞–Ω–∏–µ –∏–ª–∏ –¥–æ–∫—É–º–µ–Ω—Ç"""

    CHAT = "chat"
    KNOWLEDGE = "knowledge"
    DOCUMENT = "document"


class ClassificationType(Enum):
    """–¢–∏–ø –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –¥–ª—è —ç–∫–æ—Å–∏—Å—Ç–µ–º–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π"""

    ECOSYSTEM_VULNERABILITY = "ecosystem_vulnerability"  # –≤–æ–∑–º–æ–∂–Ω—ã–µ —Ä–∏—Å–∫–∏
    ECOSYSTEM_RISK = "ecosystem_risk"  # –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã
    ECOSYSTEM_SOLUTION = "suggested_ecosystem_solution"  # –ø—Ä–µ–¥–ª–∞–≥–∞–µ–º—ã–µ —Ä–µ—à–µ–Ω–∏—è
    NEUTRAL = "neutral"


class FactCheckResult(Enum):
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏"""

    TRUE = "true"  # —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –≤–µ—Ä–Ω–æ
    FALSE = "false"  # —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –ª–æ–∂–Ω–æ
    PARTIAL = "partial"  # —á–∞—Å—Ç–∏—á–Ω–æ –≤–µ—Ä–Ω–æ
    UNVERIFIABLE = "unverifiable"  # –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å
    UNKNOWN = "unknown"  # –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ


@dataclass
class Paragraph:
    """–ü–∞—Ä–∞–≥—Ä–∞—Ñ –¥–æ–∫—É–º–µ–Ω—Ç–∞ - –º–æ–∂–µ—Ç –±—ã—Ç—å –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–º —á–∞—Ç-—Å–æ–æ–±—â–µ–Ω–∏–µ–º –∏–ª–∏ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–º –¥–æ–∫—É–º–µ–Ω—Ç–∞.

    –ü–∞—Ä–∞–≥—Ä–∞—Ñ –º–æ–∂–µ—Ç –±—ã—Ç—å —Å–≤—è–∑–∞–Ω —Å —É–∑–ª–æ–º –∑–Ω–∞–Ω–∏—è (node_id) –∏–ª–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–º (document_id).
    –í –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã —Å–≤—è–∑–∞–Ω—ã —Å —É–∑–ª–∞–º–∏ —á–µ—Ä–µ–∑ node_id –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞.
    """

    id: str
    content: str
    author: Optional[str] = None
    author_id: Optional[int] = None
    timestamp: Optional[datetime] = None
    document_id: Optional[str] = None
    node_id: Optional[str] = None  # –°—Å—ã–ª–∫–∞ –Ω–∞ —É–∑–µ–ª –∑–Ω–∞–Ω–∏—è (–¥–ª—è –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π)
    document_type: DocumentType = DocumentType.CHAT
    metadata: Dict[str, Any] = field(default_factory=dict)
    embedding: Optional[np.ndarray] = None

    # –ü–æ–ª—è –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏ –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏
    tags: List[str] = field(default_factory=list)  # –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Ç–µ–≥–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
    # –¢–∏–ø–æ–≤–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞ (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞—Ö –∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ö –ø–æ–∏—Å–∫–∞)
    classification: Optional[ClassificationType] = None
    fact_check_result: Optional[FactCheckResult] = None
    fact_check_details: Optional[Dict[str, Any]] = None
    location: Optional[str] = None
    time_reference: Optional[str] = None
    ecosystem_id: Optional[str] = None  # –°—Å—ã–ª–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–Ω—É—é —ç–∫–æ—Å–∏—Å—Ç–µ–º—É –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞ (–µ—Å–ª–∏ –æ—Ç–Ω–æ—Å–∏—Ç—Å—è)
    # –ü–æ—Ä—è–¥–∫–æ–≤—ã–π –Ω–æ–º–µ—Ä –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞ –≤–Ω—É—Ç—Ä–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–≥–æ –ø–æ—Ä—è–¥–∫–∞)
    paragraph_index: Optional[int] = None


class ParagraphVectorSearch:
    """–ö–ª–∞—Å—Å –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º FAISS"""

    def __init__(self, model_name: str = EMBEDDING_MODEL_NAME, cache_folder: Optional[str] = None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞

        Args:
            model_name: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
                       (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–∞—è –º–æ–¥–µ–ª—å, –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—â–∞—è —Ä—É—Å—Å–∫–∏–π)
            cache_folder: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π
        """
        log(f"üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ {model_name}...")

        # –ï—Å–ª–∏ cache_folder –Ω–µ —É–∫–∞–∑–∞–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –≥–ª–æ–±–∞–ª—å–Ω—É—é –Ω–∞—Å—Ç—Ä–æ–π–∫—É
        if cache_folder is None:
            cache_folder = MODELS_CACHE_DIR

        # –£–∫–∞–∑—ã–≤–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π
        self.model = SentenceTransformer(model_name, cache_folder=cache_folder)
        self.dimension = self.model.get_sentence_embedding_dimension()

        # –•—Ä–∞–Ω–∏–º –∏–Ω–¥–µ–∫—Å—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        self.document_indexes: Dict[str, faiss.Index] = {}  # type: ignore
        self.document_paragraph_ids: Dict[str, List[str]] = {}
        self.document_paragraphs: Dict[str, List[Paragraph]] = {}
        self.document_embeddings_cache: Dict[str, Optional[np.ndarray]] = {}

        log(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞, —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {self.dimension}")


class FAISSStorage:
    """–û–±–µ—Ä—Ç–∫–∞ –¥–ª—è ParagraphVectorSearch, —á—Ç–æ–±—ã —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞—Ç—å –æ–∂–∏–¥–∞–µ–º–æ–º—É –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å—É"""

    def __init__(self, model_name: str = EMBEDDING_MODEL_NAME, cache_folder: Optional[str] = None):
        self._search_engine = ParagraphVectorSearch(model_name, cache_folder)
        # –°–≤—è–∑–∞–Ω–Ω—ã–π —Å–µ—Ä–≤–∏—Å —Ç–µ–≥–æ–≤ –∑–∞–¥–∞–µ—Ç—Å—è —Å–Ω–∞—Ä—É–∂–∏ (—Å–º. api.main.init_app),
        # –æ–±—ä—è–≤–ª—è–µ–º –µ–≥–æ —è–≤–Ω–æ –¥–ª—è mypy, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å attr-defined.
        self._tag_service: Optional[Any] = None

    def __getattr__(self, name):
        # –î–µ–ª–µ–≥–∏—Ä—É–µ–º –≤—Å–µ –∞—Ç—Ä–∏–±—É—Ç—ã –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–º—É search engine
        return getattr(self._search_engine, name)

    def _create_paragraph_id(
        self,
        content: str,
        author: Optional[str] = None,
        timestamp: Optional[datetime] = None,
        index: Optional[int] = None,
    ) -> str:
        """–°–æ–∑–¥–∞–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã–π ID –¥–ª—è –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞"""
        import uuid

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º UUID –¥–ª—è –≥–∞—Ä–∞–Ω—Ç–∏–∏ —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏, –¥–∞–∂–µ –¥–ª—è –æ–¥–∏–Ω–∞–∫–æ–≤–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        unique_id = str(uuid.uuid4())
        return f"para_{unique_id}"

    def _create_embedding(self, text: str) -> np.ndarray:
        """
        –°–æ–∑–¥–∞–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ç–µ–∫—Å—Ç–∞

        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞

        Returns:
            –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π –≤–µ–∫—Ç–æ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
        """
        embedding = self.model.encode(text, convert_to_numpy=True)

        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–ª—è –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞
        norm = np.linalg.norm(embedding)
        if norm > 0:
            embedding = embedding / norm

        return cast(np.ndarray, embedding.astype("float32"))

    def _extract_text(self, message: Dict[str, Any]) -> str:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ —Å–æ–æ–±—â–µ–Ω–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞

        Args:
            message: –°–ª–æ–≤–∞—Ä—å —Å –¥–∞–Ω–Ω—ã–º–∏ —Å–æ–æ–±—â–µ–Ω–∏—è –∏–ª–∏ TelegramMessage

        Returns:
            –¢–µ–∫—Å—Ç —Å–æ–æ–±—â–µ–Ω–∏—è
        """
        if hasattr(message, "text"):  # TelegramMessage object
            text = message.text or ""
            from_user = getattr(message, "from_user", None)
            if from_user:
                username = getattr(from_user, "username", "") or getattr(from_user, "first_name", "")
            else:
                username = ""
        elif isinstance(message, dict):  # Dictionary
            text = message.get("text", "")
            from_user = message.get("from", message.get("from_user", {}))
            if isinstance(from_user, dict):
                username = from_user.get("username", from_user.get("first_name", ""))
            else:
                username = ""
        else:
            text = ""
            username = ""

        return f"{username}: {text}"

    def _create_paragraph_from_message(
        self, message: Dict[str, Any], document_id: str, document_type: DocumentType, index: Optional[int] = None
    ) -> Paragraph:
        """–°–æ–∑–¥–∞–µ—Ç –ø–∞—Ä–∞–≥—Ä–∞—Ñ –∏–∑ —Å–æ–æ–±—â–µ–Ω–∏—è"""
        if isinstance(message, dict):
            text = message.get("text", "")
            author = message.get("from", message.get("from_user", {}))
            if isinstance(author, dict):
                author_name = author.get("username", author.get("first_name", ""))
                author_id = author.get("id")
            else:
                author_name = ""
                author_id = None

            timestamp = message.get("date")
            if timestamp:
                timestamp = datetime.fromtimestamp(timestamp)
        else:
            text = self._extract_text(message)
            author_name = ""
            author_id = None
            timestamp = None

        # –°–æ–∑–¥–∞–µ–º –ø–∞—Ä–∞–≥—Ä–∞—Ñ
        paragraph = Paragraph(
            id=self._create_paragraph_id(text, author_name, timestamp, index),
            content=text,
            author=author_name,
            author_id=author_id,
            timestamp=timestamp,
            document_id=document_id,
            document_type=document_type,
        )

        # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥
        paragraph.embedding = self._create_embedding(text)

        return paragraph

    def _group_consecutive_messages(self, messages: List[Dict[str, Any]]) -> List[Paragraph]:
        """–ì—Ä—É–ø–ø–∏—Ä—É–µ—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è –æ–¥–Ω–æ–≥–æ –∞–≤—Ç–æ—Ä–∞ –≤ –æ–¥–∏–Ω –ø–∞—Ä–∞–≥—Ä–∞—Ñ"""
        if not messages:
            return []

        grouped_paragraphs = []
        current_author = None
        current_content: List[str] = []
        current_metadata: Dict[str, Any] = {}

        for msg in messages:
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∞–≤—Ç–æ—Ä–∞ —Å–æ–æ–±—â–µ–Ω–∏—è
            if isinstance(msg, dict):
                from_user = msg.get("from", msg.get("from_user", {}))
                if isinstance(from_user, dict):
                    author_id = from_user.get("id")
                    author_name = from_user.get("username", from_user.get("first_name", ""))
                else:
                    author_id = None
                    author_name = ""
            else:
                # –î–ª—è TelegramMessage –æ–±—ä–µ–∫—Ç–æ–≤
                from_user = getattr(msg, "from_user", None)
                if from_user:
                    author_id = getattr(from_user, "id", None)
                    author_name = getattr(from_user, "username", "") or getattr(from_user, "first_name", "")
                else:
                    author_id = None
                    author_name = ""

            # –ï—Å–ª–∏ —ç—Ç–æ –ø–µ—Ä–≤—ã–π —Å–æ–æ–±—â–µ–Ω–∏–µ –∏–ª–∏ –∞–≤—Ç–æ—Ä –∏–∑–º–µ–Ω–∏–ª—Å—è
            if current_author is None or current_author != author_id:
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–π –ø–∞—Ä–∞–≥—Ä–∞—Ñ, –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å
                if current_content:
                    combined_content = "\n".join(current_content)
                    paragraph = Paragraph(
                        id=self._create_paragraph_id(combined_content, current_author),
                        content=combined_content,
                        author=current_author,
                        metadata=current_metadata,
                    )
                    grouped_paragraphs.append(paragraph)

                # –ù–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—ã–π –ø–∞—Ä–∞–≥—Ä–∞—Ñ
                current_author = author_id
                current_content = [msg.get("text", "") if isinstance(msg, dict) else getattr(msg, "text", "")]
                current_metadata = msg if isinstance(msg, dict) else {}
            else:
                # –î–æ–±–∞–≤–ª—è–µ–º –∫ —Ç–µ–∫—É—â–µ–º—É –ø–∞—Ä–∞–≥—Ä–∞—Ñ—É
                current_content.append(msg.get("text", "") if isinstance(msg, dict) else getattr(msg, "text", ""))

        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –ø–∞—Ä–∞–≥—Ä–∞—Ñ
        if current_content:
            combined_content = "\n".join(current_content)
            paragraph = Paragraph(
                id=self._create_paragraph_id(combined_content, current_author),
                content=combined_content,
                author=current_author,
                metadata=current_metadata,
            )
            grouped_paragraphs.append(paragraph)

        # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤
        for paragraph in grouped_paragraphs:
            paragraph.embedding = self._create_embedding(paragraph.content)

        return grouped_paragraphs

    def _classify_paragraph(self, paragraph: Paragraph, tag_service=None) -> Paragraph:
        """–ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç –ø–∞—Ä–∞–≥—Ä–∞—Ñ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–æ–¥—É–ª–µ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏.

        –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Ç–µ–≥–∏ –¥–ª—è –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞, —Ç–∞–∫ –∫–∞–∫ –æ–Ω –º–æ–∂–µ—Ç –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ
        –æ–ø–∏—Å—ã–≤–∞—Ç—å —É—è–∑–≤–∏–º–æ—Å—Ç–∏, —Ä–∏—Å–∫–∏ –∏ —Ä–µ—à–µ–Ω–∏—è.

        Args:
            paragraph: –ü–∞—Ä–∞–≥—Ä–∞—Ñ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
            tag_service: –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π —Å–µ—Ä–≤–∏—Å –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ç–µ–≥–∞–º–∏
        """
        try:
            # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ
            try:
                from api.detect.rolestate import classify_message_type
                from api.detect.factcheck import check_factuality
                from api.detect.localize import extract_location_and_time
            except ImportError:
                log("‚ö†Ô∏è –ú–æ–¥—É–ª–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é")
                return paragraph

            # –ò—Å–ø–æ–ª—å–∑—É–µ–º LLM –¥–ª—è —Ç–µ–≥–æ–≤ —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —è–≤–Ω–æ –≤–∫–ª—é—á–µ–Ω—ã –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –¥–µ—Ç–µ–∫—Ç–æ—Ä—ã.
            # –í –æ–±—ã—á–Ω–æ–º UX —á–∞—Ç–∞ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ª—ë–≥–∫–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ classify_message_type.
            if tag_service and ENABLE_AUTOMATIC_DETECTORS:
                import asyncio

                try:
                    # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π event loop –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
                    try:
                        loop = asyncio.get_event_loop()
                    except RuntimeError:
                        loop = asyncio.new_event_loop()
                        asyncio.set_event_loop(loop)

                    # –ü—Ä–µ–¥–ª–∞–≥–∞–µ–º —Ç–µ–≥–∏ —á–µ—Ä–µ–∑ LLM
                    suggested_tags = loop.run_until_complete(
                        tag_service.suggest_tags_for_paragraph(paragraph.content, paragraph.tags)
                    )
                    if suggested_tags:
                        paragraph.tags = suggested_tags
                        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º classification enum –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–µ—Ä–≤–æ–≥–æ —Ç–µ–≥–∞
                        if suggested_tags:
                            try:
                                classification_map = {
                                    "ecosystem_risk": ClassificationType.ECOSYSTEM_RISK,
                                    "ecosystem_vulnerability": ClassificationType.ECOSYSTEM_VULNERABILITY,
                                    "suggested_ecosystem_solution": ClassificationType.ECOSYSTEM_SOLUTION,
                                    "ecosystem_solution": ClassificationType.ECOSYSTEM_SOLUTION,
                                    "neutral": ClassificationType.NEUTRAL,
                                }
                                paragraph.classification = classification_map.get(suggested_tags[0])
                            except (ValueError, KeyError):
                                log(f"‚ö†Ô∏è –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –≤ —Ç–µ–≥–∞—Ö: {suggested_tags[0]}")
                        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—á–µ—Ç—á–∏–∫–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ç–µ–≥–æ–≤
                        tag_service.update_tag_usage(suggested_tags)
                        # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
                        for tag in suggested_tags:
                            tag_service.add_example_to_tag(tag, paragraph.content[:200])
                except Exception as e:
                    log(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏ —Ç–µ–≥–æ–≤ —á–µ—Ä–µ–∑ LLM: {e}")
                    # Fallback –Ω–∞ —Å—Ç–∞—Ä—ã–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä
                    classification_result = classify_message_type(paragraph.content)
                    if classification_result:
                        if isinstance(classification_result, str):
                            paragraph.tags = [classification_result]
                            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º classification enum –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç—Ä–æ–∫–∏
                            try:
                                classification_map = {
                                    "ecosystem_risk": ClassificationType.ECOSYSTEM_RISK,
                                    "ecosystem_vulnerability": ClassificationType.ECOSYSTEM_VULNERABILITY,
                                    "suggested_ecosystem_solution": ClassificationType.ECOSYSTEM_SOLUTION,
                                    "ecosystem_solution": ClassificationType.ECOSYSTEM_SOLUTION,
                                    "neutral": ClassificationType.NEUTRAL,
                                }
                                paragraph.classification = classification_map.get(classification_result)
                            except (ValueError, KeyError):
                                log(f"‚ö†Ô∏è –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏: {classification_result}")
                        elif isinstance(classification_result, list):
                            paragraph.tags = classification_result
                            # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π —Ç–µ–≥ –¥–ª—è classification
                            if classification_result:
                                try:
                                    classification_map = {
                                        "ecosystem_risk": ClassificationType.ECOSYSTEM_RISK,
                                        "ecosystem_vulnerability": ClassificationType.ECOSYSTEM_VULNERABILITY,
                                        "suggested_ecosystem_solution": ClassificationType.ECOSYSTEM_SOLUTION,
                                        "ecosystem_solution": ClassificationType.ECOSYSTEM_SOLUTION,
                                        "neutral": ClassificationType.NEUTRAL,
                                    }
                                    paragraph.classification = classification_map.get(classification_result[0])
                                except (ValueError, KeyError):
                                    log(f"‚ö†Ô∏è –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏: {classification_result[0]}")
            else:
                # Fallback –Ω–∞ —Å—Ç–∞—Ä—ã–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –±–µ–∑ LLM
                classification_result = classify_message_type(paragraph.content)
                if classification_result:
                    if isinstance(classification_result, str):
                        paragraph.tags = [classification_result]
                        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º classification enum –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç—Ä–æ–∫–∏
                        try:
                            # –ú–∞–ø–ø–∏–Ω–≥ —Å—Ç—Ä–æ–∫–æ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –Ω–∞ ClassificationType
                            classification_map = {
                                "ecosystem_risk": ClassificationType.ECOSYSTEM_RISK,
                                "ecosystem_vulnerability": ClassificationType.ECOSYSTEM_VULNERABILITY,
                                "suggested_ecosystem_solution": ClassificationType.ECOSYSTEM_SOLUTION,
                                "ecosystem_solution": ClassificationType.ECOSYSTEM_SOLUTION,  # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ
                                "neutral": ClassificationType.NEUTRAL,
                            }
                            paragraph.classification = classification_map.get(classification_result)
                        except (ValueError, KeyError):
                            log(f"‚ö†Ô∏è –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏: {classification_result}")
                    elif isinstance(classification_result, list):
                        paragraph.tags = classification_result
                        # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π —Ç–µ–≥ –¥–ª—è classification
                        if classification_result:
                            try:
                                classification_map = {
                                    "ecosystem_risk": ClassificationType.ECOSYSTEM_RISK,
                                    "ecosystem_vulnerability": ClassificationType.ECOSYSTEM_VULNERABILITY,
                                    "suggested_ecosystem_solution": ClassificationType.ECOSYSTEM_SOLUTION,
                                    "ecosystem_solution": ClassificationType.ECOSYSTEM_SOLUTION,
                                    "neutral": ClassificationType.NEUTRAL,
                                }
                                paragraph.classification = classification_map.get(classification_result[0])
                            except (ValueError, KeyError):
                                log(f"‚ö†Ô∏è –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏: {classification_result[0]}")

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏
            fact_check_result = check_factuality(paragraph.content)
            if fact_check_result:
                paragraph.fact_check_result = FactCheckResult(fact_check_result.get("status", "unknown"))
                paragraph.fact_check_details = fact_check_result.get("details")

            # –õ–æ–∫–∞–ª–∏–∑–∞—Ü–∏—è (–º–µ—Å—Ç–æ –∏ –≤—Ä–µ–º—è)
            location_result = extract_location_and_time(paragraph.content)
            if location_result:
                paragraph.location = location_result.get("location")
                paragraph.time_reference = location_result.get("time_reference")

            # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –¥–µ—Ç–µ–∫—Ç–æ—Ä—ã (—ç–∫–æ—Å–∏—Å—Ç–µ–º—ã / –æ—Ä–≥–∞–Ω–∏–∑–º—ã) –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤
            # –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –æ—Ç–∫–ª—é—á–µ–Ω—ã, —á—Ç–æ–±—ã –Ω–µ –ª–æ–º–∞—Ç—å UX –±—ã—Å—Ç—Ä–æ–≥–æ –æ–±—â–µ–Ω–∏—è.
            if ENABLE_AUTOMATIC_DETECTORS:
                # –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —ç–∫–æ—Å–∏—Å—Ç–µ–º (–∏—Å–ø–æ–ª—å–∑—É—è –¥–∞–Ω–Ω—ã–µ –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏)
                try:
                    from api.detect.ecosystem_scaler import detect_ecosystems
                    import asyncio

                    try:
                        loop = asyncio.get_event_loop()
                    except RuntimeError:
                        loop = asyncio.new_event_loop()
                        asyncio.set_event_loop(loop)

                    ecosystems = loop.run_until_complete(
                        detect_ecosystems(paragraph.content, location_data=location_result)
                    )

                    if ecosystems:
                        if not paragraph.metadata:
                            paragraph.metadata = {}
                        paragraph.metadata["ecosystems"] = ecosystems
                        log(f"‚úÖ –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {len(ecosystems)} —ç–∫–æ—Å–∏—Å—Ç–µ–º –≤ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–µ")
                except ImportError:
                    log("‚ö†Ô∏è –ú–æ–¥—É–ª—å –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è —ç–∫–æ—Å–∏—Å—Ç–µ–º –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                except Exception as e:
                    log(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–∏ —ç–∫–æ—Å–∏—Å—Ç–µ–º: {e}")

                # –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –æ—Ä–≥–∞–Ω–∏–∑–º–æ–≤
                try:
                    from api.detect.organism_detector import detect_organisms
                    from api.classify.organism_classifier import classify_organisms_batch
                    import asyncio

                    try:
                        loop = asyncio.get_event_loop()
                    except RuntimeError:
                        loop = asyncio.new_event_loop()
                        asyncio.set_event_loop(loop)

                    organisms = loop.run_until_complete(detect_organisms(paragraph.content))

                    if organisms:
                        classified_organisms = loop.run_until_complete(classify_organisms_batch(organisms))

                        if not paragraph.metadata:
                            paragraph.metadata = {}
                        paragraph.metadata["organisms"] = classified_organisms

                        log(f"‚úÖ –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ –∏ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–æ {len(classified_organisms)} –æ—Ä–≥–∞–Ω–∏–∑–º–æ–≤ –≤ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–µ")
                except ImportError:
                    log("‚ö†Ô∏è –ú–æ–¥—É–ª–∏ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –æ—Ä–≥–∞–Ω–∏–∑–º–æ–≤ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                except Exception as e:
                    log(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–∏ –æ—Ä–≥–∞–Ω–∏–∑–º–æ–≤: {e}")

        except Exception as e:
            log(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞: {e}")

        return paragraph

    def add_documents(
        self,
        documents: List[Dict[str, Any]],
        document_id: str,
        document_type: DocumentType = DocumentType.KNOWLEDGE,
        classify: bool = True,
    ) -> int:
        """
        –î–æ–±–∞–≤–ª—è–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ –∏–Ω–¥–µ–∫—Å

        Args:
            documents: –°–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
            document_id: ID –¥–æ–∫—É–º–µ–Ω—Ç–∞
            document_type: –¢–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞ (—á–∞—Ç –∏–ª–∏ –∑–Ω–∞–Ω–∏–µ)
            classify: –í—ã–ø–æ–ª–Ω—è—Ç—å –ª–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é –∏ –ø—Ä–æ–≤–µ—Ä–∫—É –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏

        Returns:
            –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã—Ö –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤
        """
        if not documents:
            return 0

        # –°–æ–∑–¥–∞–µ–º –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        paragraphs = []
        for i, doc in enumerate(documents):
            if isinstance(doc, dict) and "text" in doc:
                # –≠—Ç–æ —Å–æ–æ–±—â–µ–Ω–∏–µ —á–∞—Ç–∞, —Å–æ–∑–¥–∞–µ–º –ø–∞—Ä–∞–≥—Ä–∞—Ñ
                paragraph = self._create_paragraph_from_message(doc, document_id, document_type, index=i)
                paragraphs.append(paragraph)
            elif isinstance(doc, str):
                # –≠—Ç–æ —Ç–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞, —Å–æ–∑–¥–∞–µ–º –ø–∞—Ä–∞–≥—Ä–∞—Ñ
                paragraph = Paragraph(
                    id=self._create_paragraph_id(doc, index=i),
                    content=doc,
                    document_id=document_id,
                    document_type=document_type,
                )
                paragraph.embedding = self._create_embedding(doc)
                paragraphs.append(paragraph)

        if not paragraphs:
            return 0

        # –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ–º –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if classify:
            for paragraph in paragraphs:
                paragraph = self._classify_paragraph(paragraph)

        log(f"üîÑ –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è {len(paragraphs)} –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ {document_id}...")

        # –ò–∑–≤–ª–µ–∫–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
        embeddings_list = [para.embedding for para in paragraphs if para.embedding is not None]
        if not embeddings_list:
            return 0

        embeddings = np.array(embeddings_list).astype(np.float32)

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∏–Ω–¥–µ–∫—Å –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞, –µ—Å–ª–∏ –µ–≥–æ –µ—â–µ –Ω–µ—Ç
        if document_id not in self.document_indexes:
            self.document_indexes[document_id] = faiss.IndexFlatIP(self.dimension)
            self.document_paragraph_ids[document_id] = []
            self.document_paragraphs[document_id] = []
            self.document_embeddings_cache[document_id] = None

        # –î–æ–±–∞–≤–ª—è–µ–º –≤ –∏–Ω–¥–µ–∫—Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        self.document_indexes[document_id].add(embeddings)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        for paragraph in paragraphs:
            self.document_paragraph_ids[document_id].append(paragraph.id)
            self.document_paragraphs[document_id].append(paragraph)

        # –ö–≠–®–ò–†–£–ï–ú —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –ø–æ–≤—Ç–æ—Ä–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        self.document_embeddings_cache[document_id] = embeddings

        log(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ {len(paragraphs)} –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤ –≤ –¥–æ–∫—É–º–µ–Ω—Ç {document_id}, —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∑–∞–∫—ç—à–∏—Ä–æ–≤–∞–Ω—ã")
        return len(paragraphs)

    def add_chat_messages(
        self, messages: List[Dict[str, Any]], chat_id: str, group_consecutive: bool = True, classify: bool = True
    ) -> int:
        """
        –î–æ–±–∞–≤–ª—è–µ—Ç —á–∞—Ç-—Å–æ–æ–±—â–µ–Ω–∏—è –≤ –∏–Ω–¥–µ–∫—Å

        Args:
            messages: –°–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
            chat_id: ID —á–∞—Ç–∞
            group_consecutive: –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∞—Ç—å –ª–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è –æ–¥–Ω–æ–≥–æ –∞–≤—Ç–æ—Ä–∞
            classify: –í—ã–ø–æ–ª–Ω—è—Ç—å –ª–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é –∏ –ø—Ä–æ–≤–µ—Ä–∫—É –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏

        Returns:
            –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã—Ö –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤
        """
        if not messages:
            return 0

        # –§–∏–ª—å—Ç—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è —Å —Ç–µ–∫—Å—Ç–æ–º
        valid_messages = []
        for msg in messages:
            if isinstance(msg, dict) and msg.get("text"):
                valid_messages.append(msg)
            elif hasattr(msg, "text") and msg.text:
                valid_messages.append(msg)

        if not valid_messages:
            return 0

        # –°–æ–∑–¥–∞–µ–º –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã
        if group_consecutive:
            paragraphs = self._group_consecutive_messages(valid_messages)
        else:
            paragraphs = []
            for msg in valid_messages:
                paragraph = self._create_paragraph_from_message(msg, chat_id, DocumentType.CHAT)
                paragraphs.append(paragraph)

        if not paragraphs:
            return 0

        # –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ–º –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if classify:
            for paragraph in paragraphs:
                paragraph = self._classify_paragraph(paragraph)

        log(f"üîÑ –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è {len(paragraphs)} –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤ –≤ —á–∞—Ç–µ {chat_id}...")

        # –ò–∑–≤–ª–µ–∫–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
        embeddings_list = [para.embedding for para in paragraphs if para.embedding is not None]
        if not embeddings_list:
            return 0

        embeddings = np.array(embeddings_list).astype(np.float32)

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∏–Ω–¥–µ–∫—Å –¥–ª—è —á–∞—Ç–∞, –µ—Å–ª–∏ –µ–≥–æ –µ—â–µ –Ω–µ—Ç
        if chat_id not in self.document_indexes:
            self.document_indexes[chat_id] = faiss.IndexFlatIP(self.dimension)
            self.document_paragraph_ids[chat_id] = []
            self.document_paragraphs[chat_id] = []
            self.document_embeddings_cache[chat_id] = None

        # –î–æ–±–∞–≤–ª—è–µ–º –≤ –∏–Ω–¥–µ–∫—Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —á–∞—Ç–∞
        self.document_indexes[chat_id].add(embeddings)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —á–∞—Ç–∞
        for paragraph in paragraphs:
            self.document_paragraph_ids[chat_id].append(paragraph.id)
            self.document_paragraphs[chat_id].append(paragraph)

        # –ö–≠–®–ò–†–£–ï–ú —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –ø–æ–≤—Ç–æ—Ä–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —á–∞—Ç–∞
        self.document_embeddings_cache[chat_id] = embeddings

        log(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ {len(paragraphs)} –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤ –≤ —á–∞—Ç {chat_id}, —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∑–∞–∫—ç—à–∏—Ä–æ–≤–∞–Ω—ã")
        return len(paragraphs)

    def update_paragraph(self, document_id: str, paragraph: Paragraph) -> bool:
        """
        –û–±–Ω–æ–≤–ª—è–µ—Ç –ø–∞—Ä–∞–≥—Ä–∞—Ñ –≤ –∏–Ω–¥–µ–∫—Å–µ

        Args:
            document_id: ID –¥–æ–∫—É–º–µ–Ω—Ç–∞
            paragraph: –û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π –ø–∞—Ä–∞–≥—Ä–∞—Ñ

        Returns:
            True –µ—Å–ª–∏ —É—Å–ø–µ—à–Ω–æ –æ–±–Ω–æ–≤–ª–µ–Ω–æ
        """
        if document_id not in self.document_indexes:
            return False

        paragraph_id = paragraph.id
        if not paragraph_id:
            return False

        # –ò—â–µ–º –∏–Ω–¥–µ–∫—Å –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞
        try:
            if paragraph_id in self.document_paragraph_ids[document_id]:
                idx = self.document_paragraph_ids[document_id].index(paragraph_id)

                # –û–±–Ω–æ–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞
                self.document_paragraphs[document_id][idx] = paragraph

                # –û–±–Ω–æ–≤–ª—è–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥
                if paragraph.embedding is not None:
                    # –û–±–Ω–æ–≤–ª—è–µ–º –∫—ç—à —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
                    embeddings_cache = self.document_embeddings_cache[document_id]
                    if embeddings_cache is not None:
                        embeddings_cache[idx] = paragraph.embedding

                        # –ü–µ—Ä–µ—Å—Ç—Ä–∞–∏–≤–∞–µ–º –∏–Ω–¥–µ–∫—Å
                        # FAISS IndexFlatIP –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –æ–¥–Ω–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–∞ –±–µ–∑ IDMap
                        # –ü–æ—ç—Ç–æ–º—É –ø—Ä–æ—â–µ –≤—Å–µ–≥–æ –ø–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å –∏–Ω–¥–µ–∫—Å –∏–∑ –∫—ç—à–∞ (—ç—Ç–æ –±—ã—Å—Ç—Ä–æ)
                        self.document_indexes[document_id].reset()
                        self.document_indexes[document_id].add(embeddings_cache)
                        return True
        except Exception as e:
            log(f"‚ùå –û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞ –≤ –∏–Ω–¥–µ–∫—Å–µ: {e}")

        return False

    def delete_paragraph(self, document_id: str, paragraph_id: str) -> bool:
        """
        –£–¥–∞–ª—è–µ—Ç –ø–∞—Ä–∞–≥—Ä–∞—Ñ –∏–∑ –∏–Ω–¥–µ–∫—Å–∞

        Args:
            document_id: ID –¥–æ–∫—É–º–µ–Ω—Ç–∞
            paragraph_id: ID –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞

        Returns:
            True –µ—Å–ª–∏ —É—Å–ø–µ—à–Ω–æ —É–¥–∞–ª–µ–Ω–æ
        """
        if document_id not in self.document_indexes:
            return False

        try:
            if paragraph_id in self.document_paragraph_ids[document_id]:
                idx = self.document_paragraph_ids[document_id].index(paragraph_id)

                # –£–¥–∞–ª—è–µ–º –∏–∑ —Å–ø–∏—Å–∫–æ–≤
                self.document_paragraph_ids[document_id].pop(idx)
                self.document_paragraphs[document_id].pop(idx)

                # –£–¥–∞–ª—è–µ–º –∏–∑ –∫—ç—à–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
                embeddings_cache = self.document_embeddings_cache[document_id]
                if embeddings_cache is not None:
                    # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫—É –∏–∑ numpy array
                    # np.delete –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –Ω–æ–≤—ã–π –º–∞—Å—Å–∏–≤, –ø–æ—ç—Ç–æ–º—É –æ–±–Ω–æ–≤–ª—è–µ–º –∫—ç—à
                    new_embeddings_cache = np.delete(embeddings_cache, idx, axis=0)
                    self.document_embeddings_cache[document_id] = new_embeddings_cache

                    # –ü–µ—Ä–µ—Å—Ç—Ä–∞–∏–≤–∞–µ–º –∏–Ω–¥–µ–∫—Å
                    self.document_indexes[document_id].reset()
                    if len(new_embeddings_cache) > 0:
                        self.document_indexes[document_id].add(new_embeddings_cache)

                    return True
        except Exception as e:
            log(f"‚ùå –û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞ –∏–∑ –∏–Ω–¥–µ–∫—Å–∞: {e}")

        return False

    def search_similar(
        self,
        query: str,
        document_id: str,
        top_k: int = 10,
        classification_filter: Optional[ClassificationType] = None,
        fact_check_filter: Optional[FactCheckResult] = None,
        location_filter: Optional[str] = None,
        ecosystem_id_filter: Optional[str] = None,
    ) -> List[Tuple[Paragraph, float]]:
        """
        –ò—â–µ—Ç –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ—Ö–æ–∂–∏–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã –≤ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º –¥–æ–∫—É–º–µ–Ω—Ç–µ

        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            document_id: ID –¥–æ–∫—É–º–µ–Ω—Ç–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞
            top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            classification_filter: –§–∏–ª—å—Ç—Ä –ø–æ —Ç–∏–ø—É –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
            fact_check_filter: –§–∏–ª—å—Ç—Ä –ø–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏
            location_filter: –§–∏–ª—å—Ç—Ä –ø–æ –ª–æ–∫–∞—Ü–∏–∏ (None = –Ω–µ —Ñ–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å)
            ecosystem_id_filter: –§–∏–ª—å—Ç—Ä –ø–æ ID —ç–∫–æ—Å–∏—Å—Ç–µ–º—ã (None = –Ω–µ —Ñ–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å)

        Returns:
            –°–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (–ø–∞—Ä–∞–≥—Ä–∞—Ñ, –æ—Ü–µ–Ω–∫–∞ —Å—Ö–æ–∂–µ—Å—Ç–∏)
        """
        if document_id not in self.document_indexes or self.document_indexes[document_id].ntotal == 0:
            return []

        # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –∑–∞–ø—Ä–æ—Å–∞
        query_embedding = self._create_embedding(query).reshape(1, -1)

        # –ò—â–µ–º –ø–æ—Ö–æ–∂–∏–µ –≤ –∏–Ω–¥–µ–∫—Å–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º top_k, —á—Ç–æ–±—ã –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –æ—Å—Ç–∞–ª–æ—Å—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        search_k = top_k * 3 if (location_filter or ecosystem_id_filter) else top_k
        search_k = min(search_k, self.document_indexes[document_id].ntotal)
        scores, indices = self.document_indexes[document_id].search(query_embedding, search_k)

        # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        results = []
        for idx, score in zip(indices[0], scores[0]):
            if int(idx) < len(self.document_paragraphs[document_id]):
                paragraph = self.document_paragraphs[document_id][int(idx)]

                # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ–∏–ª—å—Ç—Ä—ã
                # –§–∏–ª—å—Ç—Ä –ø–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏: –ø—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –∑–Ω–∞—á–µ–Ω–∏–µ classification_filter –≤ —Ç–µ–≥–∞—Ö
                if classification_filter and classification_filter.value not in paragraph.tags:
                    continue
                if fact_check_filter and paragraph.fact_check_result != fact_check_filter:
                    continue

                # –§–∏–ª—å—Ç—Ä –ø–æ –ª–æ–∫–∞—Ü–∏–∏: –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω–∞ –ª–æ–∫–∞—Ü–∏—è, –≤–∫–ª—é—á–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã
                # —Å —ç—Ç–æ–π –ª–æ–∫–∞—Ü–∏–µ–π –∏–ª–∏ –±–µ–∑ –ª–æ–∫–∞—Ü–∏–∏ (—É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–µ)
                if location_filter:
                    para_location = paragraph.location
                    if para_location:
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–æ–≤–ø–∞–¥–∞–µ—Ç –ª–∏ –ª–æ–∫–∞—Ü–∏—è (—Ä–µ–≥–∏—Å—Ç—Ä–æ–Ω–µ–∑–∞–≤–∏—Å–∏–º–æ)
                        location_lower = location_filter.lower()
                        para_location_lower = para_location.lower()
                        # –í–∫–ª—é—á–∞–µ–º, –µ—Å–ª–∏ –ª–æ–∫–∞—Ü–∏–∏ —Å–æ–≤–ø–∞–¥–∞—é—Ç –∏–ª–∏ –ø–∞—Ä–∞–≥—Ä–∞—Ñ –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –∫ –±–æ–ª–µ–µ —à–∏—Ä–æ–∫–æ–π –æ–±–ª–∞—Å—Ç–∏
                        if location_lower not in para_location_lower and para_location_lower not in location_lower:
                            continue

                # –§–∏–ª—å—Ç—Ä –ø–æ —ç–∫–æ—Å–∏—Å—Ç–µ–º–µ: –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω–∞ —ç–∫–æ—Å–∏—Å—Ç–µ–º–∞, –≤–∫–ª—é—á–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã
                # —Å —ç—Ç–æ–π —ç–∫–æ—Å–∏—Å—Ç–µ–º–æ–π –∏–ª–∏ –±–µ–∑ —ç–∫–æ—Å–∏—Å—Ç–µ–º—ã (—É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–µ)
                if ecosystem_id_filter:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä—è–º–æ–µ –ø–æ–ª–µ ecosystem_id
                    if paragraph.ecosystem_id:
                        if paragraph.ecosystem_id != ecosystem_id_filter:
                            continue
                    else:
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º metadata["ecosystems"] –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
                        ecosystems_in_meta = paragraph.metadata.get("ecosystems", []) if paragraph.metadata else []
                        if ecosystems_in_meta:
                            # –ò—â–µ–º —ç–∫–æ—Å–∏—Å—Ç–µ–º—É –ø–æ ID –∏–ª–∏ –∏–º–µ–Ω–∏
                            ecosystem_found = False
                            for eco in ecosystems_in_meta:
                                # –ï—Å–ª–∏ —ç–∫–æ—Å–∏—Å—Ç–µ–º–∞ –≤ metadata - —ç—Ç–æ —Å–ª–æ–≤–∞—Ä—å —Å –ø–æ–ª—è–º–∏, –ø—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ name
                                # –∏–ª–∏ —Å–æ–∑–¥–∞–µ–º ID –∏–∑ name –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
                                eco_id = eco.get("id") if isinstance(eco, dict) else None
                                eco_name = eco.get("name") if isinstance(eco, dict) else str(eco)
                                # –ü–æ–∫–∞ –ø—Ä–æ—Å—Ç–æ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã —Å –¥—Ä—É–≥–∏–º–∏ —ç–∫–æ—Å–∏—Å—Ç–µ–º–∞–º–∏
                                # TODO: —É–ª—É—á—à–∏—Ç—å –ª–æ–≥–∏–∫—É —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —ç–∫–æ—Å–∏—Å—Ç–µ–º
                                if eco_id == ecosystem_id_filter:
                                    ecosystem_found = True
                                    break
                            if not ecosystem_found:
                                continue

                results.append((paragraph, float(score)))
                # –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º—Å—è, –∫–æ–≥–¥–∞ –Ω–∞–±—Ä–∞–ª–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                if len(results) >= top_k:
                    break

        return results

    async def search_similar_paragraphs(self, query: str, document_id: str, top_k: int = 10) -> List[Paragraph]:
        """
        –ò—â–µ—Ç –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ—Ö–æ–∂–∏–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã, –≤–æ–∑–≤—Ä–∞—â–∞—è —Ç–æ–ª—å–∫–æ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã –±–µ–∑ –æ—Ü–µ–Ω–æ–∫.
        –ï—Å–ª–∏ –ø—Ä—è–º—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –º–∞–ª–æ –∏–ª–∏ –Ω–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç LLM –¥–ª—è –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∞–Ω–∏—è.
        """
        # 1. –ü—Ä—è–º–æ–π –ø–æ–∏—Å–∫
        similar_pairs = self.search_similar(query, document_id, top_k)

        # –ï—Å–ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∏—Ö
        if len(similar_pairs) >= 3:
            return [para for para, score in similar_pairs]

        log(f"üîç –ú–∞–ª–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ ({len(similar_pairs)}), –ø—Ä–æ–±—É–µ–º –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∞—Ç—å –∑–∞–ø—Ä–æ—Å: '{query}'")
        from api.llm import rephrase_search_query

        rephrased_queries = await rephrase_search_query(query)

        all_results = {}  # –ò—Å–ø–æ–ª—å–∑—É–µ–º dict –¥–ª—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏ –ø–æ paragraph_id

        # –î–æ–±–∞–≤–ª—è–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        for para, score in similar_pairs:
            all_results[para.id] = (para, score)

        # –ò—â–µ–º –ø–æ –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –∑–∞–ø—Ä–æ—Å–∞–º
        for new_query in rephrased_queries:
            log(f"üîÑ –ü–æ–∏—Å–∫ –ø–æ –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–º—É –∑–∞–ø—Ä–æ—Å—É: '{new_query}'")
            new_pairs = self.search_similar(new_query, document_id, top_k=3)
            for para, score in new_pairs:
                # –ï—Å–ª–∏ –ø–∞—Ä–∞–≥—Ä–∞—Ñ —É–∂–µ –µ—Å—Ç—å, –æ—Å—Ç–∞–≤–ª—è–µ–º —Å –ª—É—á—à–∏–º —Å–∫–æ—Ä–æ–º
                if para.id not in all_results or score > all_results[para.id][1]:
                    all_results[para.id] = (para, score)

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Å–∫–æ—Ä—É
        sorted_results = sorted(all_results.values(), key=lambda x: x[1], reverse=True)

        return [para for para, score in sorted_results[:top_k]]

    def get_paragraph_by_id(self, document_id: str, paragraph_id: str) -> Optional[Paragraph]:
        """–ü–æ–ª—É—á–∞–µ—Ç –ø–∞—Ä–∞–≥—Ä–∞—Ñ –ø–æ ID"""
        if document_id not in self.document_paragraphs:
            return None

        for paragraph in self.document_paragraphs[document_id]:
            if paragraph.id == paragraph_id:
                return paragraph

        return None

    def get_document_paragraphs(self, document_id: str) -> List[Paragraph]:
        """–ü–æ–ª—É—á–∞–µ—Ç –≤—Å–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        return self.document_paragraphs.get(document_id, [])

    def get_all_documents(self) -> List[str]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        return list(self.document_indexes.keys())

    def get_paragraphs_by_classification(self, document_id: str, classification: ClassificationType) -> List[Paragraph]:
        """–ü–æ–ª—É—á–∞–µ—Ç –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã –ø–æ —Ç–∏–ø—É –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ (–ø—Ä–æ–≤–µ—Ä—è–µ—Ç classification –∏–ª–∏ –Ω–∞–ª–∏—á–∏–µ —Ç–µ–≥–∞)"""
        if document_id not in self.document_paragraphs:
            return []

        return [
            para
            for para in self.document_paragraphs[document_id]
            if para.classification == classification or classification.value in para.tags
        ]

    def get_paragraphs_by_fact_check_result(
        self, document_id: str, fact_check_result: FactCheckResult
    ) -> List[Paragraph]:
        """–ü–æ–ª—É—á–∞–µ—Ç –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã –ø–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏"""
        if document_id not in self.document_paragraphs:
            return []

        return [para for para in self.document_paragraphs[document_id] if para.fact_check_result == fact_check_result]

    def reclassify_paragraph(self, document_id: str, paragraph_id: str, tag_service=None) -> bool:
        """–ü–µ—Ä–µ–∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç –ø–∞—Ä–∞–≥—Ä–∞—Ñ"""
        paragraph = self.get_paragraph_by_id(document_id, paragraph_id)
        if not paragraph:
            return False

        # –û–±–Ω–æ–≤–ª—è–µ–º –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é
        tag_service = tag_service or getattr(self, "_tag_service", None)
        paragraph = self._classify_paragraph(paragraph, tag_service=tag_service)

        # –û–±–Ω–æ–≤–ª—è–µ–º –ø–∞—Ä–∞–≥—Ä–∞—Ñ –≤ –∏–Ω–¥–µ–∫—Å–µ
        return self.update_paragraph(document_id, paragraph)

    def reclassify_document(self, document_id: str) -> int:
        """–ü–µ—Ä–µ–∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç –≤—Å–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ"""
        paragraphs = self.get_document_paragraphs(document_id)
        updated_count = 0

        for paragraph in paragraphs:
            if self.reclassify_paragraph(document_id, paragraph.id):
                updated_count += 1

        return updated_count
