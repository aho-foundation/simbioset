"""
Weaviate Storage - –∑–∞–º–µ–Ω–∞ FAISS –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤.
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç Weaviate –¥–ª—è –ø–µ—Ä—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ–≥–æ —Ö—Ä–∞–Ω–µ–Ω–∏—è –∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –ø–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º.
"""

from typing import Dict, List, Tuple, Optional, Any, cast
import uuid
import numpy as np
from sentence_transformers import SentenceTransformer
from datetime import datetime
import weaviate
from weaviate.classes.query import Filter, MetadataQuery, HybridFusion
from weaviate.config import Timeout, AdditionalConfig
import asyncio
from functools import lru_cache

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º —Ç–∏–ø—ã –∏–∑ faiss.py –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
from api.storage.faiss import (
    Paragraph,
    DocumentType,
    ClassificationType,
    FactCheckResult,
)
from api.settings import (
    EMBEDDING_MODEL_NAME,
    MODELS_CACHE_DIR,
    WEAVIATE_URL,
    WEAVIATE_GRPC_URL,
    WEAVIATE_API_KEY,
    WEAVIATE_CLASS_NAME,
    WEAVIATE_BATCH_SIZE,
    WEAVIATE_USE_BUILTIN_AUTOSCHEMA,
    ENABLE_AUTOMATIC_DETECTORS,
    WEAVIATE_USE_HYBRID_SEARCH,
    WEAVIATE_HYBRID_ALPHA,
    WEAVIATE_USE_RERANKING,
    WEAVIATE_RERANK_LIMIT,
    WEAVIATE_EMBEDDING_CACHE_SIZE,
)
from api.storage.weaviate_schema import create_schema_if_not_exists, update_schema_if_needed
from api.logger import root_logger

log = root_logger.debug


class WeaviateStorage:
    """–•—Ä–∞–Ω–∏–ª–∏—â–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ Weaviate —Å —Å–æ–≤–º–µ—Å—Ç–∏–º—ã–º –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–º FAISSStorage"""

    def __init__(self, model_name: str = EMBEDDING_MODEL_NAME, cache_folder: Optional[str] = None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Weaviate Storage

        Args:
            model_name: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
            cache_folder: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π
        """
        log(f"üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ {model_name}...")

        if cache_folder is None:
            cache_folder = MODELS_CACHE_DIR

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        self.model = SentenceTransformer(model_name, cache_folder=cache_folder)
        self.dimension = self.model.get_sentence_embedding_dimension()

        # –ü–æ–¥–∫–ª—é—á–∞–µ–º—Å—è –∫ Weaviate (v4 API)
        auth_config = None
        if WEAVIATE_API_KEY:
            auth_config = weaviate.auth.AuthApiKey(api_key=WEAVIATE_API_KEY)

        # –ü–∞—Ä—Å–∏–º HTTP URL –¥–ª—è –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è
        weaviate_url = WEAVIATE_URL
        if not weaviate_url:
            log("‚ùå WEAVIATE_URL –Ω–µ –∑–∞–¥–∞–Ω, –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ Weaviate")
            raise ValueError("WEAVIATE_URL is required for Weaviate connection")

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å–ø–∏—Å–æ–∫ –≤–æ–∑–º–æ–∂–Ω—ã—Ö —Ö–æ—Å—Ç–æ–≤ –¥–ª—è Dokku/Docker —Å—Ä–µ–¥—ã
        url_parts = weaviate_url.replace("http://", "").replace("https://", "").split(":")
        base_host = url_parts[0] if url_parts else "localhost"
        http_port = int(url_parts[1]) if len(url_parts) > 1 else 8080
        http_secure = weaviate_url.startswith("https://")

        # –í–æ–∑–º–æ–∂–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã DNS –∏–º–µ–Ω –¥–ª—è Dokku
        possible_hosts = []
        if base_host != "localhost":
            # –†–∞–∑–±–∏—Ä–∞–µ–º base_host –Ω–∞ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
            host_parts = base_host.split(".")
            if len(host_parts) >= 2:
                service_name = host_parts[0]  # 'weaviate'
                app_name = host_parts[1] if len(host_parts) > 1 else None  # 'web'

                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –≤–∞—Ä–∏–∞–Ω—Ç—ã
                possible_hosts.extend(
                    [
                        base_host,  # weaviate.web.1
                        f"{service_name}.{app_name}",  # weaviate.web
                        service_name,  # weaviate
                        f"{service_name}.web.1",  # weaviate.web.1 (—É–∂–µ –µ—Å—Ç—å)
                    ]
                )
            else:
                possible_hosts.append(base_host)

        # –î–æ–±–∞–≤–ª—è–µ–º localhost –∫–∞–∫ fallback
        if "localhost" not in possible_hosts:
            possible_hosts.append("localhost")

        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
        possible_hosts = list(set(possible_hosts))
        log(f"üîç –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–æ–∑–º–æ–∂–Ω—ã–µ —Ö–æ—Å—Ç—ã Weaviate: {possible_hosts}")

        # –ü–∞—Ä—Å–∏–º gRPC URL –∏–ª–∏ –≤—ã—á–∏—Å–ª—è–µ–º –∏–∑ HTTP URL
        weaviate_grpc_url = WEAVIATE_GRPC_URL

        connection_success = False
        last_error: Optional[Exception] = None

        # –ü—Ä–æ–±—É–µ–º –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ –∫–∞–∂–¥–æ–º—É –≤–æ–∑–º–æ–∂–Ω–æ–º—É —Ö–æ—Å—Ç—É
        for http_host in possible_hosts:
            try:
                if not weaviate_grpc_url:
                    # –í—ã—á–∏—Å–ª—è–µ–º gRPC URL –∏–∑ HTTP URL (–ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –ø–æ—Ä—Ç—ã)
                    # –î–ª—è Weaviate —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π HTTP –ø–æ—Ä—Ç 8080, gRPC 50051
                    grpc_host = http_host
                    grpc_port = 50051 if http_port == 8080 else http_port + 1  # 8080 -> 50051, –∏–Ω–∞—á–µ +1
                    weaviate_grpc_url = f"{grpc_host}:{grpc_port}"

                grpc_parts = weaviate_grpc_url.split(":")
                grpc_host = grpc_parts[0] if grpc_parts else "localhost"
                grpc_port = int(grpc_parts[1]) if len(grpc_parts) > 1 else 50051
                grpc_secure = False  # gRPC –æ–±—ã—á–Ω–æ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç SSL –≤–æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–π —Å–µ—Ç–∏

                log(
                    f"üîó –ü—Ä–æ–±—É–µ–º –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ Weaviate - HTTP: {http_host}:{http_port} (secure: {http_secure}), gRPC: {grpc_host}:{grpc_port} (secure: {grpc_secure})"
                )

                # –°–æ–∑–¥–∞–µ–º –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º gRPC –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π
                connection_params = weaviate.connect.base.ConnectionParams.from_params(
                    http_host=http_host,
                    http_port=http_port,
                    http_secure=http_secure,
                    grpc_host=grpc_host,
                    grpc_port=grpc_port,
                    grpc_secure=grpc_secure,
                )

                # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º —Ç–∞–π–º–∞—É—Ç—ã —Å–æ–≥–ª–∞—Å–Ω–æ best practices
                timeout_config = Timeout(
                    init=10,  # —Ç–∞–π–º–∞—É—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –∫–ª–∏–µ–Ω—Ç–∞ (–±—ã–ª 30 —Å–µ–∫ –≤ main.py)
                    query=30,  # —Ç–∞–π–º–∞—É—Ç –¥–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤
                    insert=60,  # —Ç–∞–π–º–∞—É—Ç –¥–ª—è –≤—Å—Ç–∞–≤–∫–∏ (batch –æ–ø–µ—Ä–∞—Ü–∏–∏ –º–æ–≥—É—Ç –±—ã—Ç—å –¥–æ–ª–≥–∏–º–∏)
                )

                # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∫–ª–∏–µ–Ω—Ç–∞
                additional_config = AdditionalConfig(timeout=timeout_config)

                # –ï—Å–ª–∏ gRPC –ø—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª–µ–Ω, –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –∫–ª–∏–µ–Ω—Ç –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è gRPC –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
                client_kwargs = {
                    "connection_params": connection_params,
                    "auth_client_secret": auth_config,
                    "additional_config": additional_config,
                }

                self.client = weaviate.WeaviateClient(**client_kwargs)  # type: ignore[arg-type]

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ
                log("üîå –í—ã–∑—ã–≤–∞–µ–º client.connect()...")
                self.client.connect()
                log("‚úÖ client.connect() —É—Å–ø–µ—à–µ–Ω")

                connection_success = True
                log(f"‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–æ –∫ Weaviate –Ω–∞ {http_host}:{http_port}")
                break

            except weaviate.exceptions.WeaviateConnectionError as e:
                last_error = e
                log(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Weaviate {http_host}:{http_port} - {e}")
                continue
            except weaviate.exceptions.WeaviateBaseError as e:
                last_error = e
                log(f"‚ùå –û—à–∏–±–∫–∞ Weaviate –∫–ª–∏–µ–Ω—Ç–∞ –¥–ª—è {http_host}:{http_port} - {e}")
                continue
            except Exception as e:
                last_error = e
                log(f"‚ùå –ù–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ {http_host}:{http_port} - {e}")
                continue

        if not connection_success:
            log(f"üí• –í—Å–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Weaviate –ø—Ä–æ–≤–∞–ª–∏–ª–∏—Å—å. –ü–æ—Å–ª–µ–¥–Ω—è—è –æ—à–∏–±–∫–∞: {last_error}")
            if last_error:
                raise last_error
            else:
                raise RuntimeError("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ Weaviate: –≤—Å–µ —Ö–æ—Å—Ç—ã –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã")

        # –°–æ–∑–¥–∞–µ–º –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º gRPC –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π
        connection_params = weaviate.connect.base.ConnectionParams.from_params(
            http_host=http_host,
            http_port=http_port,
            http_secure=http_secure,
            grpc_host=grpc_host,
            grpc_port=grpc_port,
            grpc_secure=grpc_secure,
        )

        # –ï—Å–ª–∏ gRPC –ø—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª–µ–Ω, –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –∫–ª–∏–µ–Ω—Ç –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è gRPC –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        client_kwargs = {
            "connection_params": connection_params,
            "auth_client_secret": auth_config,
        }

        self.client = weaviate.WeaviateClient(**client_kwargs)  # type: ignore[arg-type]

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∏ —Å–æ–∑–¥–∞–µ–º —Å—Ö–µ–º—É
        try:
            log("üîå –í—ã–∑—ã–≤–∞–µ–º client.connect()...")
            self.client.connect()
            log("‚úÖ client.connect() —É—Å–ø–µ—à–µ–Ω")

            log("üìä –ü–æ–ª—É—á–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ...")
            meta = self.client.get_meta()
            log(f"‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–æ –∫ Weaviate {meta.get('version', 'unknown')} –Ω–∞ {weaviate_url}")

            # –°—Ö–µ–º–∞: –≤—Å—Ç—Ä–æ–µ–Ω–Ω–∞—è AutoSchema Weaviate –∏–ª–∏ —Ä—É—á–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ
            if WEAVIATE_USE_BUILTIN_AUTOSCHEMA:
                log("ü§ñ –í—Å—Ç—Ä–æ–µ–Ω–Ω–∞—è AutoSchema Weaviate –∞–∫—Ç–∏–≤–Ω–∞ - —Å—Ö–µ–º–∞ –±—É–¥–µ—Ç —Å–æ–∑–¥–∞–≤–∞—Ç—å—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏–∑ –¥–∞–Ω–Ω—ã—Ö")
                log("üìà –≠—Ç–æ –∏–¥–µ–∞–ª—å–Ω–æ –¥–ª—è —Å–∏–º–±–∏–æ—Å–µ—Ç–∏: —Å–≤—è–∑–∏ –∏ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –º–æ–≥—É—Ç —ç–≤–æ–ª—é—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞—Ç—å –æ—Ä–≥–∞–Ω–∏—á–µ—Å–∫–∏")
            else:
                log("üîß –†—É—á–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Å—Ö–µ–º–æ–π - —Å–æ–∑–¥–∞–µ–º –ø—Ä–µ–¥–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—É—é —Å—Ö–µ–º—É")
                create_schema_if_not_exists(self.client)
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
                if update_schema_if_needed(self.client):
                    log("üîÑ –°—Ö–µ–º–∞ –±—ã–ª–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∞")
                else:
                    log("‚úÖ –°—Ö–µ–º–∞ –∞–∫—Ç—É–∞–ª—å–Ω–∞")
        except Exception as e:
            log(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Weaviate: {e}")
            log(f"üîç –î–µ—Ç–∞–ª–∏ –æ—à–∏–±–∫–∏: {type(e).__name__}: {str(e)}")
            raise

        log(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞, —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {self.dimension}")

        # –°–≤—è–∑–∞–Ω–Ω—ã–π —Å–µ—Ä–≤–∏—Å —Ç–µ–≥–æ–≤ –∑–∞–¥–∞–µ—Ç—Å—è —Å–Ω–∞—Ä—É–∂–∏
        self._tag_service: Optional[Any] = None

        # –ö–µ—à –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω)
        self._embedding_cache_enabled = WEAVIATE_EMBEDDING_CACHE_SIZE > 0
        if self._embedding_cache_enabled:
            log(f"üíæ –ö–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –≤–∫–ª—é—á–µ–Ω–æ (—Ä–∞–∑–º–µ—Ä: {WEAVIATE_EMBEDDING_CACHE_SIZE})")

    def close(self) -> None:
        """
        –ü—Ä–∞–≤–∏–ª—å–Ω–æ –∑–∞–∫—Ä—ã–≤–∞–µ—Ç —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å Weaviate —Å–æ–≥–ª–∞—Å–Ω–æ best practices.

        –°–ª–µ–¥—É–µ—Ç –≤—ã–∑—ã–≤–∞—Ç—å –ø—Ä–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏ —Ä–∞–±–æ—Ç—ã –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è.
        """
        if hasattr(self, "client") and self.client:
            try:
                log("üîå –ó–∞–∫—Ä—ã–≤–∞–µ–º —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å Weaviate...")
                self.client.close()
                log("‚úÖ –°–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å Weaviate –∑–∞–∫—Ä—ã—Ç–æ")
            except Exception as e:
                log(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–∫—Ä—ã—Ç–∏–∏ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è —Å Weaviate: {e}")

    def __enter__(self):
        """–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä - –≤—Ö–æ–¥"""
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        """–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä - –≤—ã—Ö–æ–¥ —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –∑–∞–∫—Ä—ã—Ç–∏–µ–º"""
        self.close()

    def __del__(self):
        """–î–µ—Å—Ç—Ä—É–∫—Ç–æ—Ä - —Ñ–∏–Ω–∞–ª—å–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞ –∑–∞–∫—Ä—ã—Ç—å —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ"""
        try:
            self.close()
        except BaseException:
            pass  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ—à–∏–±–∫–∏ –≤ –¥–µ—Å—Ç—Ä—É–∫—Ç–æ—Ä–µ

    def _create_paragraph_id(
        self,
        content: str,
        author: Optional[str] = None,
        timestamp: Optional[datetime] = None,
        index: Optional[int] = None,
    ) -> str:
        """–°–æ–∑–¥–∞–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã–π ID –¥–ª—è –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞"""
        unique_id = str(uuid.uuid4())
        if index is not None:
            return f"para_{unique_id}_idx_{index}"
        return f"para_{unique_id}"

    def _create_embedding(self, text: str) -> np.ndarray:
        """
        –°–æ–∑–¥–∞–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ç–µ–∫—Å—Ç–∞ —Å –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–º –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º

        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞

        Returns:
            –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π –≤–µ–∫—Ç–æ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
        """
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–µ—à–∏—Ä–æ–≤–∞–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ
        if self._embedding_cache_enabled:
            return self._create_embedding_cached(text)

        embedding = self.model.encode(text, convert_to_numpy=True)

        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–ª—è –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞
        norm = np.linalg.norm(embedding)
        if norm > 0:
            embedding = embedding / norm

        return cast(np.ndarray, embedding.astype("float32"))

    def _create_embedding_cached(self, text: str) -> np.ndarray:
        """
        –ö–µ—à–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–æ—Å—Ç–æ–π —Å–ª–æ–≤–∞—Ä—å –¥–ª—è –∫–µ—à–∞ (lru_cache –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å numpy arrays)
        """
        if not hasattr(self, "_embedding_cache"):
            self._embedding_cache: Dict[str, np.ndarray] = {}
            self._embedding_cache_max_size = WEAVIATE_EMBEDDING_CACHE_SIZE

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–µ—à
        if text in self._embedding_cache:
            return self._embedding_cache[text]

        # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥
        embedding = self.model.encode(text, convert_to_numpy=True)

        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–ª—è –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞
        norm = np.linalg.norm(embedding)
        if norm > 0:
            embedding = embedding / norm

        embedding = cast(np.ndarray, embedding.astype("float32"))

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫–µ—à (—Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º —Ä–∞–∑–º–µ—Ä–∞)
        if len(self._embedding_cache) >= self._embedding_cache_max_size:
            # –£–¥–∞–ª—è–µ–º —Å–∞–º—ã–π —Å—Ç–∞—Ä—ã–π —ç–ª–µ–º–µ–Ω—Ç (FIFO)
            oldest_key = next(iter(self._embedding_cache))
            del self._embedding_cache[oldest_key]

        self._embedding_cache[text] = embedding
        return embedding

    def _paragraph_to_weaviate_object(self, paragraph: Paragraph) -> Dict[str, Any]:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç Paragraph –≤ –æ–±—ä–µ–∫—Ç –¥–ª—è Weaviate

        Metadata –Ω–µ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –≤ Weaviate - –æ–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ, –∫–æ—Ç–æ—Ä—ã–µ –∑–∞—Ç–µ–º —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–µ –ø–æ–ª—è (organism_ids, ecosystem_id).
        """
        # –ò–∑–≤–ª–µ–∫–∞–µ–º organism_ids –∏–∑ metadata (–¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)
        organism_ids = paragraph.metadata.get("organism_ids", []) if paragraph.metadata else []

        obj = {
            "content": paragraph.content,
            "document_id": paragraph.document_id or "",
            "node_id": paragraph.node_id or "",
            "document_type": paragraph.document_type.value if paragraph.document_type else "chat",
            "organism_ids": organism_ids,
            "organisms": paragraph.organisms or [],
            "ecosystem_id": paragraph.ecosystem_id or "",
            "location": paragraph.location or "",
            "tags": paragraph.tags or [],
            "author": paragraph.author or "",
            "author_id": paragraph.author_id or 0,
            "paragraph_index": paragraph.paragraph_index or 0,
        }

        # –î–æ–±–∞–≤–ª—è–µ–º timestamp, –µ—Å–ª–∏ –µ—Å—Ç—å
        if paragraph.timestamp:
            obj["timestamp"] = paragraph.timestamp.isoformat()

        return obj

    def _weaviate_object_to_paragraph(self, obj: Any, vector: Optional[np.ndarray] = None) -> Paragraph:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç –æ–±—ä–µ–∫—Ç –∏–∑ Weaviate v4 –≤ Paragraph

        Args:
            obj: –û–±—ä–µ–∫—Ç Weaviate v4 (Object —Å properties, uuid, vector, metadata) –∏–ª–∏ dict (–¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)
            vector: –í–µ–∫—Ç–æ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ (–µ—Å–ª–∏ –Ω–µ –≤–∫–ª—é—á–µ–Ω –≤ obj)
        """
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º v4 Object –∏–ª–∏ dict
        if hasattr(obj, "properties"):
            # v4 Object
            props = obj.properties
            paragraph_id = str(obj.uuid) if hasattr(obj, "uuid") else ""
            obj_vector = obj.vector if hasattr(obj, "vector") and obj.vector is not None else None
            if obj_vector is None:
                obj_vector = vector
            else:
                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Å–ø–∏—Å–æ–∫ –≤ numpy array
                if isinstance(obj_vector, (list, tuple)):
                    obj_vector = np.array(obj_vector, dtype=np.float32)
                elif isinstance(obj_vector, dict):
                    # –ï—Å–ª–∏ –≤–µ–∫—Ç–æ—Ä –ø—Ä–∏—Ö–æ–¥–∏—Ç –∫–∞–∫ dict, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–π vector
                    obj_vector = vector
                else:
                    # –ï—Å–ª–∏ —ç—Ç–æ —É–∂–µ numpy array –∏–ª–∏ –¥—Ä—É–≥–æ–π —Ç–∏–ø
                    try:
                        obj_vector = np.array(obj_vector, dtype=np.float32)
                    except (ValueError, TypeError):
                        obj_vector = vector
            metadata_obj = obj.metadata if hasattr(obj, "metadata") else None
        else:
            # dict (–æ–±—Ä–∞—Ç–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å)
            props = obj if isinstance(obj, dict) else obj.get("properties", obj)
            paragraph_id = obj.get("_id") or obj.get("_additional", {}).get("id") or ""
            obj_vector = vector
            metadata_obj = None

        # –ü–∞—Ä—Å–∏–º timestamp
        timestamp = None
        if props.get("timestamp"):
            try:
                ts_val = props["timestamp"]
                if isinstance(ts_val, str):
                    timestamp = datetime.fromisoformat(ts_val.replace("Z", "+00:00"))
                elif isinstance(ts_val, datetime):
                    timestamp = ts_val
                elif hasattr(ts_val, "isoformat"):
                    timestamp = ts_val
                elif isinstance(ts_val, dict):
                    # v4 –º–æ–∂–µ—Ç –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å timestamp –∫–∞–∫ dict —Å –ø–æ–ª—è–º–∏
                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º, —Ç–∞–∫ –∫–∞–∫ —ç—Ç–æ –Ω–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
                    pass
            except Exception:
                pass

        # Metadata –Ω–µ —Ö—Ä–∞–Ω–∏—Ç—Å—è –≤ Weaviate, —Å–æ–∑–¥–∞–µ–º –ø—É—Å—Ç–æ–π dict
        # organism_ids –∏–∑–≤–ª–µ–∫–∞–µ–º –∏–∑ –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ –ø–æ–ª—è
        organism_ids = props.get("organism_ids", [])
        metadata = {}
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º organism_ids –≤ metadata –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å –∫–æ–¥–æ–º, –∫–æ—Ç–æ—Ä—ã–π –æ–∂–∏–¥–∞–µ—Ç –∏—Ö —Ç–∞–º
        if organism_ids:
            metadata["organism_ids"] = organism_ids

        # –ë–µ–∑–æ–ø–∞—Å–Ω–æ –∏–∑–≤–ª–µ–∫–∞–µ–º —á–∏—Å–ª–æ–≤—ã–µ –ø–æ–ª—è
        author_id = props.get("author_id")
        if isinstance(author_id, dict):
            log(f"‚ö†Ô∏è author_id is dict: {author_id}")
            author_id = None
        elif author_id is not None:
            try:
                author_id = int(author_id)
            except (ValueError, TypeError) as e:
                log(f"‚ö†Ô∏è Cannot convert author_id {author_id} ({type(author_id)}) to int: {e}")
                author_id = None

        paragraph_index = props.get("paragraph_index")
        if isinstance(paragraph_index, dict):
            log(f"‚ö†Ô∏è paragraph_index is dict: {paragraph_index}")
            paragraph_index = None
        elif paragraph_index is not None:
            try:
                paragraph_index = int(paragraph_index)
            except (ValueError, TypeError) as e:
                log(f"‚ö†Ô∏è Cannot convert paragraph_index {paragraph_index} ({type(paragraph_index)}) to int: {e}")
                paragraph_index = None

        paragraph = Paragraph(
            id=paragraph_id,
            content=props.get("content", ""),
            author=props.get("author"),
            author_id=author_id,
            timestamp=timestamp,
            document_id=props.get("document_id"),
            node_id=props.get("node_id"),
            document_type=DocumentType(props.get("document_type", "chat")),
            metadata=metadata,
            embedding=obj_vector,
            tags=props.get("tags", []),
            location=props.get("location"),
            ecosystem_id=props.get("ecosystem_id"),
            paragraph_index=paragraph_index,
        )

        return paragraph

    def _extract_text(self, message: Dict[str, Any]) -> str:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ —Å–æ–æ–±—â–µ–Ω–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞

        Args:
            message: –°–ª–æ–≤–∞—Ä—å —Å –¥–∞–Ω–Ω—ã–º–∏ —Å–æ–æ–±—â–µ–Ω–∏—è –∏–ª–∏ TelegramMessage

        Returns:
            –¢–µ–∫—Å—Ç —Å–æ–æ–±—â–µ–Ω–∏—è
        """
        if hasattr(message, "text"):  # TelegramMessage object
            text = message.text or ""
            from_user = getattr(message, "from_user", None)
            if from_user:
                username = getattr(from_user, "username", "") or getattr(from_user, "first_name", "")
            else:
                username = ""
        elif isinstance(message, dict):  # Dictionary
            text = message.get("text", "")
            from_user = message.get("from", message.get("from_user", {}))
            if isinstance(from_user, dict):
                username = from_user.get("username", from_user.get("first_name", ""))
            else:
                username = ""
        else:
            text = ""
            username = ""

        return f"{username}: {text}"

    def _create_paragraph_from_message(
        self, message: Dict[str, Any], document_id: str, document_type: DocumentType, index: Optional[int] = None
    ) -> Paragraph:
        """–°–æ–∑–¥–∞–µ—Ç –ø–∞—Ä–∞–≥—Ä–∞—Ñ –∏–∑ —Å–æ–æ–±—â–µ–Ω–∏—è"""
        if isinstance(message, dict):
            text = message.get("text", "")
            author = message.get("from", message.get("from_user", {}))
            if isinstance(author, dict):
                author_name = author.get("username", author.get("first_name", ""))
                author_id = author.get("id")
            else:
                author_name = ""
                author_id = None

            timestamp = message.get("date") if isinstance(message.get("date"), datetime) else None
        else:
            text = ""
            author_name = ""
            author_id = None
            timestamp = None

        # –°–æ–∑–¥–∞–µ–º –ø–∞—Ä–∞–≥—Ä–∞—Ñ
        paragraph = Paragraph(
            id=self._create_paragraph_id(text, author_name, timestamp, index),
            content=text,
            author=author_name,
            author_id=author_id,
            timestamp=timestamp,
            document_id=document_id,
            document_type=document_type,
            paragraph_index=index,
        )

        # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥
        paragraph.embedding = self._create_embedding(text)

        return paragraph

    def _group_consecutive_messages(self, messages: List[Dict[str, Any]]) -> List[Paragraph]:
        """–ì—Ä—É–ø–ø–∏—Ä—É–µ—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è –æ–¥–Ω–æ–≥–æ –∞–≤—Ç–æ—Ä–∞ –≤ –æ–¥–∏–Ω –ø–∞—Ä–∞–≥—Ä–∞—Ñ"""
        if not messages:
            return []

        grouped_paragraphs = []
        current_author = None
        current_content: list[str] = []
        current_metadata: dict[str, Any] = {}
        current_timestamp = None

        for msg in messages:
            if isinstance(msg, dict):
                author_id: Optional[int] = None
                author_name: Optional[str] = None

                from_user = msg.get("from", msg.get("from_user", {}))
                if isinstance(from_user, dict):
                    author_id = from_user.get("id")
                    author_name = from_user.get("username") or from_user.get("first_name")

                # –ï—Å–ª–∏ —ç—Ç–æ –ø–µ—Ä–≤—ã–π —Å–æ–æ–±—â–µ–Ω–∏–µ –∏–ª–∏ –∞–≤—Ç–æ—Ä –∏–∑–º–µ–Ω–∏–ª—Å—è
                if current_author is None or current_author != author_id:
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–π –ø–∞—Ä–∞–≥—Ä–∞—Ñ, –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å
                    if current_content:
                        combined_content = "\n".join(current_content)
                        paragraph = Paragraph(
                            id=self._create_paragraph_id(combined_content, current_author),
                            content=combined_content,
                            author=current_author,
                            metadata=current_metadata.copy(),
                            timestamp=current_timestamp,
                        )
                        grouped_paragraphs.append(paragraph)

                    # –ù–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—ã–π –ø–∞—Ä–∞–≥—Ä–∞—Ñ
                    current_author = author_name or f"user_{author_id}" if author_id else "unknown"
                    current_content = []
                    current_metadata = {}
                    current_timestamp = msg.get("date") if isinstance(msg.get("date"), datetime) else None

                current_content.append(msg.get("text", ""))

        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –ø–∞—Ä–∞–≥—Ä–∞—Ñ
        if current_content:
            combined_content = "\n".join(current_content)
            paragraph = Paragraph(
                id=self._create_paragraph_id(combined_content, current_author),
                content=combined_content,
                author=current_author,
                metadata=current_metadata.copy(),
                timestamp=current_timestamp,
            )
            grouped_paragraphs.append(paragraph)

        for paragraph in grouped_paragraphs:
            paragraph.embedding = self._create_embedding(paragraph.content)

        return grouped_paragraphs

    def _classify_paragraph(self, paragraph: Paragraph, tag_service=None) -> Paragraph:
        """–ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç –ø–∞—Ä–∞–≥—Ä–∞—Ñ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–æ–¥—É–ª–µ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏.

        –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Ç–µ–≥–∏ –¥–ª—è –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞, —Ç–∞–∫ –∫–∞–∫ –æ–Ω –º–æ–∂–µ—Ç –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ
        –æ–ø–∏—Å—ã–≤–∞—Ç—å —É—è–∑–≤–∏–º–æ—Å—Ç–∏, —Ä–∏—Å–∫–∏ –∏ —Ä–µ—à–µ–Ω–∏—è.
        """
        try:
            # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ
            try:
                from api.detect.rolestate import classify_message_type
                from api.detect.factcheck import check_factuality
                from api.detect.localize import extract_location_and_time
                from api.detect.organism_detector import detect_organisms
                from api.detect.ecosystem_scaler import detect_ecosystems
            except ImportError:
                log("‚ö†Ô∏è –ú–æ–¥—É–ª–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é")
                return paragraph

            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≥–∏–±—Ä–∏–¥–Ω—É—é –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é (Weaviate + LLM) –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã –æ–±–∞ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞
            if tag_service and ENABLE_AUTOMATIC_DETECTORS and self._is_weaviate_available():
                # –ì–∏–±—Ä–∏–¥–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è: –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ—Ö–æ–∂–∏–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
                hybrid_tags = self._classify_with_hybrid_approach(paragraph, tag_service)
                if hybrid_tags:
                    paragraph.tags = hybrid_tags
                    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º classification enum
                    self._set_classification_from_tags(paragraph, hybrid_tags)
                    # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ç–µ–≥–æ–≤
                    tag_service.update_tag_usage(hybrid_tags)
                    # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–∏–º–µ—Ä—ã
                    for tag in hybrid_tags:
                        tag_service.add_example_to_tag(tag, paragraph.content[:200])
                else:
                    # Fallback –Ω–∞ –æ–±—ã—á–Ω—É—é LLM –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é
                    self._classify_with_llm_fallback_sync(paragraph, tag_service)
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—ã—á–Ω—É—é LLM –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é –µ—Å–ª–∏ —è–≤–Ω–æ –≤–∫–ª—é—á–µ–Ω—ã –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –¥–µ—Ç–µ–∫—Ç–æ—Ä—ã
            elif tag_service and ENABLE_AUTOMATIC_DETECTORS:
                import asyncio

                try:
                    # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π event loop –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
                    try:
                        loop = asyncio.get_event_loop()
                    except RuntimeError:
                        loop = asyncio.new_event_loop()
                        asyncio.set_event_loop(loop)

                    # –ü—Ä–µ–¥–ª–∞–≥–∞–µ–º —Ç–µ–≥–∏ —á–µ—Ä–µ–∑ LLM
                    suggested_tags = loop.run_until_complete(
                        tag_service.suggest_tags_for_paragraph(paragraph.content, paragraph.tags)
                    )
                    if suggested_tags:
                        paragraph.tags = suggested_tags
                        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º classification enum –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–µ—Ä–≤–æ–≥–æ —Ç–µ–≥–∞
                        if suggested_tags:
                            try:
                                classification_map = {
                                    "ecosystem_risk": ClassificationType.ECOSYSTEM_RISK,
                                    "ecosystem_vulnerability": ClassificationType.ECOSYSTEM_VULNERABILITY,
                                    "suggested_ecosystem_solution": ClassificationType.ECOSYSTEM_SOLUTION,
                                    "ecosystem_solution": ClassificationType.ECOSYSTEM_SOLUTION,
                                }
                                paragraph.classification = classification_map.get(suggested_tags[0])
                            except (ValueError, KeyError):
                                log(f"‚ö†Ô∏è –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –≤ —Ç–µ–≥–∞—Ö: {suggested_tags[0]}")
                        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—á–µ—Ç—á–∏–∫–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ç–µ–≥–æ–≤
                        tag_service.update_tag_usage(suggested_tags)
                        # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
                        for tag in suggested_tags:
                            tag_service.add_example_to_tag(tag, paragraph.content[:200])
                except Exception as e:
                    log(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏ —Ç–µ–≥–æ–≤ —á–µ—Ä–µ–∑ LLM: {e}")
                    # Fallback –Ω–∞ —Å—Ç–∞—Ä—ã–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä
                    classification_result = classify_message_type(paragraph.content)
                    if classification_result:
                        if isinstance(classification_result, str):
                            paragraph.tags = [classification_result]
                            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º classification enum –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç—Ä–æ–∫–∏
                            try:
                                classification_map = {
                                    "ecosystem_risk": ClassificationType.ECOSYSTEM_RISK,
                                    "ecosystem_vulnerability": ClassificationType.ECOSYSTEM_VULNERABILITY,
                                    "suggested_ecosystem_solution": ClassificationType.ECOSYSTEM_SOLUTION,
                                    "ecosystem_solution": ClassificationType.ECOSYSTEM_SOLUTION,
                                    "neutral": ClassificationType.NEUTRAL,
                                }
                                paragraph.classification = classification_map.get(classification_result)
                            except (ValueError, KeyError):
                                log(f"‚ö†Ô∏è –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏: {classification_result}")
                        elif isinstance(classification_result, list):
                            paragraph.tags = classification_result
                            # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π —Ç–µ–≥ –¥–ª—è classification
                            if classification_result:
                                try:
                                    classification_map = {
                                        "ecosystem_risk": ClassificationType.ECOSYSTEM_RISK,
                                        "ecosystem_vulnerability": ClassificationType.ECOSYSTEM_VULNERABILITY,
                                        "suggested_ecosystem_solution": ClassificationType.ECOSYSTEM_SOLUTION,
                                        "ecosystem_solution": ClassificationType.ECOSYSTEM_SOLUTION,
                                        "neutral": ClassificationType.NEUTRAL,
                                    }
                                    paragraph.classification = classification_map.get(classification_result[0])
                                except (ValueError, KeyError):
                                    log(f"‚ö†Ô∏è –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏: {classification_result[0]}")

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏
            fact_check_result = check_factuality(paragraph.content)
            if fact_check_result:
                paragraph.fact_check_result = FactCheckResult(fact_check_result.get("status", "unknown"))
                paragraph.fact_check_details = fact_check_result.get("details")

            # –õ–æ–∫–∞–ª–∏–∑–∞—Ü–∏—è (–º–µ—Å—Ç–æ –∏ –≤—Ä–µ–º—è)
            location_result = extract_location_and_time(paragraph.content)
            if location_result:
                paragraph.location = location_result.get("location")
                paragraph.time_reference = location_result.get("time_reference")

            # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –¥–µ—Ç–µ–∫—Ç–æ—Ä—ã (—ç–∫–æ—Å–∏—Å—Ç–µ–º—ã / –æ—Ä–≥–∞–Ω–∏–∑–º—ã) –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤
            # –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –æ—Ç–∫–ª—é—á–µ–Ω—ã, —á—Ç–æ–±—ã –Ω–µ –ª–æ–º–∞—Ç—å UX –±—ã—Å—Ç—Ä–æ–≥–æ –æ–±—â–µ–Ω–∏—è.
            if ENABLE_AUTOMATIC_DETECTORS:
                # –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —ç–∫–æ—Å–∏—Å—Ç–µ–º (–∏—Å–ø–æ–ª—å–∑—É—è –¥–∞–Ω–Ω—ã–µ –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏)
                try:
                    import asyncio

                    try:
                        loop = asyncio.get_event_loop()
                    except RuntimeError:
                        loop = asyncio.new_event_loop()
                        asyncio.set_event_loop(loop)

                    ecosystems = loop.run_until_complete(
                        detect_ecosystems(paragraph.content, location_data=location_result)
                    )

                    if ecosystems:
                        if not paragraph.metadata:
                            paragraph.metadata = {}
                        paragraph.metadata["ecosystems"] = ecosystems
                        log(f"‚úÖ –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {len(ecosystems)} —ç–∫–æ—Å–∏—Å—Ç–µ–º –≤ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–µ")
                except ImportError:
                    log("‚ö†Ô∏è –ú–æ–¥—É–ª—å –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è —ç–∫–æ—Å–∏—Å—Ç–µ–º –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                except Exception as e:
                    log(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–∏ —ç–∫–æ—Å–∏—Å—Ç–µ–º: {e}")

                # –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –æ—Ä–≥–∞–Ω–∏–∑–º–æ–≤
                try:
                    import asyncio

                    try:
                        loop = asyncio.get_event_loop()
                    except RuntimeError:
                        loop = asyncio.new_event_loop()
                        asyncio.set_event_loop(loop)

                    organisms = loop.run_until_complete(detect_organisms(paragraph.content))

                    if organisms:
                        # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –æ—Ä–≥–∞–Ω–∏–∑–º–æ–≤
                        try:
                            from api.classify.organism_classifier import classify_organisms_batch

                            classified_organisms = loop.run_until_complete(classify_organisms_batch(organisms))
                        except ImportError:
                            # –ï—Å–ª–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ
                            classified_organisms = organisms
                            log("‚ö†Ô∏è –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –æ—Ä–≥–∞–Ω–∏–∑–º–æ–≤ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ")

                        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–≥–∞–Ω–∏–∑–º—ã –≤ dedicated –ø–æ–ª–µ
                        paragraph.organisms = classified_organisms

                        # –¢–∞–∫–∂–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ metadata –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
                        if not paragraph.metadata:
                            paragraph.metadata = {}
                        paragraph.metadata["organisms"] = classified_organisms

                        log(f"‚úÖ –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ –∏ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–æ {len(classified_organisms)} –æ—Ä–≥–∞–Ω–∏–∑–º–æ–≤ –≤ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–µ")
                except ImportError:
                    log("‚ö†Ô∏è –ú–æ–¥—É–ª–∏ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –æ—Ä–≥–∞–Ω–∏–∑–º–æ–≤ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                except Exception as e:
                    log(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–∏ –æ—Ä–≥–∞–Ω–∏–∑–º–æ–≤: {e}")

        except Exception as e:
            log(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞: {e}")

        return paragraph

    def _is_weaviate_available(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å Weaviate –¥–ª—è –≥–∏–±—Ä–∏–¥–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏"""
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —É –Ω–∞—Å –µ—Å—Ç—å URL (—É–∂–µ –ø—Ä–æ–≤–µ—Ä–µ–Ω–æ –≤ __init__)
            return bool(WEAVIATE_URL)
        except:
            return False

    def _classify_with_hybrid_approach(self, paragraph: Paragraph, tag_service) -> Optional[List[str]]:
        """–ì–∏–±—Ä–∏–¥–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è: –∏—Å–ø–æ–ª—å–∑—É–µ—Ç Weaviate –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ—Ö–æ–∂–∏—Ö –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤"""
        try:
            # –ò—â–µ–º –ø–æ—Ö–æ–∂–∏–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã –≤ Weaviate
            similar_paragraphs = self._find_similar_classified_paragraphs(paragraph.content, limit=5)

            if not similar_paragraphs:
                log("ü§ñ –ì–∏–±—Ä–∏–¥–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è: –ø–æ—Ö–æ–∂–∏—Ö –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –∏—Å–ø–æ–ª—å–∑—É–µ–º LLM")
                return None

            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é –ø–æ—Ö–æ–∂–∏—Ö –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤
            tag_scores: dict[str, int] = {}
            classification_counts: dict[str, int] = {}

            for similar_para in similar_paragraphs:
                # –°–æ–±–∏—Ä–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —Ç–µ–≥–∞–º
                for tag in similar_para.tags:
                    tag_scores[tag] = tag_scores.get(tag, 0) + 1

                # –°–æ–±–∏—Ä–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —Ç–∏–ø–∞–º –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
                if similar_para.classification:
                    class_name = similar_para.classification.value
                    classification_counts[class_name] = classification_counts.get(class_name, 0) + 1

            # –í—ã–±–∏—Ä–∞–µ–º –Ω–∞–∏–±–æ–ª–µ–µ –≤–µ—Ä–æ—è—Ç–Ω—ã–µ —Ç–µ–≥–∏ (score > 1)
            candidate_tags = [tag for tag, score in tag_scores.items() if score > 1]

            if candidate_tags:
                log(f"ü§ñ –ì–∏–±—Ä–∏–¥–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è: –Ω–∞–π–¥–µ–Ω—ã –∫–∞–Ω–¥–∏–¥–∞—Ç—ã –∏–∑ –ø–æ—Ö–æ–∂–∏—Ö –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤: {candidate_tags}")

                # –ò—Å–ø–æ–ª—å–∑—É–µ–º LLM –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –ø–æ—Ö–æ–∂–∏—Ö –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤
                return self._refine_classification_with_llm(
                    paragraph.content, candidate_tags, similar_paragraphs[:2], tag_service
                )

            return None

        except Exception as e:
            log(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –≤ –≥–∏–±—Ä–∏–¥–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏: {e}")
            return None

    def _find_similar_classified_paragraphs(self, query: str, limit: int = 5) -> List[Paragraph]:
        """–ù–∞—Ö–æ–¥–∏—Ç –ø–æ—Ö–æ–∂–∏–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã, –∫–æ—Ç–æ—Ä—ã–µ —É–∂–µ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω—ã"""
        try:
            # –ò—â–µ–º –≤–æ –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö —Å —Ñ–∏–ª—å—Ç—Ä–æ–º –ø–æ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞–º
            # –≠—Ç–æ —É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è - –≤ —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –Ω—É–∂–Ω–æ —Ñ–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å –ø–æ –Ω–∞–ª–∏—á–∏—é —Ç–µ–≥–æ–≤
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º asyncio –¥–ª—è –≤—ã–∑–æ–≤–∞ async –º–µ—Ç–æ–¥–∞ –∏–∑ sync –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            try:
                loop = asyncio.get_event_loop()
                if loop.is_running():
                    # –ï—Å–ª–∏ event loop —É–∂–µ –∑–∞–ø—É—â–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º create_task
                    # –ù–æ —ç—Ç–æ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–µ—Ç –≤ sync –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ, –ø–æ—ç—Ç–æ–º—É –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫
                    log("‚ö†Ô∏è Event loop —É–∂–µ –∑–∞–ø—É—â–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤")
                    return []
                else:
                    results = loop.run_until_complete(self.search_similar_paragraphs(query, "all", top_k=limit * 2))
            except RuntimeError:
                # –ï—Å–ª–∏ –Ω–µ—Ç event loop, —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π
                results = asyncio.run(self.search_similar_paragraphs(query, "all", top_k=limit * 2))

            # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã
            classified_results = [para for para in results if para.tags and len(para.tags) > 0][:limit]

            log(f"ü§ñ –ù–∞–π–¥–µ–Ω–æ {len(classified_results)} –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–æ—Ö–æ–∂–∏—Ö –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤")
            return classified_results

        except Exception as e:
            log(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –ø–æ—Ö–æ–∂–∏—Ö –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤: {e}")
            return []

    def _classify_with_llm_fallback_sync(self, paragraph: Paragraph, tag_service):
        """Fallback –Ω–∞ –æ–±—ã—á–Ω—É—é LLM –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é (—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)"""
        import asyncio

        try:
            # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π event loop –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
            try:
                loop = asyncio.get_event_loop()
            except RuntimeError:
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)

            # –ü—Ä–µ–¥–ª–∞–≥–∞–µ–º —Ç–µ–≥–∏ —á–µ—Ä–µ–∑ LLM
            suggested_tags = loop.run_until_complete(
                tag_service.suggest_tags_for_paragraph(paragraph.content, paragraph.tags)
            )
            if suggested_tags:
                paragraph.tags = suggested_tags
                self._set_classification_from_tags(paragraph, suggested_tags)
                # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ç–µ–≥–æ–≤
                tag_service.update_tag_usage(suggested_tags)
                # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–∏–º–µ—Ä—ã
                for tag in suggested_tags:
                    tag_service.add_example_to_tag(tag, paragraph.content[:200])
        except Exception as e:
            log(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ LLM –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏: {e}")

    def _set_classification_from_tags(self, paragraph: Paragraph, tags: List[str]):
        """–£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç classification enum –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–≥–æ–≤"""
        if not tags:
            return

        classification_map = {
            "ecosystem_risk": ClassificationType.ECOSYSTEM_RISK,
            "ecosystem_vulnerability": ClassificationType.ECOSYSTEM_VULNERABILITY,
            "suggested_ecosystem_solution": ClassificationType.ECOSYSTEM_SOLUTION,
            "ecosystem_solution": ClassificationType.ECOSYSTEM_SOLUTION,
            "neutral": ClassificationType.NEUTRAL,
        }
        paragraph.classification = classification_map.get(tags[0])

    def _refine_classification_with_llm(
        self, content: str, candidate_tags: List[str], context_paragraphs: List[Paragraph], tag_service
    ) -> Optional[List[str]]:
        """–£—Ç–æ—á–Ω—è–µ—Ç –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é —Å –ø–æ–º–æ—â—å—é LLM, –∏—Å–ø–æ–ª—å–∑—É—è –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø–æ—Ö–æ–∂–∏—Ö –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤"""
        try:
            # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è —É—Ç–æ—á–Ω–µ–Ω–∏—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
            context = "\n".join(
                [f"–ü–æ—Ö–æ–∂–∏–π —Ç–µ–∫—Å—Ç: {p.content[:200]}... –¢–µ–≥–∏: {', '.join(p.tags)}" for p in context_paragraphs]
            )

            prompt = f"""–ù–∞ –æ—Å–Ω–æ–≤–µ —Å–ª–µ–¥—É—é—â–∏—Ö –ø–æ—Ö–æ–∂–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤ –∏ –∏—Ö –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏,
–æ–ø—Ä–µ–¥–µ–ª–∏ –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ–¥—Ö–æ–¥—è—â–∏–µ —Ç–µ–≥–∏ –¥–ª—è –Ω–æ–≤–æ–≥–æ —Ç–µ–∫—Å—Ç–∞.

–ü–û–•–û–ñ–ò–ï –¢–ï–ö–°–¢–´:
{context}

–ù–û–í–´–ô –¢–ï–ö–°–¢:
{content}

–ö–ê–ù–î–ò–î–ê–¢–´ –¢–ï–ì–û–í: {", ".join(candidate_tags)}

–í–µ—Ä–Ω–∏ —Ç–æ–ª—å–∫–æ —Å–ø–∏—Å–æ–∫ —Ç–µ–≥–æ–≤ —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é, –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤."""

            # –ò—Å–ø–æ–ª—å–∑—É–µ–º tag_service –¥–ª—è –≤—ã–∑–æ–≤–∞ LLM
            import asyncio

            try:
                loop = asyncio.get_event_loop()
            except RuntimeError:
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)

            result = loop.run_until_complete(tag_service.call_llm_for_tags(prompt))

            if result and isinstance(result, list):
                return result
            elif result and isinstance(result, str):
                return [tag.strip() for tag in result.split(",") if tag.strip()]

        except Exception as e:
            log(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ —É—Ç–æ—á–Ω–µ–Ω–∏–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏: {e}")

        return candidate_tags  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã—Ö –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏

    def add_documents(
        self,
        documents: List[Dict[str, Any]],
        document_id: str,
        document_type: DocumentType = DocumentType.KNOWLEDGE,
        classify: bool = True,
    ) -> int:
        """
        –î–æ–±–∞–≤–ª—è–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ Weaviate

        Args:
            documents: –°–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
            document_id: ID –¥–æ–∫—É–º–µ–Ω—Ç–∞
            document_type: –¢–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞ (—á–∞—Ç –∏–ª–∏ –∑–Ω–∞–Ω–∏–µ)
            classify: –í—ã–ø–æ–ª–Ω—è—Ç—å –ª–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é –∏ –ø—Ä–æ–≤–µ—Ä–∫—É –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏

        Returns:
            –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã—Ö –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤
        """
        if not documents:
            return 0

        # –°–æ–∑–¥–∞–µ–º –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        paragraphs = []
        for i, doc in enumerate(documents):
            if isinstance(doc, dict) and "text" in doc:
                paragraph = self._create_paragraph_from_message(doc, document_id, document_type, index=i)
                paragraphs.append(paragraph)
            elif isinstance(doc, str):
                paragraph = Paragraph(
                    id=self._create_paragraph_id(doc, index=i),
                    content=doc,
                    document_id=document_id,
                    document_type=document_type,
                    paragraph_index=i,
                )
                paragraph.embedding = self._create_embedding(doc)
                paragraphs.append(paragraph)

        if not paragraphs:
            return 0

        # –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ–º –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if classify:
            for paragraph in paragraphs:
                paragraph = self._classify_paragraph(paragraph)

        log(f"üîÑ –î–æ–±–∞–≤–ª–µ–Ω–∏–µ {len(paragraphs)} –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤ –≤ Weaviate –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ {document_id}...")

        # –î–æ–±–∞–≤–ª—è–µ–º –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã –≤ Weaviate –±–∞—Ç—á–∞–º–∏ (v4 API)
        from weaviate.classes.data import DataObject

        collection = self.client.collections.get(WEAVIATE_CLASS_NAME)
        added_count = 0

        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –æ–±—ä–µ–∫—Ç—ã –¥–ª—è batch insert
        objects_to_insert = []
        for paragraph in paragraphs:
            if paragraph.embedding is None:
                paragraph.embedding = self._create_embedding(paragraph.content)

            obj = self._paragraph_to_weaviate_object(paragraph)
            vector = paragraph.embedding.tolist()

            # –í v4 –∏—Å–ø–æ–ª—å–∑—É–µ–º UUID –∏–∑ paragraph.id –∏–ª–∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –Ω–æ–≤—ã–π
            para_uuid = paragraph.id.replace("para_", "") if paragraph.id.startswith("para_") else paragraph.id
            try:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –≤–∞–ª–∏–¥–Ω—ã–π –ª–∏ UUID
                para_uuid_obj = uuid.UUID(para_uuid)
            except (ValueError, AttributeError):
                # –ï—Å–ª–∏ –Ω–µ –≤–∞–ª–∏–¥–Ω—ã–π, –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –Ω–æ–≤—ã–π
                para_uuid_obj = uuid.uuid4()

            # –í v4 –∏—Å–ø–æ–ª—å–∑—É–µ–º DataObject –¥–ª—è –æ–±—ä–µ–∫—Ç–æ–≤ —Å –∫–∞—Å—Ç–æ–º–Ω—ã–º –≤–µ–∫—Ç–æ—Ä–æ–º
            objects_to_insert.append(
                DataObject(
                    uuid=para_uuid_obj,
                    properties=obj,
                    vector=vector,
                )
            )

        # –í—Å—Ç–∞–≤–ª—è–µ–º –±–∞—Ç—á–∞–º–∏
        if objects_to_insert:
            # –í v4 –∏—Å–ø–æ–ª—å–∑—É–µ–º insert_many –¥–ª—è batch –æ–ø–µ—Ä–∞—Ü–∏–π
            batch_size = WEAVIATE_BATCH_SIZE
            for i in range(0, len(objects_to_insert), batch_size):
                batch = objects_to_insert[i : i + batch_size]
                result = collection.data.insert_many(batch)
                added_count += len(batch)

        log(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ {added_count} –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤ –≤ Weaviate –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ {document_id}")
        return added_count

    def add_chat_messages(
        self, messages: List[Dict[str, Any]], chat_id: str, group_consecutive: bool = True, classify: bool = True
    ) -> int:
        """
        –î–æ–±–∞–≤–ª—è–µ—Ç —á–∞—Ç-—Å–æ–æ–±—â–µ–Ω–∏—è –≤ Weaviate

        Args:
            messages: –°–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
            chat_id: ID —á–∞—Ç–∞
            group_consecutive: –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∞—Ç—å –ª–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è –æ–¥–Ω–æ–≥–æ –∞–≤—Ç–æ—Ä–∞
            classify: –í—ã–ø–æ–ª–Ω—è—Ç—å –ª–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é –∏ –ø—Ä–æ–≤–µ—Ä–∫—É –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏

        Returns:
            –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã—Ö –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤
        """
        if not messages:
            return 0

        # –§–∏–ª—å—Ç—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è —Å —Ç–µ–∫—Å—Ç–æ–º
        valid_messages = []
        for msg in messages:
            if isinstance(msg, dict) and msg.get("text"):
                valid_messages.append(msg)
            elif hasattr(msg, "text") and msg.text:
                valid_messages.append(msg)

        if not valid_messages:
            return 0

        # –°–æ–∑–¥–∞–µ–º –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã
        if group_consecutive:
            paragraphs = self._group_consecutive_messages(valid_messages)
        else:
            paragraphs = []
            for i, msg in enumerate(valid_messages):
                paragraph = self._create_paragraph_from_message(msg, chat_id, DocumentType.CHAT, index=i)
                paragraphs.append(paragraph)

        if not paragraphs:
            return 0

        # –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ–º –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if classify:
            for paragraph in paragraphs:
                paragraph = self._classify_paragraph(paragraph)

        log(f"üîÑ –î–æ–±–∞–≤–ª–µ–Ω–∏–µ {len(paragraphs)} –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤ –≤ Weaviate –¥–ª—è —á–∞—Ç–∞ {chat_id}...")

        # –î–æ–±–∞–≤–ª—è–µ–º –≤ Weaviate –±–∞—Ç—á–∞–º–∏ (v4 API)
        from weaviate.classes.data import DataObject

        collection = self.client.collections.get(WEAVIATE_CLASS_NAME)
        added_count = 0

        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –æ–±—ä–µ–∫—Ç—ã –¥–ª—è batch insert
        objects_to_insert = []
        for paragraph in paragraphs:
            if paragraph.embedding is None:
                paragraph.embedding = self._create_embedding(paragraph.content)

            obj = self._paragraph_to_weaviate_object(paragraph)
            vector = paragraph.embedding.tolist()

            # –í v4 –∏—Å–ø–æ–ª—å–∑—É–µ–º UUID –∏–∑ paragraph.id –∏–ª–∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –Ω–æ–≤—ã–π
            para_uuid = paragraph.id.replace("para_", "") if paragraph.id.startswith("para_") else paragraph.id
            try:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –≤–∞–ª–∏–¥–Ω—ã–π –ª–∏ UUID
                para_uuid_obj = uuid.UUID(para_uuid)
            except (ValueError, AttributeError):
                # –ï—Å–ª–∏ –Ω–µ –≤–∞–ª–∏–¥–Ω—ã–π, –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –Ω–æ–≤—ã–π
                para_uuid_obj = uuid.uuid4()

            # –í v4 –∏—Å–ø–æ–ª—å–∑—É–µ–º DataObject –¥–ª—è –æ–±—ä–µ–∫—Ç–æ–≤ —Å –∫–∞—Å—Ç–æ–º–Ω—ã–º –≤–µ–∫—Ç–æ—Ä–æ–º
            objects_to_insert.append(
                DataObject(
                    uuid=para_uuid_obj,
                    properties=obj,
                    vector=vector,
                )
            )

        # –í—Å—Ç–∞–≤–ª—è–µ–º –±–∞—Ç—á–∞–º–∏
        if objects_to_insert:
            # –í v4 –∏—Å–ø–æ–ª—å–∑—É–µ–º insert_many –¥–ª—è batch –æ–ø–µ—Ä–∞—Ü–∏–π
            batch_size = WEAVIATE_BATCH_SIZE
            for i in range(0, len(objects_to_insert), batch_size):
                batch = objects_to_insert[i : i + batch_size]
                result = collection.data.insert_many(batch)
                added_count += len(batch)

        log(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ {added_count} –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤ –≤ Weaviate –¥–ª—è —á–∞—Ç–∞ {chat_id}")
        return added_count

    def _build_filters(
        self,
        document_id: Optional[str] = None,
        classification_filter: Optional[ClassificationType] = None,
        fact_check_filter: Optional[FactCheckResult] = None,
        location_filter: Optional[str] = None,
        ecosystem_id_filter: Optional[str] = None,
        organism_ids_filter: Optional[List[str]] = None,
        tags_filter: Optional[List[str]] = None,
        exclude_tags: Optional[List[str]] = None,
        timestamp_from: Optional[int] = None,
        timestamp_to: Optional[int] = None,
    ) -> Optional[Any]:  # type: ignore[return-value]
        """
        –°—Ç—Ä–æ–∏—Ç —Ñ–∏–ª—å—Ç—Ä Weaviate v4 —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π (OR, NOT, –¥–∏–∞–ø–∞–∑–æ–Ω—ã).

        Args:
            document_id: –§–∏–ª—å—Ç—Ä –ø–æ ID –¥–æ–∫—É–º–µ–Ω—Ç–∞
            classification_filter: –§–∏–ª—å—Ç—Ä –ø–æ —Ç–∏–ø—É –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
            fact_check_filter: –§–∏–ª—å—Ç—Ä –ø–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏
            location_filter: –§–∏–ª—å—Ç—Ä –ø–æ –ª–æ–∫–∞—Ü–∏–∏
            ecosystem_id_filter: –§–∏–ª—å—Ç—Ä –ø–æ ID —ç–∫–æ—Å–∏—Å—Ç–µ–º—ã
            organism_ids_filter: –§–∏–ª—å—Ç—Ä –ø–æ —Å–ø–∏—Å–∫—É ID –æ—Ä–≥–∞–Ω–∏–∑–º–æ–≤
            tags_filter: –§–∏–ª—å—Ç—Ä –ø–æ —Ç–µ–≥–∞–º (OR –ª–æ–≥–∏–∫–∞ - –ª—é–±–æ–π –∏–∑ —Ç–µ–≥–æ–≤)
            exclude_tags: –ò—Å–∫–ª—é—á–∏—Ç—å –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã —Å —ç—Ç–∏–º–∏ —Ç–µ–≥–∞–º–∏ (NOT –ª–æ–≥–∏–∫–∞)
            timestamp_from: –§–∏–ª—å—Ç—Ä –ø–æ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–º—É timestamp
            timestamp_to: –§–∏–ª—å—Ç—Ä –ø–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–º—É timestamp

        Returns:
            –û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π —Ñ–∏–ª—å—Ç—Ä –∏–ª–∏ None
        """
        filters = []

        # –§–∏–ª—å—Ç—Ä –ø–æ document_id
        if document_id:
            filters.append(Filter.by_property("document_id").equal(document_id))

        # –§–∏–ª—å—Ç—Ä –ø–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ (—á–µ—Ä–µ–∑ tags)
        if classification_filter:
            filters.append(Filter.by_property("tags").contains_any([classification_filter.value]))

        # –§–∏–ª—å—Ç—Ä –ø–æ fact_check_result
        if fact_check_filter:
            filters.append(Filter.by_property("fact_check_result").equal(fact_check_filter.value))

        # –§–∏–ª—å—Ç—Ä –ø–æ –ª–æ–∫–∞—Ü–∏–∏
        if location_filter:
            filters.append(Filter.by_property("location").equal(location_filter))

        # –§–∏–ª—å—Ç—Ä –ø–æ —ç–∫–æ—Å–∏—Å—Ç–µ–º–µ
        if ecosystem_id_filter:
            filters.append(Filter.by_property("ecosystem_id").equal(ecosystem_id_filter))

        # –§–∏–ª—å—Ç—Ä –ø–æ organism_ids
        if organism_ids_filter:
            filters.append(Filter.by_property("organism_ids").contains_any(organism_ids_filter))

        # –§–∏–ª—å—Ç—Ä –ø–æ —Ç–µ–≥–∞–º (OR –ª–æ–≥–∏–∫–∞ - –ª—é–±–æ–π –∏–∑ —Ç–µ–≥–æ–≤)
        if tags_filter:
            filters.append(Filter.by_property("tags").contains_any(tags_filter))

        # –ò—Å–∫–ª—é—á–µ–Ω–∏–µ —Ç–µ–≥–æ–≤ (NOT –ª–æ–≥–∏–∫–∞)
        # –í Weaviate v4 –∏—Å–ø–æ–ª—å–∑—É–µ–º contains_none –¥–ª—è –∏—Å–∫–ª—é—á–µ–Ω–∏—è —Ç–µ–≥–æ–≤
        if exclude_tags:
            filters.append(Filter.by_property("tags").contains_none(exclude_tags))

        # –§–∏–ª—å—Ç—Ä –ø–æ timestamp (–¥–∏–∞–ø–∞–∑–æ–Ω)
        if timestamp_from is not None or timestamp_to is not None:
            timestamp_filters = []
            if timestamp_from is not None:
                timestamp_filters.append(Filter.by_property("timestamp").greater_or_equal(timestamp_from))
            if timestamp_to is not None:
                timestamp_filters.append(Filter.by_property("timestamp").less_or_equal(timestamp_to))
            if timestamp_filters:
                filters.append(Filter.all_of(timestamp_filters))

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ñ–∏–ª—å—Ç—Ä—ã —á–µ—Ä–µ–∑ AND
        # –ü—Ä–∏–≤–æ–¥–∏–º –∫ Any –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å —Ç–∏–ø–∞–º–∏ Weaviate
        if len(filters) > 1:
            return cast(Any, Filter.all_of(filters))
        elif len(filters) == 1:
            return cast(Any, filters[0])
        else:
            return None

    def search_similar(
        self,
        query: str,
        document_id: str,
        top_k: int = 10,
        classification_filter: Optional[ClassificationType] = None,
        fact_check_filter: Optional[FactCheckResult] = None,
        location_filter: Optional[str] = None,
        ecosystem_id_filter: Optional[str] = None,
        organism_ids_filter: Optional[List[str]] = None,
        tags_filter: Optional[List[str]] = None,
        exclude_tags: Optional[List[str]] = None,
        timestamp_from: Optional[int] = None,
        timestamp_to: Optional[int] = None,
        use_hybrid: Optional[bool] = None,
        hybrid_alpha: Optional[float] = None,
    ) -> List[Tuple[Paragraph, float]]:
        """
        –ò—â–µ—Ç –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ—Ö–æ–∂–∏–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã –≤ Weaviate —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –ø–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º (v4 API).
        –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç Hybrid Search (–≤–µ–∫—Ç–æ—Ä–Ω—ã–π + BM25) –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏.

        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å.
            document_id: ID –¥–æ–∫—É–º–µ–Ω—Ç–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞.
            top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.
            classification_filter: –§–∏–ª—å—Ç—Ä –ø–æ —Ç–∏–ø—É –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏.
            fact_check_filter: –§–∏–ª—å—Ç—Ä –ø–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏.
            location_filter: –§–∏–ª—å—Ç—Ä –ø–æ –ª–æ–∫–∞—Ü–∏–∏.
            ecosystem_id_filter: –§–∏–ª—å—Ç—Ä –ø–æ ID —ç–∫–æ—Å–∏—Å—Ç–µ–º—ã.
            organism_ids_filter: –§–∏–ª—å—Ç—Ä –ø–æ —Å–ø–∏—Å–∫—É ID –æ—Ä–≥–∞–Ω–∏–∑–º–æ–≤.
            use_hybrid: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Hybrid Search (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏–∑ WEAVIATE_USE_HYBRID_SEARCH).
            hybrid_alpha: –ë–∞–ª–∞–Ω—Å –º–µ–∂–¥—É BM25 –∏ –≤–µ–∫—Ç–æ—Ä–Ω—ã–º –ø–æ–∏—Å–∫–æ–º (0-1, –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏–∑ WEAVIATE_HYBRID_ALPHA).

        Returns:
            –°–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (–ø–∞—Ä–∞–≥—Ä–∞—Ñ, –æ—Ü–µ–Ω–∫–∞ —Å—Ö–æ–∂–µ—Å—Ç–∏).
        """
        collection = self.client.collections.get(WEAVIATE_CLASS_NAME)

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ Hybrid Search
        use_hybrid_search = use_hybrid if use_hybrid is not None else WEAVIATE_USE_HYBRID_SEARCH
        alpha = hybrid_alpha if hybrid_alpha is not None else WEAVIATE_HYBRID_ALPHA

        # –°—Ç—Ä–æ–∏–º —Ñ–∏–ª—å—Ç—Ä —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π
        combined_filter = self._build_filters(
            document_id=document_id,
            classification_filter=classification_filter,
            fact_check_filter=fact_check_filter,
            location_filter=location_filter,
            ecosystem_id_filter=ecosystem_id_filter,
            organism_ids_filter=organism_ids_filter,
            tags_filter=tags_filter,
            exclude_tags=exclude_tags,
            timestamp_from=timestamp_from,
            timestamp_to=timestamp_to,
        )

        try:
            # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫: Hybrid Search –∏–ª–∏ –æ–±—ã—á–Ω—ã–π –≤–µ–∫—Ç–æ—Ä–Ω—ã–π
            if use_hybrid_search:
                try:
                    # Hybrid Search: –∫–æ–º–±–∏–Ω–∞—Ü–∏—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –∏ BM25
                    query_embedding = self._create_embedding(query).tolist()
                    response = collection.query.hybrid(
                        query=query,  # –¢–µ–∫—Å—Ç –¥–ª—è BM25
                        vector=query_embedding,  # –í–µ–∫—Ç–æ—Ä –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
                        alpha=alpha,  # –ë–∞–ª–∞–Ω—Å: 0 = —Ç–æ–ª—å–∫–æ BM25, 1 = —Ç–æ–ª—å–∫–æ –≤–µ–∫—Ç–æ—Ä–Ω—ã–π
                        fusion_type=HybridFusion.RELATIVE_SCORE,  # –û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Å–∫–æ—Ä–æ–≤
                        limit=top_k,
                        filters=cast(Any, combined_filter),  # type: ignore[arg-type]
                        return_metadata=MetadataQuery(score=True, distance=True),
                        include_vector=True,
                    )
                    log(f"üîç Hybrid Search (alpha={alpha}): BM25 + –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫")
                except Exception as hybrid_error:
                    # Fallback –Ω–∞ –æ–±—ã—á–Ω—ã–π –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ –µ—Å–ª–∏ Hybrid Search –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è
                    log(f"‚ö†Ô∏è Hybrid Search –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è, –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫: {hybrid_error}")
                    query_embedding = self._create_embedding(query).tolist()
                    response = collection.query.near_vector(
                        near_vector=query_embedding,
                        limit=top_k,
                        filters=cast(Any, combined_filter),  # type: ignore[arg-type]
                        return_metadata=MetadataQuery(distance=True),
                        include_vector=True,
                    )
                    log("üîç –í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ (fallback)")
            else:
                # –û–±—ã—á–Ω—ã–π –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫
                query_embedding = self._create_embedding(query).tolist()
                response = collection.query.near_vector(
                    near_vector=query_embedding,
                    limit=top_k,
                    filters=cast(Any, combined_filter),  # type: ignore[arg-type]
                    return_metadata=MetadataQuery(distance=True),
                    include_vector=True,
                )
                log("üîç –í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫")

            results = []
            for obj in response.objects:
                # –ü–æ–ª—É—á–∞–µ–º –≤–µ–∫—Ç–æ—Ä –∏–∑ –æ–±—ä–µ–∫—Ç–∞
                vector = None
                if obj.vector is not None:
                    if isinstance(obj.vector, dict):
                        # Named vectors - –∏–∑–≤–ª–µ–∫–∞–µ–º default –∏–ª–∏ –ø–µ—Ä–≤—ã–π –¥–æ—Å—Ç—É–ø–Ω—ã–π
                        if "default" in obj.vector:
                            vector_list = obj.vector["default"]
                        elif len(obj.vector) > 0:
                            vector_list = list(obj.vector.values())[0]
                        else:
                            vector_list = None

                        if vector_list and isinstance(vector_list, (list, tuple)):
                            vector = np.array(vector_list, dtype=np.float32)
                    elif isinstance(obj.vector, (list, tuple)):
                        vector = np.array(obj.vector, dtype=np.float32)

                paragraph = self._weaviate_object_to_paragraph(obj, vector=vector)

                # –ü–æ–ª—É—á–∞–µ–º score (–¥–ª—è Hybrid Search) –∏–ª–∏ distance (–¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞)
                similarity = 0.0
                if obj.metadata:
                    # –î–ª—è Hybrid Search –∏—Å–ø–æ–ª—å–∑—É–µ–º score
                    if hasattr(obj.metadata, "score") and obj.metadata.score is not None:
                        try:
                            score_val = obj.metadata.score
                            if isinstance(score_val, (int, float)):
                                similarity = float(score_val)
                            elif isinstance(score_val, dict):
                                # –ï—Å–ª–∏ score –ø—Ä–∏—Ö–æ–¥–∏—Ç –∫–∞–∫ dict, –∏—Å–ø–æ–ª—å–∑—É–µ–º default
                                similarity = 0.5
                            else:
                                similarity = float(score_val) if score_val is not None else 0.0
                        except (ValueError, TypeError) as e:
                            log(f"‚ö†Ô∏è Cannot convert score {obj.metadata.score} to float: {e}")
                            similarity = 0.0
                    # –î–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º distance
                    elif hasattr(obj.metadata, "distance") and obj.metadata.distance is not None:
                        try:
                            distance_val = obj.metadata.distance
                            if isinstance(distance_val, (int, float)):
                                distance = float(distance_val)
                            elif isinstance(distance_val, dict):
                                log(f"‚ö†Ô∏è distance is dict: {distance_val}")
                                distance = 1.0
                            else:
                                distance = float(distance_val) if distance_val is not None else 1.0
                            similarity = 1.0 - distance  # –î–ª—è –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è
                        except (ValueError, TypeError) as e:
                            log(f"‚ö†Ô∏è Cannot convert distance {obj.metadata.distance} to float: {e}")
                            similarity = 0.0

                results.append((paragraph, float(similarity)))

            return results
        except Exception as e:
            log(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –≤ Weaviate: {e}")
            return []

    def search_with_reranking(
        self, query: str, document_id: str, top_k: int = 10, rerank_limit: Optional[int] = None, **filters
    ) -> List[Tuple[Paragraph, float]]:
        """
        –ü–æ–∏—Å–∫ —Å –ø–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ–º —á–µ—Ä–µ–∑ Cross-Encoder –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏.

        –î–≤—É—Ö—ç—Ç–∞–ø–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å:
        1. –ë—ã—Å—Ç—Ä—ã–π –ø–µ—Ä–≤–∏—á–Ω—ã–π –ø–æ–∏—Å–∫ (–ø–æ–ª—É—á–∞–µ–º –±–æ–ª—å—à–µ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤)
        2. –ü–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ —á–µ—Ä–µ–∑ Cross-Encoder (—Ç–æ—á–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏)

        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            document_id: ID –¥–æ–∫—É–º–µ–Ω—Ç–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞
            top_k: –§–∏–Ω–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            rerank_limit: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –¥–ª—è reranking (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏–∑ WEAVIATE_RERANK_LIMIT)
            **filters: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ñ–∏–ª—å—Ç—Ä—ã –¥–ª—è –ø–æ–∏—Å–∫–∞

        Returns:
            –°–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (–ø–∞—Ä–∞–≥—Ä–∞—Ñ, –æ—Ü–µ–Ω–∫–∞ —Å—Ö–æ–∂–µ—Å—Ç–∏) –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        """
        rerank_limit = rerank_limit or WEAVIATE_RERANK_LIMIT

        # 1. –ü–µ—Ä–≤–∏—á–Ω—ã–π –ø–æ–∏—Å–∫ (–±—ã—Å—Ç—Ä—ã–π, –º–Ω–æ–≥–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤)
        candidates = self.search_similar(query=query, document_id=document_id, top_k=rerank_limit, **filters)

        if not candidates or len(candidates) <= top_k:
            # –ï—Å–ª–∏ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –º–∞–ª–æ, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ –µ—Å—Ç—å
            return candidates[:top_k]

        # 2. –ü–µ—Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ —á–µ—Ä–µ–∑ Cross-Encoder
        try:
            from sentence_transformers import CrossEncoder

            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ª–µ–≥–∫–æ–≤–µ—Å–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è reranking
            # –ú–æ–∂–Ω–æ –∑–∞–º–µ–Ω–∏—Ç—å –Ω–∞ –±–æ–ª–µ–µ –º–æ—â–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è –ª—É—á—à–µ–π —Ç–æ—á–Ω–æ—Å—Ç–∏
            cross_encoder = CrossEncoder("cross-encoder/ms-marco-MiniLM-L-6-v2")

            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø–∞—Ä (–∑–∞–ø—Ä–æ—Å, –¥–æ–∫—É–º–µ–Ω—Ç)
            pairs = [(query, para.content) for para, _ in candidates]

            # –ü–æ–ª—É—á–∞–µ–º –Ω–æ–≤—ã–µ —Å–∫–æ—Ä—ã —á–µ—Ä–µ–∑ Cross-Encoder
            rerank_scores = cross_encoder.predict(pairs)

            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –Ω–æ–≤—ã–º —Å–∫–æ—Ä–∞–º
            reranked = sorted(zip(candidates, rerank_scores), key=lambda x: x[1], reverse=True)

            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ø-k —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            results = [(para, float(score)) for (para, _), score in reranked[:top_k]]
            log(f"üéØ Reranking: {len(candidates)} –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ ‚Üí {len(results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
            return results

        except ImportError:
            log("‚ö†Ô∏è Cross-Encoder –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º reranking. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install sentence-transformers")
            return candidates[:top_k]
        except Exception as e:
            log(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ reranking: {e}, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –±–µ–∑ reranking")
            return candidates[:top_k]

    async def search_similar_paragraphs(
        self,
        query: str,
        document_id: str,
        top_k: int = 10,
        classification_filter: Optional[ClassificationType] = None,
        fact_check_filter: Optional[FactCheckResult] = None,
        location_filter: Optional[str] = None,
        ecosystem_id_filter: Optional[str] = None,
        organism_ids_filter: Optional[List[str]] = None,
        use_reranking: Optional[bool] = None,
    ) -> List[Paragraph]:
        """
        –ò—â–µ—Ç –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ—Ö–æ–∂–∏–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã, –≤–æ–∑–≤—Ä–∞—â–∞—è —Ç–æ–ª—å–∫–æ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã –±–µ–∑ –æ—Ü–µ–Ω–æ–∫.
        –ï—Å–ª–∏ –ø—Ä—è–º—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –º–∞–ª–æ –∏–ª–∏ –Ω–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç LLM –¥–ª—è –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∞–Ω–∏—è.
        –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç Cross-Encoder Reranking –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏.

        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            document_id: ID –¥–æ–∫—É–º–µ–Ω—Ç–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞
            top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            classification_filter: –§–∏–ª—å—Ç—Ä –ø–æ —Ç–∏–ø—É –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
            fact_check_filter: –§–∏–ª—å—Ç—Ä –ø–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏
            location_filter: –§–∏–ª—å—Ç—Ä –ø–æ –ª–æ–∫–∞—Ü–∏–∏
            ecosystem_id_filter: –§–∏–ª—å—Ç—Ä –ø–æ ID —ç–∫–æ—Å–∏—Å—Ç–µ–º—ã
            organism_ids_filter: –§–∏–ª—å—Ç—Ä –ø–æ —Å–ø–∏—Å–∫—É ID –æ—Ä–≥–∞–Ω–∏–∑–º–æ–≤
            use_reranking: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Cross-Encoder Reranking (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏–∑ WEAVIATE_USE_RERANKING)

        Returns:
            –°–ø–∏—Å–æ–∫ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤ –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        """
        use_rerank = use_reranking if use_reranking is not None else WEAVIATE_USE_RERANKING

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º reranking –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ
        if use_rerank:
            similar_pairs = self.search_with_reranking(
                query=query,
                document_id=document_id,
                top_k=top_k,
                classification_filter=classification_filter,
                fact_check_filter=fact_check_filter,
                location_filter=location_filter,
                ecosystem_id_filter=ecosystem_id_filter,
                organism_ids_filter=organism_ids_filter,
            )
        else:
            # –û–±—ã—á–Ω—ã–π –ø–æ–∏—Å–∫
            similar_pairs = self.search_similar(
                query,
                document_id,
                top_k,
                classification_filter,
                fact_check_filter,
                location_filter,
                ecosystem_id_filter,
                organism_ids_filter,
            )

        # –ï—Å–ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∏—Ö
        if len(similar_pairs) >= 3:
            return [para for para, score in similar_pairs]

        log(f"üîç –ú–∞–ª–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ ({len(similar_pairs)}), –ø—Ä–æ–±—É–µ–º –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∞—Ç—å –∑–∞–ø—Ä–æ—Å: '{query}'")
        from api.llm import rephrase_search_query

        rephrased_queries = await rephrase_search_query(query)

        all_results = {}  # –ò—Å–ø–æ–ª—å–∑—É–µ–º dict –¥–ª—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏ –ø–æ paragraph_id

        # –î–æ–±–∞–≤–ª—è–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        for para, score in similar_pairs:
            all_results[para.id] = (para, score)

        # –ò—â–µ–º –ø–æ –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –∑–∞–ø—Ä–æ—Å–∞–º
        for new_query in rephrased_queries:
            log(f"üîÑ –ü–æ–∏—Å–∫ –ø–æ –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–º—É –∑–∞–ø—Ä–æ—Å—É: '{new_query}'")
            new_pairs = self.search_similar(
                new_query,
                document_id,
                top_k=3,
                classification_filter=classification_filter,
                fact_check_filter=fact_check_filter,
                location_filter=location_filter,
                ecosystem_id_filter=ecosystem_id_filter,
                organism_ids_filter=organism_ids_filter,
            )
            for para, score in new_pairs:
                # –ï—Å–ª–∏ –ø–∞—Ä–∞–≥—Ä–∞—Ñ —É–∂–µ –µ—Å—Ç—å, –æ—Å—Ç–∞–≤–ª—è–µ–º —Å –ª—É—á—à–∏–º —Å–∫–æ—Ä–æ–º
                if para.id not in all_results or score > all_results[para.id][1]:
                    all_results[para.id] = (para, score)

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Å–∫–æ—Ä—É
        sorted_results = sorted(all_results.values(), key=lambda x: x[1], reverse=True)

        return [para for para, score in sorted_results[:top_k]]

    def get_paragraph_by_id(self, document_id: str, paragraph_id: str) -> Optional[Paragraph]:
        """–ü–æ–ª—É—á–∞–µ—Ç –ø–∞—Ä–∞–≥—Ä–∞—Ñ –ø–æ ID –∏–∑ Weaviate (v4 API).

        Args:
            document_id: ID –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å FAISSStorage, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏)
            paragraph_id: ID –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞ (UUID –≤ Weaviate)
        """
        try:
            collection = self.client.collections.get(WEAVIATE_CLASS_NAME)

            # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å –æ–±—ä–µ–∫—Ç –Ω–∞–ø—Ä—è–º—É—é –ø–æ UUID
            try:
                para_uuid = paragraph_id.replace("para_", "") if paragraph_id.startswith("para_") else paragraph_id
                para_uuid_obj = uuid.UUID(para_uuid)

                obj = collection.query.fetch_object_by_id(
                    uuid=para_uuid_obj,
                    include_vector=True,
                )

                if obj:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º document_id, –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω
                    if document_id and obj.properties.get("document_id") != document_id:
                        return None
                    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ Paragraph (–≤–µ–∫—Ç–æ—Ä –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è –≤–Ω—É—Ç—Ä–∏ _weaviate_object_to_paragraph)
                    return self._weaviate_object_to_paragraph(obj, vector=None)
            except (ValueError, AttributeError) as e:
                # –ï—Å–ª–∏ –Ω–µ –≤–∞–ª–∏–¥–Ω—ã–π UUID, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∑–∞–ø—Ä–æ—Å —Å —Ñ–∏–ª—å—Ç—Ä–æ–º
                if not document_id:
                    return None

                # –ò—â–µ–º –ø–æ document_id –∏ –ø—Ä–æ–≤–µ—Ä—è–µ–º paragraph_id –≤—Ä—É—á–Ω—É—é
                filters = [Filter.by_property("document_id").equal(document_id)]

                response = collection.query.fetch_objects(  # type: ignore
                    limit=10000,  # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞
                    filters=Filter.all_of(filters) if len(filters) > 1 else filters[0],
                    include_vector=True,
                )

                for obj in response.objects:  # type: ignore
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º UUID
                    if str(obj.uuid) == paragraph_id or str(obj.uuid) == para_uuid:
                        # –í–µ–∫—Ç–æ—Ä –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è –≤–Ω—É—Ç—Ä–∏ _weaviate_object_to_paragraph
                        return self._weaviate_object_to_paragraph(obj, vector=None)

            return None
        except Exception as e:
            log(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞ {paragraph_id} –∏–∑ Weaviate: {e}")
            return None

    def get_document_paragraphs(self, document_id: str) -> List[Paragraph]:
        """–ü–æ–ª—É—á–∞–µ—Ç –≤—Å–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏–∑ Weaviate (v4 API)."""
        try:
            collection = self.client.collections.get(WEAVIATE_CLASS_NAME)

            response = collection.query.fetch_objects(
                filters=Filter.by_property("document_id").equal(document_id),
                limit=10000,  # TODO: Implement pagination if documents can be very large
                include_vector=True,
            )

            paragraphs = []
            for obj in response.objects:
                try:
                    vector = None
                    if obj.vector is not None:
                        # –í v4 –≤–µ–∫—Ç–æ—Ä—ã –º–æ–≥—É—Ç –±—ã—Ç—å dict –¥–ª—è named vectors –∏–ª–∏ list –¥–ª—è default
                        if isinstance(obj.vector, dict):
                            # –ï—Å–ª–∏ —ç—Ç–æ dict, –ø—Ä–æ–±—É–µ–º –∏–∑–≤–ª–µ—á—å default –≤–µ–∫—Ç–æ—Ä –∏–ª–∏ –ø–µ—Ä–≤—ã–π –¥–æ—Å—Ç—É–ø–Ω—ã–π
                            if "default" in obj.vector:
                                vector_list = obj.vector["default"]
                            elif len(obj.vector) > 0:
                                # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π –¥–æ—Å—Ç—É–ø–Ω—ã–π –≤–µ–∫—Ç–æ—Ä
                                vector_list = list(obj.vector.values())[0]
                            else:
                                vector_list = None

                            if vector_list and isinstance(vector_list, (list, tuple)):
                                vector = np.array(vector_list, dtype=np.float32)
                            else:
                                log(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –≤–µ–∫—Ç–æ—Ä –∏–∑ dict: {obj.vector}")
                        elif isinstance(obj.vector, (list, tuple)):
                            vector = np.array(obj.vector, dtype=np.float32)
                        else:
                            log(f"‚ö†Ô∏è –ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Ç–∏–ø vector: {type(obj.vector)}")
                    paragraph = self._weaviate_object_to_paragraph(obj, vector=vector)
                    paragraphs.append(paragraph)
                except Exception as e:
                    log(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ–±—ä–µ–∫—Ç–∞ {obj.uuid}: {e}")
                    import traceback

                    log(f"Traceback: {traceback.format_exc()}")
                    # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º, —á—Ç–æ–±—ã –Ω–µ –ø–∞–¥–∞—Ç—å –Ω–∞ –æ–¥–Ω–æ–º –æ–±—ä–µ–∫—Ç–µ
                    continue

            return paragraphs
        except Exception as e:
            log(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞ {document_id} –∏–∑ Weaviate: {e}")
            import traceback

            log(f"Traceback: {traceback.format_exc()}")
            return []

    def get_all_documents(self) -> List[str]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö document_id –∏–∑ Weaviate (v4 API)."""
        try:
            collection = self.client.collections.get(WEAVIATE_CLASS_NAME)

            # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –æ–±—ä–µ–∫—Ç—ã —Å document_id (—Å –ø–∞–≥–∏–Ω–∞—Ü–∏–µ–π –¥–ª—è –±–æ–ª—å—à–∏—Ö –∫–æ–ª–ª–µ–∫—Ü–∏–π)
            document_ids = set()
            limit = 1000
            offset = 0

            while True:
                response = collection.query.fetch_objects(
                    limit=limit,
                    offset=offset,
                    return_properties=["document_id"],
                )

                if not response.objects:
                    break

                for obj in response.objects:
                    doc_id = obj.properties.get("document_id")
                    if doc_id:
                        document_ids.add(doc_id)

                if len(response.objects) < limit:
                    break

                offset += limit

            return sorted([str(doc_id) for doc_id in document_ids])
        except Exception as e:
            log(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å–ø–∏—Å–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ Weaviate: {e}")
            return []

    def update_paragraph(self, document_id: str, paragraph: Paragraph) -> bool:
        """
        –û–±–Ω–æ–≤–ª—è–µ—Ç –ø–∞—Ä–∞–≥—Ä–∞—Ñ –≤ Weaviate (v4 API).

        Args:
            document_id: ID –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å FAISSStorage)
            paragraph: –û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π –ø–∞—Ä–∞–≥—Ä–∞—Ñ.

        Returns:
            True –µ—Å–ª–∏ —É—Å–ø–µ—à–Ω–æ –æ–±–Ω–æ–≤–ª–µ–Ω–æ.
        """
        if not paragraph.id:
            log("‚ùå –ù–µ–≤–æ–∑–º–æ–∂–Ω–æ –æ–±–Ω–æ–≤–∏—Ç—å –ø–∞—Ä–∞–≥—Ä–∞—Ñ: –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç ID")
            return False

        try:
            collection = self.client.collections.get(WEAVIATE_CLASS_NAME)

            # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥, –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
            if paragraph.embedding is None:
                paragraph.embedding = self._create_embedding(paragraph.content)

            obj = self._paragraph_to_weaviate_object(paragraph)
            vector = paragraph.embedding.tolist()

            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º paragraph.id –≤ UUID
            para_uuid = paragraph.id.replace("para_", "") if paragraph.id.startswith("para_") else paragraph.id
            try:
                para_uuid_obj = uuid.UUID(para_uuid)
            except (ValueError, AttributeError):
                log(f"‚ùå –ù–µ–≤–æ–∑–º–æ–∂–Ω–æ –æ–±–Ω–æ–≤–∏—Ç—å –ø–∞—Ä–∞–≥—Ä–∞—Ñ: –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–π UUID {paragraph.id}")
                return False

            # –û–±–Ω–æ–≤–ª—è–µ–º –æ–±—ä–µ–∫—Ç –≤ Weaviate v4
            collection.data.update(
                uuid=para_uuid_obj,
                properties=obj,
                vector=vector,
            )
            log(f"‚úÖ –ü–∞—Ä–∞–≥—Ä–∞—Ñ {paragraph.id} —É—Å–ø–µ—à–Ω–æ –æ–±–Ω–æ–≤–ª–µ–Ω –≤ Weaviate")
            return True
        except Exception as e:
            log(f"‚ùå –û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞ {paragraph.id} –≤ Weaviate: {e}")
            return False

    def delete_paragraph(self, _document_id: str, paragraph_id: str) -> bool:
        """
        –£–¥–∞–ª—è–µ—Ç –ø–∞—Ä–∞–≥—Ä–∞—Ñ –∏–∑ Weaviate (v4 API).

        Args:
            document_id: ID –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å FAISSStorage)
            paragraph_id: ID –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞.

        Returns:
            True –µ—Å–ª–∏ —É—Å–ø–µ—à–Ω–æ —É–¥–∞–ª–µ–Ω–æ.
        """
        try:
            collection = self.client.collections.get(WEAVIATE_CLASS_NAME)

            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º paragraph_id –≤ UUID
            para_uuid = paragraph_id.replace("para_", "") if paragraph_id.startswith("para_") else paragraph_id
            try:
                para_uuid_obj = uuid.UUID(para_uuid)
            except (ValueError, AttributeError):
                log(f"‚ùå –ù–µ–≤–æ–∑–º–æ–∂–Ω–æ —É–¥–∞–ª–∏—Ç—å –ø–∞—Ä–∞–≥—Ä–∞—Ñ: –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–π UUID {paragraph_id}")
                return False

            # –£–¥–∞–ª—è–µ–º –æ–±—ä–µ–∫—Ç –≤ Weaviate v4
            collection.data.delete_by_id(uuid=para_uuid_obj)
            log(f"‚úÖ –ü–∞—Ä–∞–≥—Ä–∞—Ñ {paragraph_id} —É—Å–ø–µ—à–Ω–æ —É–¥–∞–ª–µ–Ω –∏–∑ Weaviate")
            return True
        except Exception as e:
            log(f"‚ùå –û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞ {paragraph_id} –∏–∑ Weaviate: {e}")
            return False

    def reclassify_paragraph(self, document_id: str, paragraph_id: str, tag_service=None) -> bool:
        """–ü–µ—Ä–µ–∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç –ø–∞—Ä–∞–≥—Ä–∞—Ñ –∏ –æ–±–Ω–æ–≤–ª—è–µ—Ç –µ–≥–æ –≤ Weaviate."""
        paragraph = self.get_paragraph_by_id(document_id, paragraph_id)
        if not paragraph:
            return False

        # –û–±–Ω–æ–≤–ª—è–µ–º –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é
        tag_service = tag_service or getattr(self, "_tag_service", None)
        paragraph = self._classify_paragraph(paragraph, tag_service=tag_service)

        # –û–±–Ω–æ–≤–ª—è–µ–º –ø–∞—Ä–∞–≥—Ä–∞—Ñ –≤ Weaviate
        return self.update_paragraph(document_id, paragraph)

    def reclassify_document(self, document_id: str) -> int:
        """–ü–µ—Ä–µ–∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç –≤—Å–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ –∏ –æ–±–Ω–æ–≤–ª—è–µ—Ç –∏—Ö –≤ Weaviate (v4 API)."""
        paragraphs = self.get_document_paragraphs(document_id)
        updated_count = 0

        for paragraph in paragraphs:
            if self.reclassify_paragraph(document_id, paragraph.id):
                updated_count += 1

        return updated_count
