# Накопление семантической базы знаний в Weaviate во время чата

## Обзор

**Во время разговора с LLM система автоматически накапливает семантическую базу знаний в Weaviate**, и это делает последующие разговоры все более интересными и контекстуальными.

## Как это работает

### 1. Сохранение сообщений в Weaviate

**При каждом сообщении:**

```python
# api/chat/routes.py:248-254
await paragraph_service.save_chat_message_paragraphs(
    message_content=message_data.message,
    session_id=actual_session_id,
    author=message_data.author,
    author_id=None,
    timestamp=user_message_node.timestamp,
)
```

**Что происходит:**
1. Сообщение пользователя разбивается на параграфы
2. Каждый параграф векторизуется (создается эмбеддинг)
3. Параграфы сохраняются в Weaviate с метаданными:
   - `session_id` (ID сессии чата)
   - `author` (автор сообщения)
   - `document_type: CHAT` (тип документа)
   - `timestamp` (временная метка)
   - Автоматическая классификация (теги, локация, экосистемы, организмы)

**То же самое для ответов LLM:**

```python
# api/chat/routes.py:413-419
await paragraph_service.save_chat_message_paragraphs(
    message_content=response_content,
    session_id=actual_session_id,
    author="assistant",
    author_id=None,
    timestamp=ai_response_node.timestamp,
)
```

### 2. Использование накопленных знаний

**При каждом новом сообщении система:**

#### a) Ищет релевантные параграфы из истории

```python
# api/chat/context_builder.py:137-139
similar_paragraphs = await storage.search_similar_paragraphs(
    query=message, 
    document_id=session_id, 
    top_k=5
)
```

**Семантический поиск находит:**
- Похожие темы из предыдущих сообщений
- Связанные концепции из текущей сессии
- Релевантные ответы LLM из истории

#### b) Строит графовый контекст

```python
# api/chat/context_builder.py:143-147
graph_context = await graph_builder.build_graph_augmented_context(
    paragraphs=similar_paragraphs,
    max_depth=2,
    max_relationships=10,
)
```

**Граф связей:**
- Находит симбиотические связи между организмами
- Обнаруживает экосистемные паттерны
- Выявляет причинно-следственные связи

#### c) Формирует контекст для LLM

```python
# api/chat/routes.py:362-373
llm_context = prompt_template.format(
    conversation_summary=f"[CONVERSATION CONTEXT]\n{conversation_summary}\n",
    recent_messages=f"[RECENT MESSAGES]\n{recent_messages}\n",
    ecosystem_context=f"[LOCAL ECOSYSTEM]\n{ecosystem_context}\n",
    graph_context=f"[SYMBIOTIC RELATIONSHIPS]\n{graph_context}\n",
    message=message_data.message,
)
```

**LLM получает:**
- Сводку предыдущих сообщений
- Последние релевантные сообщения
- Контекст экосистемы (если определен)
- Граф симбиотических связей
- Релевантные параграфы из базы знаний

## Почему разговор становится интереснее

### 1. **Контекстуальная память**

Система помнит:
- Что обсуждалось ранее в сессии
- Какие концепции были упомянуты
- Какие связи были обнаружены
- Какие экосистемы и организмы упоминались

**Пример:**
```
Пользователь: "Расскажи про дуб"
LLM: [отвечает про дуб]

Пользователь: "А что с ним связано?"
LLM: [использует семантический поиск, находит предыдущее сообщение про дуб,
      ищет связанные организмы - микориза, жёлуди, белки и т.д.]
```

### 2. **Семантический поиск вместо точного совпадения**

Weaviate использует векторный поиск, который понимает **смысл**, а не только ключевые слова:

```
Запрос: "растения которые помогают друг другу"
Найдет: "симбиоз между дубом и микоризой"
         "взаимодействие растений в экосистеме"
         "мутуализм в природе"
```

### 3. **Автоматическая классификация**

Каждое сообщение автоматически классифицируется:

```python
# api/storage/weaviate_storage.py:435-618
def _classify_paragraph(self, paragraph: Paragraph):
    # Определяет теги (ecosystem_risk, ecosystem_vulnerability, etc.)
    # Обнаруживает организмы
    # Обнаруживает экосистемы
    # Извлекает локацию
    # Проверяет достоверность
```

**Это позволяет:**
- Фильтровать по типам контента
- Находить похожие темы по тегам
- Группировать связанные концепции

### 4. **Граф симбиотических связей**

Система строит граф связей между:
- Организмами
- Экосистемами
- Концепциями
- Локациями

**Это позволяет LLM:**
- Видеть связи между разными темами
- Предлагать связанные концепции
- Объяснять сложные экосистемные взаимодействия

## Пример работы

### Первое сообщение

```
Пользователь: "Что такое симбиоз?"
LLM: [отвечает, объясняет концепцию]
```

**Что сохраняется в Weaviate:**
- Вопрос пользователя (векторизован, классифицирован)
- Ответ LLM (векторизован, классифицирован)
- Обнаруженные организмы (если упомянуты)
- Обнаруженные экосистемы (если упомянуты)
- Теги: "symbiosis", "mutualism", etc.

### Второе сообщение

```
Пользователь: "Приведи пример"
LLM: [использует семантический поиск]
     [находит предыдущее сообщение про симбиоз]
     [находит связанные примеры из базы знаний]
     [отвечает с контекстом предыдущего разговора]
```

**Что происходит:**
1. Семантический поиск находит предыдущее сообщение про симбиоз
2. Граф связей находит примеры симбиоза из базы знаний
3. LLM получает контекст и дает более релевантный ответ

### Третье сообщение

```
Пользователь: "А что с дубом?"
LLM: [понимает, что речь про симбиоз с дубом]
     [находит все упоминания дуба в истории]
     [находит связанные организмы - микориза, жёлуди]
     [отвечает с учетом всего контекста]
```

**Накопленный контекст:**
- История про симбиоз
- Примеры симбиоза
- Связи с дубом
- Связанные организмы

## Технические детали

### Векторизация

```python
# api/storage/weaviate_storage.py:155-172
def _create_embedding(self, text: str) -> np.ndarray:
    embedding = self.model.encode(text, convert_to_numpy=True)
    # Нормализуем для косинусного сходства
    norm = np.linalg.norm(embedding)
    if norm > 0:
        embedding = embedding / norm
    return embedding.astype("float32")
```

**Модель:** SentenceTransformer (по умолчанию)
**Размерность:** Зависит от модели (обычно 384-768)

### Семантический поиск

```python
# api/storage/weaviate_storage.py:1088-1151
async def search_similar_paragraphs(
    self,
    query: str,
    document_id: str,
    top_k: int = 10,
    ...
) -> List[Paragraph]:
    # 1. Прямой поиск
    similar_pairs = self.search_similar(query, document_id, top_k, ...)
    
    # 2. Если результатов мало - перефразирование запроса
    if len(similar_pairs) < 3:
        rephrased_queries = await rephrase_search_query(query)
        # Поиск по перефразированным запросам
```

**Особенности:**
- Векторный поиск по косинусному сходству
- Автоматическое перефразирование при малом количестве результатов
- Фильтрация по метаданным (теги, локация, экосистемы)

### Автоматическая классификация

```python
# api/storage/weaviate_storage.py:435-618
def _classify_paragraph(self, paragraph: Paragraph):
    # 1. Гибридная классификация (Weaviate + LLM)
    if tag_service and ENABLE_AUTOMATIC_DETECTORS:
        hybrid_tags = self._classify_with_hybrid_approach(paragraph, tag_service)
    
    # 2. Обнаружение организмов
    organisms = await detect_organisms(paragraph.content)
    
    # 3. Обнаружение экосистем
    ecosystems = await detect_ecosystems(paragraph.content)
    
    # 4. Извлечение локации
    location_result = extract_location_and_time(paragraph.content)
```

## Преимущества накопления знаний

### 1. **Контекстуальность**

LLM помнит весь контекст разговора и может:
- Ссылаться на предыдущие сообщения
- Развивать ранее обсужденные темы
- Поддерживать логику разговора

### 2. **Глубина понимания**

С каждым сообщением система:
- Узнает больше о предпочтениях пользователя
- Понимает контекст обсуждения
- Видит связи между концепциями

### 3. **Персонализация**

Система адаптируется:
- К стилю общения пользователя
- К интересующим темам
- К уровню знаний пользователя

### 4. **Экосистемное мышление**

Система видит:
- Связи между организмами
- Паттерны в экосистемах
- Причинно-следственные связи

## Ограничения

### 1. **Область поиска**

По умолчанию поиск ограничен текущей сессией (`document_id=session_id`):
- Не видит сообщения из других сессий
- Не использует знания других пользователей (если не настроено иначе)

### 2. **Объем контекста**

LLM имеет ограничение на размер контекста:
- Не все найденные параграфы могут быть включены
- Система использует умное сжатие и фильтрацию

### 3. **Качество эмбеддингов**

Зависит от модели:
- Более качественные модели = лучшее понимание смысла
- Многоязычность может быть ограничена

## Настройка

### Расширение области поиска

Чтобы использовать знания из всех сессий:

```python
# В api/chat/context_builder.py
similar_paragraphs = await storage.search_similar_paragraphs(
    query=message,
    document_id=None,  # Искать во всех документах
    top_k=10,
)
```

### Увеличение количества результатов

```python
# В api/chat/routes.py:350-356
graph_context = await build_graph_context(
    message=message_data.message,
    session_id=actual_session_id,
    storage=storage,
    max_depth=3,  # Увеличить глубину графа
    max_relationships=20,  # Увеличить количество связей
)
```

## Заключение

**Да, система действительно накапливает семантическую базу знаний во время чата**, и это делает разговор:

✅ **Более контекстуальным** - помнит предыдущие сообщения
✅ **Более глубоким** - видит связи между концепциями
✅ **Более персонализированным** - адаптируется к пользователю
✅ **Более интересным** - предлагает связанные темы и идеи

Каждое сообщение обогащает базу знаний, и последующие ответы становятся все более релевантными и интересными!
