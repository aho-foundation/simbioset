"""
–¢–µ—Å—Ç—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫—Ä–∞–µ–≤—ã—Ö —Å–ª—É—á–∞–µ–≤
"""

from api.storage.faiss import FAISSStorage, Paragraph


class TestEdgeCases:
    """–¢–µ—Å—Ç—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫—Ä–∞–µ–≤—ã—Ö —Å–ª—É—á–∞–µ–≤"""

    def setup_method(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ç–µ—Å—Ç–æ–≤"""
        self.search_engine = FAISSStorage()

    def test_empty_documents_list(self):
        """–¢–µ—Å—Ç –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –ø—É—Å—Ç–æ–≥–æ —Å–ø–∏—Å–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        count = self.search_engine.add_documents([], document_id="empty_test")
        assert count == 0

        paragraphs = self.search_engine.get_document_paragraphs("empty_test")
        assert len(paragraphs) == 0

    def test_empty_chat_messages_list(self):
        """–¢–µ—Å—Ç –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –ø—É—Å—Ç–æ–≥–æ —Å–ø–∏—Å–∫–∞ —á–∞—Ç-—Å–æ–æ–±—â–µ–Ω–∏–π"""
        count = self.search_engine.add_chat_messages([], chat_id="empty_chat_test")
        assert count == 0

        paragraphs = self.search_engine.get_document_paragraphs("empty_chat_test")
        assert len(paragraphs) == 0

    def test_documents_without_text(self):
        """–¢–µ—Å—Ç –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –±–µ–∑ —Ç–µ–∫—Å—Ç–∞"""
        documents = [
            {"title": "–ó–∞–≥–æ–ª–æ–≤–æ–∫ –±–µ–∑ —Ç–µ–∫—Å—Ç–∞"},  # –ù–µ—Ç –ø–æ–ª—è text
            {"text": ""},  # –ü—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç
            {"text": "  "},  # –¢–µ–∫—Å—Ç —Ç–æ–ª—å–∫–æ –∏–∑ –ø—Ä–æ–±–µ–ª–æ–≤
        ]

        count = self.search_engine.add_documents(documents, document_id="no_text_test")
        # –¢–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–π –¥–æ–∫—É–º–µ–Ω—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –¥–æ–±–∞–≤–ª–µ–Ω (—Å –ø—Ä–æ–±–µ–ª–∞–º–∏)
        # –í —Ä–µ–∞–ª—å–Ω–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –≤–æ–∑–º–æ–∂–Ω–æ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø—É—Å—Ç—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤
        paragraphs = self.search_engine.get_document_paragraphs("no_text_test")
        # –í –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏, –º–æ–∂–µ—Ç –±—ã—Ç—å 0 –∏–ª–∏ 1 –ø–∞—Ä–∞–≥—Ä–∞—Ñ

    def test_invalid_document_id(self):
        """–¢–µ—Å—Ç —Ä–∞–±–æ—Ç—ã —Å –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–º ID –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        # –ü–æ–ø—ã—Ç–∫–∞ –ø–æ–ª—É—á–∏—Ç—å –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã –∏–∑ –Ω–µ—Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        paragraphs = self.search_engine.get_document_paragraphs("nonexistent_doc")
        assert paragraphs == []

        # –ü–æ–ø—ã—Ç–∫–∞ –ø–æ–ª—É—á–∏—Ç—å –ø–∞—Ä–∞–≥—Ä–∞—Ñ –ø–æ ID –∏–∑ –Ω–µ—Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        para = self.search_engine.get_paragraph_by_id("nonexistent_doc", "some_id")
        assert para is None

        # –ü–æ–ø—ã—Ç–∫–∞ –æ–±–Ω–æ–≤–∏—Ç—å –ø–∞—Ä–∞–≥—Ä–∞—Ñ –≤ –Ω–µ—Å—É—â–µ—Å—Ç–≤—É—é—â–µ–º –¥–æ–∫—É–º–µ–Ω—Ç–µ
        fake_para = Paragraph(id="fake", content="fake")
        success = self.search_engine.update_paragraph("nonexistent_doc", fake_para)
        assert success is False

        # –ü–æ–ø—ã—Ç–∫–∞ —É–¥–∞–ª–∏—Ç—å –ø–∞—Ä–∞–≥—Ä–∞—Ñ –∏–∑ –Ω–µ—Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        success = self.search_engine.delete_paragraph("nonexistent_doc", "some_id")
        assert success is False

    def test_invalid_paragraph_id(self):
        """–¢–µ—Å—Ç —Ä–∞–±–æ—Ç—ã —Å –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–º ID –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞"""
        # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç
        documents = [{"text": "–¢–µ—Å—Ç–æ–≤—ã–π —Ç–µ–∫—Å—Ç"}]
        self.search_engine.add_documents(documents, document_id="invalid_id_test")

        # –ü–æ–ø—ã—Ç–∫–∞ –ø–æ–ª—É—á–∏—Ç—å –Ω–µ—Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –ø–∞—Ä–∞–≥—Ä–∞—Ñ
        para = self.search_engine.get_paragraph_by_id("invalid_id_test", "nonexistent_para")
        assert para is None

        # –ü–æ–ø—ã—Ç–∫–∞ –æ–±–Ω–æ–≤–∏—Ç—å –Ω–µ—Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –ø–∞—Ä–∞–≥—Ä–∞—Ñ
        fake_para = Paragraph(id="nonexistent_para", content="fake")
        success = self.search_engine.update_paragraph("invalid_id_test", fake_para)
        assert success is False

        # –ü–æ–ø—ã—Ç–∫–∞ —É–¥–∞–ª–∏—Ç—å –Ω–µ—Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –ø–∞—Ä–∞–≥—Ä–∞—Ñ
        success = self.search_engine.delete_paragraph("invalid_id_test", "nonexistent_para")
        assert success is False

    def test_long_text_handling(self):
        """–¢–µ—Å—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª–∏–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞"""
        long_text = "–¢–µ—Å—Ç. " * 1000  # –û—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        documents = [{"text": long_text}]

        count = self.search_engine.add_documents(documents, document_id="long_text_test")
        assert count == 1

        paragraphs = self.search_engine.get_document_paragraphs("long_text_test")
        assert len(paragraphs) == 1
        assert paragraphs[0].content == long_text

    def test_special_characters(self):
        """–¢–µ—Å—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤"""
        special_text = "–¢–µ—Å—Ç —Å —Å–∏–º–≤–æ–ª–∞–º–∏: \n \t \" ' < > & % @ # $ ‚Ç¨ ¬£ ¬•"
        documents = [{"text": special_text}]

        count = self.search_engine.add_documents(documents, document_id="special_chars_test")
        assert count == 1

        paragraphs = self.search_engine.get_document_paragraphs("special_chars_test")
        assert len(paragraphs) == 1
        assert paragraphs[0].content == special_text

    def test_unicode_text(self):
        """–¢–µ—Å—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏ —é–Ω–∏–∫–æ–¥ —Ç–µ–∫—Å—Ç–∞"""
        unicode_text = "–¢–µ—Å—Ç —Å —é–Ω–∏–∫–æ–¥–æ–º: ‰∏≠Êñá ÿßŸÑÿπÿ±ÿ®Ÿäÿ© —Ä—É—Å—Å–∫–∏–π üöÄ"
        documents = [{"text": unicode_text}]

        count = self.search_engine.add_documents(documents, document_id="unicode_test")
        assert count == 1

        paragraphs = self.search_engine.get_document_paragraphs("unicode_test")
        assert len(paragraphs) == 1
        assert paragraphs[0].content == unicode_text

    def test_search_with_empty_query(self):
        """–¢–µ—Å—Ç –ø–æ–∏—Å–∫–∞ —Å –ø—É—Å—Ç—ã–º –∑–∞–ø—Ä–æ—Å–æ–º"""
        documents = [{"text": "–¢–µ—Å—Ç–æ–≤—ã–π –¥–æ–∫—É–º–µ–Ω—Ç"}]
        self.search_engine.add_documents(documents, document_id="empty_query_test")

        results = self.search_engine.search_similar("", "empty_query_test")
        # –†–µ–∑—É–ª—å—Ç–∞—Ç –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç—ã–º –∏–ª–∏ —Å–æ–¥–µ—Ä–∂–∞—Ç—å –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏
        assert isinstance(results, list)

    def test_search_with_very_long_query(self):
        """–¢–µ—Å—Ç –ø–æ–∏—Å–∫–∞ —Å –æ—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã–º –∑–∞–ø—Ä–æ—Å–æ–º"""
        documents = [{"text": "–¢–µ—Å—Ç–æ–≤—ã–π –¥–æ–∫—É–º–µ–Ω—Ç"}]
        self.search_engine.add_documents(documents, document_id="long_query_test")

        long_query = "–ü–æ–∏—Å–∫. " * 1000
        results = self.search_engine.search_similar(long_query, "long_query_test")
        assert isinstance(results, list)

    def test_duplicate_paragraphs(self):
        """–¢–µ—Å—Ç –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤"""
        documents = [
            {"text": "–û–¥–∏–Ω–∞–∫–æ–≤—ã–π —Ç–µ–∫—Å—Ç"},
            {"text": "–û–¥–∏–Ω–∞–∫–æ–≤—ã–π —Ç–µ–∫—Å—Ç"},  # –¥—É–±–ª–∏–∫–∞—Ç
        ]

        count = self.search_engine.add_documents(documents, document_id="duplicate_test")
        # –û–±–∞ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞ –±—É–¥—É—Ç –¥–æ–±–∞–≤–ª–µ–Ω—ã, —Ç–∞–∫ –∫–∞–∫ —É –Ω–∏—Ö —Ä–∞–∑–Ω—ã–µ ID
        paragraphs = self.search_engine.get_document_paragraphs("duplicate_test")
        assert len(paragraphs) == 2
        assert paragraphs[0].content == paragraphs[1].content
        assert paragraphs[0].id != paragraphs[1].id  # —Ä–∞–∑–Ω—ã–µ ID

    def test_get_all_documents_empty(self):
        """–¢–µ—Å—Ç –ø–æ–ª—É—á–µ–Ω–∏—è —Å–ø–∏—Å–∫–∞ –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –ø—É—Å—Ç–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞"""
        all_docs = self.search_engine.get_all_documents()
        assert all_docs == []
